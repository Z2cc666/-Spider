#!/Users/z2cc/miniconda3/bin/python
# -*- coding: UTF-8 -*-
import os
import re
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import json
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache

# å…³é”®è¯ - æ‰©å¤§èŒƒå›´ä»¥è·å–æ›´å¤šç›¸å…³æ–‡ç« 
KEYWORDS = [
    "è´ªæ±¡", "è…è´¥", "å—è´¿", "å¤„ç½š", "åŒå¼€", "è°ƒæŸ¥", "èµ·è¯‰", "çºªæ£€", "å»‰æ´", "æŠ•æ¡ˆ", "è¢«æŸ¥", "è¿è§„", "è¿çºª", "å¤„åˆ†", "å…èŒ", "æ’¤èŒ",
    "åŒ»é™¢", "åŒ»ç”Ÿ", "é™¢é•¿", "ä¸»ä»»", "åŒ»ç–—", "è¯å“", "å›æ‰£", "çº¢åŒ…", "è´¿èµ‚", "è¿æ³•", "çŠ¯ç½ª", "ç«‹æ¡ˆ", "é€®æ•", "æ‹˜ç•™", "åˆ¤åˆ‘",
    "åŒ»ä¿", "åŒ»ä¿åŸºé‡‘", "éª—ä¿", "å¥—ä¿", "è™šå¼€", "å‘ç¥¨", "ç¨åŠ¡", "é€ƒç¨", "æ¼ç¨", "å·ç¨", "è¡¥ç¨", "ç½šæ¬¾", "æ²¡æ”¶", "è¿½ç¼´",
    "ä¸»åŠ¨æŠ•æ¡ˆ", "è¢«æŸ¥", "è¢«è¯‰", "è¢«é€®æ•", "è¢«æ‹˜ç•™", "è¢«åˆ¤åˆ‘", "è¢«å¼€é™¤", "è¢«å…èŒ", "è¢«å¤„åˆ†", "è¢«åŒå¼€", "è¢«è°ƒæŸ¥", "è¢«ç«‹æ¡ˆ",
    "å‰¯é™¢é•¿", "å‰¯ä¸»ä»»", "å§”å‘˜", "ç§‘é•¿", "å‰¯ç§‘é•¿", "æŠ¤å£«é•¿", "è¯å¸ˆ", "é™¢é•¿åŠ©ç†", "ä¸»ä»»åŒ»å¸ˆ", "å‰¯ä¸»ä»»åŒ»å¸ˆ", "ä¸»æ²»åŒ»å¸ˆ", "æŠ¤å£«", "æŠ¤å¸ˆ",
    "åŒ»ç–—è…è´¥", "åŒ»è¯è…è´¥", "åŒ»ç–—å›æ‰£", "è¯å“å›æ‰£", "åŒ»ç–—è´¿èµ‚", "è¯å“è´¿èµ‚", "åŒ»ç–—è¿æ³•", "è¯å“è¿æ³•", "åŒ»ç–—çŠ¯ç½ª", "è¯å“çŠ¯ç½ª"
]

# åŒ»è¯æ…§å…¬ä¼—å·é…ç½®
GZH_LIST = [
    {
        "name": "åŒ»è¯æ…§",
        "token": "1022341612",  # æ›´æ–°token
        "fakeid": "MjM5MzU5MzUwNQ%3D%3D",  # ä»URLä¸­æå–çš„fakeid
        "cookie": "RK=DC2Uq4Wf9P; ptcz=c9f4dcf0c0fb279d2316b228ce1d2d7a6b107f591ae8bbce0eac0ce98bc9de36; wxuin=51340895845860; mm_lang=zh_CN; _hp2_id.1405110977=%7B%22userId%22%3A%226801447023479475%22%2C%22pageviewId%22%3A%228306667787246811%22%2C%22sessionId%22%3A%224504468753015668%22%2C%22identity%22%3Anull%2C%22trackerVersion%22%3A%224.0%22%7D; ua_id=mxGDXOVuOo8d0D2hAAAAACdqUxp53FqemlDjGf2eSLM=; rewardsn=; wxtokenkey=777; poc_sid=HBg3iGijlmGc_2ocHEPN26JgrEcR59UETkMwwy7P; _clck=3911425635|1|fy6|0; uuid=8e983b004f998c2f2486628daa965d23; rand_info=CAESIMFXa7pbxrMmqQVBmDy8My3x6V5q80/zyyDTpZ2tsrja; slave_bizuin=3911425635; data_bizuin=3911425635; bizuin=3911425635; data_ticket=sbBLX2/f8X437MsBZwOn/8Td8c5XU5k77liknjRNwChFMBeuW9H5ZZl2mgaAshvO; slave_sid=WXVGQ3FBNG0zTlpDTGlUSGJQTnpIYlhpeDU3YllubzZRckwxSXdwZ05oSkxIQ0hxdmk1QVY2UHVtRXBwUmJkN2VGUHVVeF9RazBiY05Fa3FQYWlodU9xcnFhRmtBdFA2T2ptbk9BMEo4NFNOWHFzamtXWERldExQZm1CRzllZVBVSUtJNUVTYW5nR0FOeFJw; slave_user=gh_b3cdf815ccbf; xid=24ae585f4840c2e3c147341722f8af33; _clsk=1y2cy24|1754273381311|4|1|mp.weixin.qq.com/weheat-agent/payload/record"
    },
]

# åˆ›å»ºå…¨å±€ä¼šè¯å¯¹è±¡ï¼Œå¤ç”¨è¿æ¥
SESSION = requests.Session()
SESSION.headers.update({
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
})

# æ€§èƒ½é…ç½® - æé«˜å¤šçº¿ç¨‹æ€§èƒ½
PERFORMANCE_CONFIG = {
    'max_workers': 8,  # å¢åŠ å¹¶å‘çº¿ç¨‹æ•°
    'timeout': 15,     # å¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´
    'page_delay': 0.3, # å‡å°‘é¡µé¢é—´å»¶è¿Ÿ
    'cache_size': 256  # å¢åŠ LRUç¼“å­˜å¤§å°
}

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ•ˆç‡
COMPILED_REGEXES = {
    'date': re.compile(r"(\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}æ—¥?)|(\d{1,2}/\d{1,2}/\d{4})|(\d{4}-\d{1,2}-\d{1,2})"),
    'province': re.compile(r"(åŒ—äº¬|å¤©æ´¥|ä¸Šæµ·|é‡åº†|æ²³åŒ—|å±±è¥¿|è¾½å®|å‰æ—|é»‘é¾™æ±Ÿ|æ±Ÿè‹|æµ™æ±Ÿ|å®‰å¾½|ç¦å»º|æ±Ÿè¥¿|å±±ä¸œ|æ²³å—|æ¹–åŒ—|æ¹–å—|å¹¿ä¸œ|æµ·å—|å››å·|è´µå·|äº‘å—|é™•è¥¿|ç”˜è‚ƒ|é’æµ·|å°æ¹¾|å†…è’™å¤|å¹¿è¥¿|è¥¿è—|å®å¤|æ–°ç–†|é¦™æ¸¯|æ¾³é—¨)"),
    'position': re.compile(r"(è‘£äº‹é•¿|æ€»è£|æ€»ç»ç†|é¦–å¸­æ‰§è¡Œå®˜|CEO|å‰¯æ€»è£|å‰¯æ€»ç»ç†|è‘£äº‹|å‰¯è‘£äº‹é•¿|è‘£äº‹ä¼šæˆå‘˜|æ€»ç›‘|å‰¯æ€»ç›‘|ç»ç†|å‰¯ç»ç†|ä¸»ç®¡|ä¸»ä»»åŒ»å¸ˆ|å‰¯ä¸»ä»»åŒ»å¸ˆ|ä¸»æ²»åŒ»å¸ˆ|ä¸»ç®¡åŒ»å¸ˆ|ä½é™¢åŒ»å¸ˆ|å…šå§”ä¹¦è®°|çºªå§”ä¹¦è®°|é™¢é•¿åŠ©ç†|ä¸»ä»»å§”å‘˜|å‰¯ä¸»ä»»å§”å‘˜|å‰¯é™¢é•¿|é™¢é•¿|å‰¯ä¸»ä»»|ä¸»ä»»|å‰¯ç§‘é•¿|ç§‘é•¿|å‰¯å¤„é•¿|å¤„é•¿|å‰¯å±€é•¿|å±€é•¿|å…é•¿|å‰¯å…é•¿|æŠ¤å£«é•¿|ä¹¦è®°|å§”å‘˜|åŒ»ç”Ÿ|è¯å¸ˆ|æŠ¤å£«|æŠ¤å¸ˆ|å·¥ç¨‹å¸ˆ|ä¼šè®¡å¸ˆ|åŠ©ç†|ç§˜ä¹¦|ä¸“å‘˜|ä¸»åŠ|å¹²äº‹|èŒå‘˜|å·¥ä½œäººå‘˜)")
}

# ä¿æŒå…¼å®¹æ€§çš„åˆ«åï¼ˆä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼‰
DATE_RE = COMPILED_REGEXES['date']
PROVINCE_RE = COMPILED_REGEXES['province'] 
POSITION_RE = COMPILED_REGEXES['position']
HOSPITAL_RE = re.compile(r"[\u4e00-\u9fa5]{2,}(åŒ»é™¢|å«ç”Ÿé™¢|å«ç”ŸæœåŠ¡ä¸­å¿ƒ|ä¸­åŒ»é™¢|åŒ»ç§‘å¤§å­¦é™„å±åŒ»é™¢|äººæ°‘åŒ»é™¢|ä¸­å¿ƒåŒ»é™¢|ç¬¬ä¸€åŒ»é™¢|ç¬¬äºŒåŒ»é™¢|ç¬¬ä¸‰åŒ»é™¢)")
NAME_RE = re.compile(r"[\u4e00-\u9fa5]{2,4}")

@lru_cache(maxsize=PERFORMANCE_CONFIG['cache_size'])
def quick_title_filter(title):
    """å¿«é€Ÿæ ‡é¢˜è¿‡æ»¤ - ä½¿ç”¨ç¼“å­˜æé«˜æ•ˆç‡"""
    return any(keyword in title for keyword in KEYWORDS)

def fetch_articles_batch(gzh, start_page=0, batch_size=20, page_size=10):
    """æ‰¹é‡æŠ“å–å…¬ä¼—å·æ–‡ç« åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬ä½¿ç”¨ä¼šè¯å¤ç”¨"""
    batch_articles = []
    consecutive_empty_pages = 0  # è¿ç»­ç©ºé¡µè®¡æ•°
    max_consecutive_empty = 3    # æœ€å¤§è¿ç»­ç©ºé¡µæ•°
    
    end_page = start_page + batch_size
    print(f"å¼€å§‹æŠ“å–ç¬¬{start_page+1}é¡µåˆ°ç¬¬{end_page}é¡µ...")
    
    for page in range(start_page, end_page):
        params = {
            'sub': 'list',
            'search_field': 'null',
            "begin": page * page_size,
            "count": page_size,
            'query': '',
            'fakeid': gzh['fakeid'],
            'type': '101_1',
            'free_publish_type': '1',
            'sub_action': 'list_ex',
            'fingerprint': '3ac89b4e2b10b8054438ff4808dccd28',
            'token': gzh['token'],
            'lang': 'zh_CN',
            'f': 'json',
            'ajax': '1',
        }
        headers = {
            'cookie': gzh['cookie'],
            'referer': f'https://mp.weixin.qq.com/cgi-bin/appmsg?t=media/appmsg_edit_v2&action=edit&isNew=1&type=77&createType=0&token={gzh["token"]}&lang=zh_CN&timestamp={int(time.time()*1000)}',
            'accept': '*/*',
            'accept-language': 'zh-CN,zh;q=0.9',
            'x-requested-with': 'XMLHttpRequest',
            'sec-ch-ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin'
        }
        try:
            # ä½¿ç”¨å…¨å±€ä¼šè¯å¤ç”¨è¿æ¥
            resp = SESSION.get("https://mp.weixin.qq.com/cgi-bin/appmsgpublish", 
                              params=params, headers=headers, timeout=PERFORMANCE_CONFIG['timeout'])
            print(f"ç¬¬{page+1}é¡µè¯·æ±‚çŠ¶æ€ç : {resp.status_code}")
            if resp.status_code != 200:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
                return batch_articles, False  # è¿”å›Falseè¡¨ç¤ºå‡ºé”™
            result = resp.json()
            if result.get("base_resp", {}).get("ret") != 0:
                print(f"APIè¿”å›é”™è¯¯: {result.get('base_resp', {})}")
                return batch_articles, False
            publish_page = result.get("publish_page")
            if not publish_page:
                print("æœªæ‰¾åˆ°publish_pageæ•°æ®")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    print(f"è¿ç»­{max_consecutive_empty}é¡µæ— æ•°æ®ï¼Œåœæ­¢æœ¬æ‰¹æ¬¡")
                    break
                continue
                
            articleList = json.loads(publish_page)
            articles = articleList.get("publish_list", [])
            if not articles:
                print(f"ç¬¬{page+1}é¡µæ²¡æœ‰æ–‡ç« ")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    print(f"è¿ç»­{max_consecutive_empty}é¡µæ— æ–‡ç« ï¼Œåœæ­¢æœ¬æ‰¹æ¬¡")
                    break
                continue
            else:
                consecutive_empty_pages = 0
            
            print(f"ç¬¬{page+1}é¡µè·å–åˆ°{len(articles)}ç¯‡æ–‡ç« ")
            batch_articles.extend(articles)
            
            # é¡µé¢é—´å»¶è¿Ÿ
            time.sleep(PERFORMANCE_CONFIG['page_delay'])
                
        except Exception as e:
            print(f"{gzh['name']} ç¬¬{page+1}é¡µæŠ“å–å¤±è´¥: {e}")
            consecutive_empty_pages += 1
            if consecutive_empty_pages >= max_consecutive_empty:
                print(f"è¿ç»­{max_consecutive_empty}æ¬¡å¤±è´¥ï¼Œåœæ­¢æœ¬æ‰¹æ¬¡")
                break
            time.sleep(3)
            continue
    
    success = len(batch_articles) > 0
    print(f"æ‰¹æ¬¡è·å–å®Œæˆï¼Œå…±è·å–åˆ° {len(batch_articles)} ç¯‡æ–‡ç« ")
    return batch_articles, success

def get_article_content(link, gzh_name, session=None):
    """è·å–æ–‡ç« æ­£æ–‡ - ç‰¹åˆ«å¤„ç†è“è‰²å­—ä½“ä¸­çš„æœºæ„/èŒä½ä¿¡æ¯"""
    if session is None:
        session = SESSION
        
    try:
        headers = {
            'referer': 'https://mp.weixin.qq.com/',
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8'
        }
        resp = session.get(link, headers=headers, timeout=PERFORMANCE_CONFIG['timeout'])
        
        if resp.status_code != 200:
            return ''
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # å°è¯•å¤šç§æ–¹å¼è·å–å†…å®¹
        content_div = soup.find('div', {'id': 'js_content'}) or \
                     soup.find('div', class_='rich_media_content') or \
                     soup.find('div', class_='content') or \
                     soup.find('article')
        
        if content_div:
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in content_div(["script", "style"]):
                script.decompose()
            
            # å…ˆå°è¯•æå–è“è‰²å­—ä½“ä¿¡æ¯
            enhanced_text = extract_blue_text_info(content_div)
            if enhanced_text:
                return enhanced_text
            
            # å¦‚æœæ²¡æœ‰è“è‰²å­—ä½“ï¼Œè¿”å›å¸¸è§„æ–‡æœ¬
            text = content_div.get_text(separator='\n', strip=True)
            return text
        else:
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šåŒºåŸŸï¼Œå°è¯•è·å–æ•´ä¸ªbody
            body = soup.find('body')
            if body:
                enhanced_text = extract_blue_text_info(body)
                if enhanced_text:
                    return enhanced_text
                text = body.get_text(separator='\n', strip=True)
                return text
            return ''
            
    except Exception as e:
        # å‡å°‘é”™è¯¯è¾“å‡ºï¼Œé¿å…å½±å“æ€§èƒ½
        return ''

def extract_blue_text_info(soup_element):
    """ä¸“é—¨æå–è“è‰²å­—ä½“ä¸­çš„æœºæ„/èŒä½ä¿¡æ¯"""
    try:
        enhanced_content = []
        base_text = soup_element.get_text(separator='\n', strip=True)
        
        # æŸ¥æ‰¾è“è‰²å­—ä½“å…ƒç´ ï¼ˆå¤šç§å¯èƒ½çš„CSSè¡¨ç¤ºæ–¹å¼ï¼‰
        blue_elements = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è“è‰²å­—ä½“æ ‡ç­¾
        for element in soup_element.find_all():
            style = element.get('style', '')
            class_name = ' '.join(element.get('class', []))
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è“è‰²å­—ä½“
            is_blue = (
                'color:blue' in style.replace(' ', '') or
                'color:#' in style and ('blue' in style or '0000ff' in style.lower() or 
                                       '0066cc' in style.lower() or '003399' in style.lower()) or
                'blue' in class_name.lower() or
                'highlight' in class_name.lower() or
                element.name in ['strong', 'b'] and any(keyword in element.get_text() for keyword in 
                    ['åŒ»é™¢', 'å§”å‘˜ä¼š', 'å±€', 'å…', 'ç§‘', 'å¤„', 'éƒ¨', 'ä¸­å¿ƒ', 'å…¬å¸', 'é›†å›¢'])
            )
            
            if is_blue:
                blue_text = element.get_text(strip=True)
                if blue_text and len(blue_text) > 2:
                    blue_elements.append({
                        'text': blue_text,
                        'element': element
                    })
        
        # å¦‚æœæ‰¾åˆ°è“è‰²å­—ä½“ï¼Œæ„å»ºå¢å¼ºçš„æ–‡æœ¬
        if blue_elements:
            enhanced_content.append("=== è“è‰²å­—ä½“ä¿¡æ¯æå– ===")
            for blue_item in blue_elements:
                blue_text = blue_item['text']
                element = blue_item['element']
                
                # è·å–è“è‰²å­—ä½“åé¢çš„æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«äººåï¼‰
                next_text = ""
                try:
                    # æŸ¥æ‰¾ç´§è·Ÿåœ¨è“è‰²å­—ä½“åé¢çš„æ–‡æœ¬
                    next_sibling = element.next_sibling
                    if next_sibling:
                        if hasattr(next_sibling, 'get_text'):
                            next_text = next_sibling.get_text(strip=True)
                        else:
                            next_text = str(next_sibling).strip()
                    
                    # å¦‚æœæ²¡æœ‰ç›´æ¥å…„å¼ŸèŠ‚ç‚¹ï¼ŒæŸ¥æ‰¾çˆ¶çº§çš„ä¸‹ä¸€ä¸ªæ–‡æœ¬
                    if not next_text and element.parent:
                        parent_next = element.parent.next_sibling
                        if parent_next:
                            if hasattr(parent_next, 'get_text'):
                                next_text = parent_next.get_text(strip=True)
                            else:
                                next_text = str(parent_next).strip()
                except:
                    pass
                
                # æ„å»ºå¢å¼ºä¿¡æ¯
                enhanced_line = f"ã€æœºæ„/èŒä½ã€‘{blue_text}"
                if next_text and len(next_text) <= 10:  # å¯èƒ½æ˜¯äººå
                    enhanced_line += f" ã€äººåã€‘{next_text}"
                enhanced_content.append(enhanced_line)
            
            enhanced_content.append("=== åŸæ–‡å†…å®¹ ===")
            enhanced_content.append(base_text)
            return '\n'.join(enhanced_content)
        
        return None  # æ²¡æœ‰æ‰¾åˆ°è“è‰²å­—ä½“
        
    except Exception as e:
        return None

def process_single_article(article_data):
    """å¤„ç†å•ç¯‡æ–‡ç« çš„å‡½æ•° - ç”¨äºå¹¶å‘å¤„ç†"""
    try:
        article, gzh_name, create_time = article_data
        title = article.get("title", "")
        link = article.get("link", "")
        
        # å¿«é€Ÿæ ‡é¢˜é¢„ç­›é€‰
        if not quick_title_filter(title):
            return None
            
        # è·å–æ–‡ç« å†…å®¹
        content = get_article_content(link, gzh_name)
        
        # å†…å®¹å…³é”®è¯ç­›é€‰
        if not any(k in content for k in KEYWORDS):
            return None
            
        return {
            'title': title,
            'link': link,
            'content': content,
            'create_time': create_time
        }
    except Exception as e:
        return None

def extract_first(pattern, text):
    """æ”¹è¿›çš„æ­£åˆ™æå–ç¬¬ä¸€ä¸ªåŒ¹é…"""
    if not text:
        return ''
    
    matches = pattern.findall(text)
    if matches:
        if isinstance(matches[0], tuple):
            for match in matches:
                for group in match:
                    if group and len(group) > 1:
                        return group
        else:
            return max(matches, key=len)
    
    return ''

def is_valid_chinese_name(name):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä¸­æ–‡å§“å - æ›´ä¸¥æ ¼çš„éªŒè¯"""
    if not name or len(name) < 2 or len(name) > 4:
        return False
    
    # å¿…é¡»å…¨éƒ¨æ˜¯ä¸­æ–‡å­—ç¬¦
    if not all('\u4e00' <= c <= '\u9fff' for c in name):
        return False
    
    # æ‰©å±•çš„å¸¸è§å§“æ°åˆ—è¡¨
    common_surnames = [
        'ç‹', 'æ', 'å¼ ', 'åˆ˜', 'é™ˆ', 'æ¨', 'èµµ', 'é»„', 'å‘¨', 'å´', 'å¾', 'å­™', 'èƒ¡', 'æœ±', 
        'é«˜', 'æ—', 'ä½•', 'éƒ­', 'é©¬', 'ç½—', 'æ¢', 'å®‹', 'éƒ‘', 'è°¢', 'éŸ©', 'å”', 'å†¯', 'äº', 
        'è‘£', 'è§', 'ç¨‹', 'æ›¹', 'è¢', 'é‚“', 'è®¸', 'å‚…', 'æ²ˆ', 'æ›¾', 'å½­', 'å•', 'è‹', 'å¢', 
        'è’‹', 'è”¡', 'è´¾', 'ä¸', 'é­', 'è–›', 'å¶', 'é˜', 'ä½™', 'æ½˜', 'æœ', 'æˆ´', 'å¤', 'é’Ÿ', 
        'æ±ª', 'ç”°', 'ä»»', 'å§œ', 'èŒƒ', 'æ–¹', 'çŸ³', 'å§š', 'è°­', 'å»–', 'é‚¹', 'ç†Š', 'é‡‘', 'é™†', 
        'éƒ', 'å­”', 'ç™½', 'å´”', 'åº·', 'æ¯›', 'é‚±', 'ç§¦', 'æ±Ÿ', 'å²', 'é¡¾', 'ä¾¯', 'é‚µ', 'å­Ÿ', 
        'é¾™', 'ä¸‡', 'æ®µ', 'é›·', 'é’±', 'æ±¤', 'å°¹', 'é»', 'æ˜“', 'å¸¸', 'æ­¦', 'ä¹”', 'è´º', 'èµ–', 
        'é¾š', 'æ–‡', 'é˜š', 'éª†', 'é‚¢', 'ä¸¥', 'å­Ÿ', 'å´', 'æ¬§é˜³', 'å¸é©¬', 'ä¸Šå®˜', 'è¯¸è‘›', 
        'ä¸œæ–¹', 'å—å®«', 'è¥¿é—¨', 'åŒ—å†¥', 'ä»¤ç‹', 'çš‡ç”«', 'å°‰è¿Ÿ', 'å…¬å­™', 'æ…•å®¹', 'é•¿å­™'
    ]
    
    # å¿…é¡»ä»¥å¸¸è§å§“æ°å¼€å¤´
    if not any(name.startswith(surname) for surname in common_surnames):
        return False
    
    # å¤§å¹…æ‰©å±•çš„æ— æ•ˆè¯æ±‡åˆ—è¡¨ - åŒ…å«ç”¨æˆ·æåˆ°çš„é”™è¯¯åå­—
    invalid_words = {
        # åŸºç¡€æ— æ•ˆè¯
        'åŒ»è¯æ…§', 'èµ›æŸè“', 'åŒ»é™¢', 'è¯Šæ‰€', 'å«ç”Ÿé™¢', 'å«ç”Ÿæ‰€', 'å§”å‘˜ä¼š', 'å…šå§”ä¼š', 'è‘£äº‹ä¼š',
        
        # èŒä½ç›¸å…³è¯æ±‡
        'ä¸»ä»»åŒ»å¸ˆ', 'å‰¯ä¸»ä»»åŒ»å¸ˆ', 'ä¸»æ²»åŒ»å¸ˆ', 'ä½é™¢åŒ»å¸ˆ', 'æŠ¤å¸ˆ', 'æŠ¤å£«', 'è¯å¸ˆ', 
        'è‘£äº‹é•¿', 'å‰¯æ€»è£', 'æ€»ç»ç†', 'å‰¯æ€»ç»ç†', 'æ€»ç›‘', 'å‰¯æ€»ç›‘', 'ç»ç†', 'å‰¯ç»ç†',
        'é™¢é•¿', 'å‰¯é™¢é•¿', 'ä¸»ä»»', 'å‰¯ä¸»ä»»', 'ç§‘é•¿', 'å‰¯ç§‘é•¿', 'å¤„é•¿', 'å‰¯å¤„é•¿',
        'å±€é•¿', 'å‰¯å±€é•¿', 'å…é•¿', 'å‰¯å…é•¿', 'ä¹¦è®°', 'å‰¯ä¹¦è®°', 'å§”å‘˜', 'å¸¸å§”',
        
        # ç”¨æˆ·ç‰¹åˆ«æåˆ°çš„é”™è¯¯åå­—
        'ä¸¥è‚ƒ', 'ä¸‡å…ƒ', 'å¸¸å§”', 'çœå§”', 'å¸‚å§”', 'å¿å§”', 'åŒºå§”', 'å…šå§”', 'çºªå§”',
        
        # æ—¶é—´ç›¸å…³è¯æ±‡
        'æ˜¨æ—¥', 'ä»Šæ—¥', 'è¿‘æ—¥', 'æ—¥å‰', 'ç›®å‰', 'ç°åœ¨', 'å½“å‰', 'æ­¤å‰', 'æ­¤å', 
        'ä¹‹å‰', 'ä¹‹å', 'ä»¥æ¥', 'ä¸Šåˆ', 'ä¸‹åˆ', 'æ™šä¸Š', 'æ·±å¤œ', 'å‡Œæ™¨', 'æ—©ä¸Š', 
        'ä¸­åˆ', 'å‚æ™š', 'å¤œé—´', 'ç™½å¤©', 'å¤œæ™š', 'æœˆåº•', 'æœˆåˆ', 'æœˆä¸­', 'å¹´åˆ', 
        'å¹´ä¸­', 'å¹´åº•', 'å­£åº¦', 'åŠå¹´', 'å…¨å¹´', 'å½“å¹´', 'å»å¹´', 'æ˜å¹´',
        
        # æ•°å­—å’Œé‡‘é¢ç›¸å…³
        'ä¸‡å…ƒ', 'åƒå…ƒ', 'ç™¾å…ƒ', 'äº¿å…ƒ', 'åä¸‡', 'ç™¾ä¸‡', 'åƒä¸‡', 'ä¸€ä¸‡', 'ä¸¤ä¸‡', 
        'ä¸‰ä¸‡', 'å››ä¸‡', 'äº”ä¸‡', 'å…­ä¸‡', 'ä¸ƒä¸‡', 'å…«ä¸‡', 'ä¹ä¸‡', 'åä¸‡',
        
        # è¿æ³•ç›¸å…³è¯æ±‡
        'è¿æ³•', 'è¿è§„', 'è¿çºª', 'çŠ¯ç½ª', 'è…è´¥', 'å—è´¿', 'è´ªæ±¡', 'æŒªç”¨', 'ä¾µå ', 
        'æ»¥ç”¨', 'åŒå¼€', 'å…èŒ', 'æ’¤èŒ', 'å¼€é™¤', 'å¤„åˆ†', 'è°ƒæŸ¥', 'å®¡æŸ¥', 'ç«‹æ¡ˆ',
        'èµ·è¯‰', 'åˆ¤åˆ‘', 'æ‹˜ç•™', 'é€®æ•', 'ç›‘ç¦', 'ç¾æŠ¼', 'å–ä¿', 'ç¼“åˆ‘',
        
        # è¡Œæ”¿åŒºåˆ’ç›¸å…³
        'çœä»½', 'å¸‚åŒº', 'å¿åŒº', 'è¡—é“', 'ç¤¾åŒº', 'æ‘å§”', 'å±…å§”', 'ä¹¡é•‡', 'å¼€å‘åŒº',
        
        # åª’ä½“å’Œå‘å¸ƒç›¸å…³
        'å†…å®¹', 'æ¥æº', 'æ¶ˆæ¯', 'é€šæŠ¥', 'å…¬å‘Š', 'å£°æ˜', 'é€šçŸ¥', 'å…¬ç¤º', 'å‘å¸ƒ', 
        'æŠ¥é“', 'æ–°é—»', 'åª’ä½“', 'è®°è€…', 'ç¼–è¾‘', 'ä½œè€…', 'è½¬è½½', 'å…³æ³¨', 'ç‚¹å‡»', 
        'æ‰«ç ', 'è®¢é˜…', 'åˆ†äº«', 'è¯„è®º', 'ç‚¹èµ', 'æ”¶è—',
        
        # æœºæ„ç±»å‹è¯æ±‡
        'éƒ¨é—¨', 'å•ä½', 'æœºæ„', 'ç»„ç»‡', 'å›¢ä½“', 'åä¼š', 'å­¦ä¼š', 'è”ç›Ÿ', 'é›†å›¢', 
        'å…¬å¸', 'ä¼ä¸š', 'å·¥å‚', 'è½¦é—´', 'ç­ç»„', 'å°ç»„', 'å›¢é˜Ÿ', 'é˜Ÿä¼',
        
        # æ˜ç¡®çš„é”™è¯¯è¯†åˆ«è¯æ±‡ï¼ˆä»å®é™…æ•°æ®ä¸­è§‚å¯Ÿåˆ°çš„ï¼‰
        'æ–¹ç»“æœ', 'åº·æ–¹ç”Ÿç‰©', 'ç”Ÿç‰©åŒ»', 'äºæ–‡æ˜', 'æå¥åº·', 'èµµå¾·å', 
        'æ–¹ç”Ÿç‰©', 'ç‰©åŒ»ä»£', 'ä»£è¡¨å›¢', 'è°ƒæŸ¥ç»„', 'å®˜æ–¹ç»“', 'ç»“æœå…¬', 'å…¬å¸ƒå…³', 
        'å…³äºåŒ»', 'åŒ»å­¦ä¸“', 'ä¸“å®¶è¢«', 'è¢«æŸ¥ç»', 'ç»è°ƒå–', 'å–é‡åŒ»', 'åŒ»ç”Ÿå¸‚', 
        'å¸‚å«ç”Ÿ', 'ç”Ÿå¥åŒ»', 'åŒ»é•¿åŒ—', 'åŒ—äº¬è¿™', 'è¿™ä¸¤åŒ»', 'åŒ»ç”Ÿè´µ', 'è´µå·çœ', 
        'çœäººæ–¹', 'äººæ–¹é¢', 'æ–¹é¢çš„', 'é¢çš„é—®', 'é—®é¢˜çš„', 'é¢˜çš„å¤„', 'å¤„ç†æƒ…',
        
        # å¸¸è§çš„é”™è¯¯ç»„åˆ
        'è¢«æŸ¥', 'è¢«æŠ“', 'è¢«æ•', 'è¢«è¯‰', 'è¢«å‘Š', 'è¢«å®¡', 'è¢«åˆ¤', 'è¢«ç½š', 'è¢«å¤„',
        'æ¶‰å«Œ', 'æ¶‰æ¡ˆ', 'æ¶‰åŠ', 'æ¶‰åŠåˆ°', 'å› ä¸º', 'ç”±äº', 'æ ¹æ®', 'æŒ‰ç…§',
        'æ¥å—', 'æ”¶å—', 'è·å¾—', 'å–å¾—', 'å¾—åˆ°', 'æ‹¿åˆ°', 'æ”¶åˆ°', 'ç»™äºˆ',
        
        # å…¶ä»–å®¹æ˜“è¯¯è¯†åˆ«çš„è¯æ±‡
        'ä¸€äº›', 'ä¸€å®š', 'ä¸€ç›´', 'ä¸€èˆ¬', 'ä¸€æ—¦', 'ä¸€æ–¹', 'ä¸€æ ·', 'ä¸€èµ·', 'ä¸€åˆ‡',
        'å¯¹äº', 'å¯¹æ–¹', 'å¯¹æ­¤', 'å¯¹å¾…', 'å…³äº', 'å…³ç³»', 'å…³é”®', 'å…³æ³¨', 'å…³å¿ƒ',
        'å°±æ˜¯', 'å°±åœ¨', 'å°±æœ‰', 'å°±èƒ½', 'å°±ä¼š', 'å°±è¦', 'å°±å¯', 'å°±æ­¤', 'å°±æ­¤',
        'å¯ä»¥', 'å¯èƒ½', 'å¯æ˜¯', 'å¯è§', 'å¯è°“', 'å¯æƒœ', 'å¯è¡Œ', 'å¯ä¿¡', 'å¯ç–‘',
        'è¿˜æœ‰', 'è¿˜æ˜¯', 'è¿˜åœ¨', 'è¿˜ä¼š', 'è¿˜èƒ½', 'è¿˜è¦', 'è¿˜å¯', 'è¿˜å°†', 'è¿˜å¾—',
        'éƒ½æ˜¯', 'éƒ½æœ‰', 'éƒ½åœ¨', 'éƒ½ä¼š', 'éƒ½èƒ½', 'éƒ½è¦', 'éƒ½å¯', 'éƒ½å°†', 'éƒ½å¾—',
        
        # ç‰¹æ®Šå­—ç¬¦ç»„åˆ
        'ä¸æ˜¯', 'ä¸åœ¨', 'ä¸ä¼š', 'ä¸èƒ½', 'ä¸è¦', 'ä¸å¯', 'ä¸å¾—', 'ä¸å¦‚', 'ä¸ä½†',
        'æ²¡æœ‰', 'æ²¡åœ¨', 'æ²¡ä¼š', 'æ²¡èƒ½', 'æ²¡è¦', 'æ²¡å¯', 'æ²¡å¾—', 'æ²¡æ³•', 'æ²¡äº‹'
    }
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ— æ•ˆè¯æ±‡åˆ—è¡¨ä¸­
    if name in invalid_words:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ•ˆè¯æ±‡çš„ä¸€éƒ¨åˆ†
    for invalid_word in invalid_words:
        if len(invalid_word) >= 2 and invalid_word in name:
            return False
    
    # æ£€æŸ¥ä¸åˆç†çš„é‡å¤å­—ç¬¦
    if len(set(name)) == 1:  # å…¨éƒ¨æ˜¯ç›¸åŒå­—ç¬¦
        return False
    
    # æ£€æŸ¥å¸¸è§çš„é”™è¯¯æ¨¡å¼
    error_patterns = [
        r'.*è¢«.*',  # åŒ…å«"è¢«"çš„
        r'.*æ¶‰.*',  # åŒ…å«"æ¶‰"çš„
        r'.*æŸ¥.*',  # åŒ…å«"æŸ¥"çš„
        r'.*å¤„.*',  # åŒ…å«"å¤„"çš„
        r'.*è°ƒ.*',  # åŒ…å«"è°ƒ"çš„
        r'.*å®¡.*',  # åŒ…å«"å®¡"çš„
        r'.*æ¡ˆ.*',  # åŒ…å«"æ¡ˆ"çš„
        r'.*ç½ª.*',  # åŒ…å«"ç½ª"çš„
        r'.*æ³•.*',  # åŒ…å«"æ³•"çš„
        r'.*çºª.*',  # åŒ…å«"çºª"çš„
        r'.*å…ƒ$',   # ä»¥"å…ƒ"ç»“å°¾çš„
        r'.*å§”$',   # ä»¥"å§”"ç»“å°¾çš„
        r'.*é™¢$',   # ä»¥"é™¢"ç»“å°¾çš„
        r'.*å±€$',   # ä»¥"å±€"ç»“å°¾çš„
        r'.*å…$',   # ä»¥"å…"ç»“å°¾çš„
        r'.*éƒ¨$',   # ä»¥"éƒ¨"ç»“å°¾çš„
        r'.*ä¼š$',   # ä»¥"ä¼š"ç»“å°¾çš„
    ]
    
    for pattern in error_patterns:
        if re.match(pattern, name):
            return False
    
    return True

def extract_name(text):
    """æ”¹è¿›çš„å§“åæå–é€»è¾‘ - æ›´å‡†ç¡®çš„è¯†åˆ«"""
    names = []
    
    # ä¼˜å…ˆå¤„ç†è“è‰²å­—ä½“æ ‡è®°ä¸­çš„äººå
    blue_text_names = re.findall(r'ã€äººåã€‘([^ã€ã€‘\n]+)', text)
    if blue_text_names:
        for name in blue_text_names:
            name = name.strip()
            if is_valid_chinese_name(name):
                names.append(name)
        if names:
            return names  # è¿”å›æ‰€æœ‰æœ‰æ•ˆå§“å
    
    # æ‰©å±•çš„å¸¸è§å§“æ°
    common_surnames = [
        'ç‹', 'æ', 'å¼ ', 'åˆ˜', 'é™ˆ', 'æ¨', 'èµµ', 'é»„', 'å‘¨', 'å´', 'å¾', 'å­™', 'èƒ¡', 'æœ±', 
        'é«˜', 'æ—', 'ä½•', 'éƒ­', 'é©¬', 'ç½—', 'æ¢', 'å®‹', 'éƒ‘', 'è°¢', 'éŸ©', 'å”', 'å†¯', 'äº', 
        'è‘£', 'è§', 'ç¨‹', 'æ›¹', 'è¢', 'é‚“', 'è®¸', 'å‚…', 'æ²ˆ', 'æ›¾', 'å½­', 'å•', 'è‹', 'å¢', 
        'è’‹', 'è”¡', 'è´¾', 'ä¸', 'é­', 'è–›', 'å¶', 'é˜', 'ä½™', 'æ½˜', 'æœ', 'æˆ´', 'å¤', 'é’Ÿ', 
        'æ±ª', 'ç”°', 'ä»»', 'å§œ', 'èŒƒ', 'æ–¹', 'çŸ³', 'å§š', 'è°­', 'å»–', 'é‚¹', 'ç†Š', 'é‡‘', 'é™†', 
        'éƒ', 'å­”', 'ç™½', 'å´”', 'åº·', 'æ¯›', 'é‚±', 'ç§¦', 'æ±Ÿ', 'å²', 'é¡¾', 'ä¾¯', 'é‚µ', 'å­Ÿ', 
        'é¾™', 'ä¸‡', 'æ®µ', 'é›·', 'é’±', 'æ±¤', 'å°¹', 'é»', 'æ˜“', 'å¸¸', 'æ­¦', 'ä¹”', 'è´º', 'èµ–', 
        'é¾š', 'æ–‡', 'é˜š', 'éª†', 'é‚¢', 'ä¸¥'
    ]
    
    # æ–¹æ³•1: æŸ¥æ‰¾"XæŸ"ã€"XæŸæŸ"ç­‰åŒ¿åæ¨¡å¼
    for surname in common_surnames:
        anonymous_patterns = [
            f"{surname}æŸæŸæŸ",
            f"{surname}æŸæŸ",
            f"{surname}æŸ"
        ]
        for pattern in anonymous_patterns:
            if pattern in text:
                names.append(pattern)
                break
    
    # æ–¹æ³•2: ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å®Œæ•´å§“å
    # èŒä½ + å§“åæ¨¡å¼
    position_name_patterns = [
        r'(é™¢é•¿|å‰¯é™¢é•¿|ä¸»ä»»|å‰¯ä¸»ä»»|ç§‘é•¿|å‰¯ç§‘é•¿|å¤„é•¿|å‰¯å¤„é•¿|å±€é•¿|å‰¯å±€é•¿|å…é•¿|å‰¯å…é•¿|ä¹¦è®°|å‰¯ä¹¦è®°|å§”å‘˜|ä¸»å¸­|å‰¯ä¸»å¸­|è‘£äº‹é•¿|å‰¯è‘£äº‹é•¿|æ€»ç»ç†|å‰¯æ€»ç»ç†|æ€»è£|å‰¯æ€»è£|æ€»ç›‘|å‰¯æ€»ç›‘|ç»ç†|å‰¯ç»ç†|ä¸»ç®¡|ä¸»ä»»åŒ»å¸ˆ|å‰¯ä¸»ä»»åŒ»å¸ˆ|ä¸»æ²»åŒ»å¸ˆ|ä¸»ç®¡åŒ»å¸ˆ|ä½é™¢åŒ»å¸ˆ|æŠ¤å£«é•¿|è¯å¸ˆ|æŠ¤å¸ˆ|åŒ»ç”Ÿ|æŠ¤å£«)([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡é˜šéª†é‚¢ä¸¥][\u4e00-\u9fff]{1,3})(?=è¢«|æ¶‰|æ¥å—|å› |ä¸¥é‡|è¿|çš„)',
        
        # å§“å + è¿æ³•å…³é”®è¯æ¨¡å¼
        r'([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡é˜šéª†é‚¢ä¸¥][\u4e00-\u9fff]{1,3})(?=è¢«æŸ¥|è¢«åŒå¼€|è¢«å¼€é™¤|è¢«å…èŒ|è¢«å¤„åˆ†|æ¶‰å«Œ|è¢«é€®æ•|è¢«æ‹˜ç•™|è¢«åˆ¤åˆ‘|ä¸»åŠ¨æŠ•æ¡ˆ|è¢«è¯‰|è¢«è°ƒæŸ¥|è¢«ç«‹æ¡ˆ|è¢«æ’¤èŒ|å› æ¶‰å«Œ|å› ä¸¥é‡è¿çºª|æ¥å—è°ƒæŸ¥|æ¥å—å®¡æŸ¥)',
        
        # å¯¹...è¿›è¡Œè°ƒæŸ¥/å¤„åˆ†æ¨¡å¼
        r'å¯¹([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡é˜šéª†é‚¢ä¸¥][\u4e00-\u9fff]{1,3})(?=ä¸¥é‡è¿çºªè¿æ³•|è¿›è¡Œ|ç«‹æ¡ˆ|è°ƒæŸ¥|å®¡æŸ¥|å¤„åˆ†)'
    ]
    
    for pattern in position_name_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                # å¯¹äºå…ƒç»„ï¼Œå–ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆå§“åéƒ¨åˆ†ï¼‰
                name = match[1] if len(match) > 1 else match[0]
            else:
                name = match
            
            if is_valid_chinese_name(name):
                names.append(name)
    
    # å»é‡å¹¶è¿”å›
    valid_names = list(set(names))
    return valid_names

def extract_description(text, person_name=""):
    """æ”¹è¿›çš„è¿æ³•äº‹ä»¶æè¿°æå–"""
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
    relevant_sentences = []
    
    # é«˜æƒé‡å…³é”®è¯
    high_priority_keywords = [
        'è¢«æŸ¥', 'è¢«åŒå¼€', 'è¢«å¼€é™¤', 'è¢«å…èŒ', 'è¢«å¤„åˆ†', 'æ¶‰å«Œ', 'è¢«é€®æ•', 'è¢«æ‹˜ç•™', 
        'è¢«åˆ¤åˆ‘', 'ä¸»åŠ¨æŠ•æ¡ˆ', 'è¢«è¯‰', 'è¢«è°ƒæŸ¥', 'è¢«ç«‹æ¡ˆ', 'è¢«æ’¤èŒ', 'ä¸¥é‡è¿çºªè¿æ³•',
        'è´ªæ±¡', 'è…è´¥', 'å—è´¿', 'è¡Œè´¿', 'æŒªç”¨', 'æ»¥ç”¨èŒæƒ', 'ç©å¿½èŒå®ˆ'
    ]
    
    # è¡Œä¸šå…³é”®è¯
    industry_keywords = [
        'åŒ»é™¢', 'åŒ»ç”Ÿ', 'é™¢é•¿', 'ä¸»ä»»', 'åŒ»ç–—', 'è¯å“', 'å›æ‰£', 'çº¢åŒ…', 'åŒ»ä¿', 'éª—ä¿',
        'å±€é•¿', 'å¤„é•¿', 'ç§‘é•¿', 'å§”å‘˜', 'è‘£äº‹', 'ç»ç†', 'æ€»ç›‘', 'ä¸»ç®¡',
        'ä¼ä¸š', 'å…¬å¸', 'é›†å›¢', 'å•ä½', 'æœºæ„', 'ç»„ç»‡'
    ]
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 20 or len(sentence) > 300:
            continue
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
        has_high_priority = any(kw in sentence for kw in high_priority_keywords)
        has_industry = any(kw in sentence for kw in industry_keywords)
        has_general_keyword = any(kw in sentence for kw in KEYWORDS)
        has_person = person_name and person_name in sentence if person_name else True
        
        if (has_high_priority or (has_industry and has_general_keyword)) and has_person:
            clean_sentence = re.sub(r'\s+', '', sentence)
            if len(clean_sentence) >= 20:
                relevant_sentences.append({
                    'sentence': clean_sentence,
                    'priority': 3 if has_high_priority else 2 if has_industry else 1,
                    'length': len(clean_sentence)
                })
    
    if relevant_sentences:
        # å¦‚æœæœ‰æŒ‡å®šäººåï¼Œä¼˜å…ˆè¿”å›åŒ…å«è¯¥äººåçš„å¥å­
        if person_name:
            person_sentences = [s for s in relevant_sentences if person_name in s['sentence']]
            if person_sentences:
                best_sentence = max(person_sentences, key=lambda s: (s['priority'], min(s['length'], 150)))
                return best_sentence['sentence']
        
        # å¦åˆ™è¿”å›ä¼˜å…ˆçº§æœ€é«˜çš„å¥å­
        best_sentence = max(relevant_sentences, key=lambda s: (s['priority'], min(s['length'], 150)))
        return best_sentence['sentence']
    
    return ""

def extract_position(text, person_name=""):
    """æ”¹è¿›çš„èŒä½æå–å‡½æ•°"""
    if not text:
        return ""
    
    # ä¼˜å…ˆå¤„ç†è“è‰²å­—ä½“æ ‡è®°ä¸­çš„èŒä½ä¿¡æ¯
    blue_text_positions = re.findall(r'ã€æœºæ„/èŒä½ã€‘([^ã€ã€‘\n]+)', text)
    if blue_text_positions:
        for mixed_text in blue_text_positions:
            mixed_text = mixed_text.strip()
            position = extract_position_from_mixed_text(mixed_text)
            if position:
                return position
    
    # èŒä½å±‚çº§ï¼ˆä»é«˜åˆ°ä½ï¼‰
    position_hierarchy = [
        ['è‘£äº‹é•¿', 'æ€»è£', 'æ€»ç»ç†', 'é¦–å¸­æ‰§è¡Œå®˜', 'CEO', 'æ€»ç›‘', 'å‰¯æ€»è£', 'å‰¯æ€»ç»ç†'],
        ['é™¢é•¿', 'å‰¯é™¢é•¿', 'å…šå§”ä¹¦è®°', 'çºªå§”ä¹¦è®°', 'å±€é•¿', 'å‰¯å±€é•¿', 'å…é•¿', 'å‰¯å…é•¿'],
        ['å¤„é•¿', 'å‰¯å¤„é•¿', 'ç§‘é•¿', 'å‰¯ç§‘é•¿', 'ä¸»ä»»', 'å‰¯ä¸»ä»»', 'ä¸»ä»»å§”å‘˜', 'å‰¯ä¸»ä»»å§”å‘˜'],
        ['è‘£äº‹', 'å‰¯è‘£äº‹é•¿', 'æ€»ç›‘', 'å‰¯æ€»ç›‘', 'ç»ç†', 'å‰¯ç»ç†', 'ä¸»ç®¡'],
        ['ä¸»ä»»åŒ»å¸ˆ', 'å‰¯ä¸»ä»»åŒ»å¸ˆ', 'ä¸»æ²»åŒ»å¸ˆ', 'ä¸»ç®¡åŒ»å¸ˆ', 'ä½é™¢åŒ»å¸ˆ'],
        ['å§”å‘˜', 'æŠ¤å£«é•¿', 'åŒ»ç”Ÿ', 'è¯å¸ˆ', 'é™¢é•¿åŠ©ç†', 'æŠ¤å£«', 'æŠ¤å¸ˆ', 'å·¥ç¨‹å¸ˆ', 'ä¼šè®¡å¸ˆ']
    ]
    
    # å¦‚æœæœ‰æŒ‡å®šäººåï¼Œä¼˜å…ˆåœ¨äººåé™„è¿‘æŸ¥æ‰¾èŒä½
    if person_name:
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\nï¼Œ]', text)
        person_contexts = []
        for sentence in sentences:
            if person_name in sentence:
                person_contexts.append(sentence)
        context_text = 'ã€‚'.join(person_contexts)
        if context_text:
            text = context_text
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾èŒä½
    all_matches = []
    for pattern in [
        r'(è‘£äº‹é•¿|æ€»è£|æ€»ç»ç†|é¦–å¸­æ‰§è¡Œå®˜|CEO|æ€»ç›‘|å‰¯æ€»è£|å‰¯æ€»ç»ç†)',
        r'(é™¢é•¿|å‰¯é™¢é•¿|å…šå§”ä¹¦è®°|çºªå§”ä¹¦è®°|å±€é•¿|å‰¯å±€é•¿|å…é•¿|å‰¯å…é•¿)',
        r'(å¤„é•¿|å‰¯å¤„é•¿|ç§‘é•¿|å‰¯ç§‘é•¿|ä¸»ä»»|å‰¯ä¸»ä»»|ä¸»ä»»å§”å‘˜|å‰¯ä¸»ä»»å§”å‘˜)',
        r'(è‘£äº‹|å‰¯è‘£äº‹é•¿|æ€»ç›‘|å‰¯æ€»ç›‘|ç»ç†|å‰¯ç»ç†|ä¸»ç®¡)',
        r'(ä¸»ä»»åŒ»å¸ˆ|å‰¯ä¸»ä»»åŒ»å¸ˆ|ä¸»æ²»åŒ»å¸ˆ|ä¸»ç®¡åŒ»å¸ˆ|ä½é™¢åŒ»å¸ˆ)',
        r'(å§”å‘˜|æŠ¤å£«é•¿|åŒ»ç”Ÿ|è¯å¸ˆ|é™¢é•¿åŠ©ç†|æŠ¤å£«|æŠ¤å¸ˆ|å·¥ç¨‹å¸ˆ|ä¼šè®¡å¸ˆ)'
    ]:
        matches = re.findall(pattern, text)
        all_matches.extend(matches)
    
    if not all_matches:
        return ""
    
    # æŒ‰å±‚çº§ä¼˜å…ˆçº§é€‰æ‹©æœ€é«˜èŒä½
    for level in position_hierarchy:
        for position in all_matches:
            if position in level:
                return position
    
    return all_matches[0] if all_matches else ""

def extract_position_from_mixed_text(mixed_text):
    """ä»æ··åˆçš„æœºæ„/èŒä½æ–‡æœ¬ä¸­æå–èŒä½"""
    position_keywords = [
        'è‘£äº‹é•¿', 'æ€»è£', 'æ€»ç»ç†', 'å‰¯æ€»è£', 'å‰¯æ€»ç»ç†', 'å‰¯è‘£äº‹é•¿',
        'é™¢é•¿', 'å‰¯é™¢é•¿', 'å…šå§”ä¹¦è®°', 'çºªå§”ä¹¦è®°', 'å±€é•¿', 'å‰¯å±€é•¿', 'å…é•¿', 'å‰¯å…é•¿',
        'å¤„é•¿', 'å‰¯å¤„é•¿', 'ç§‘é•¿', 'å‰¯ç§‘é•¿', 'ä¸»ä»»', 'å‰¯ä¸»ä»»', 'ä¸»ä»»å§”å‘˜', 'å‰¯ä¸»ä»»å§”å‘˜',
        'è‘£äº‹', 'æ€»ç›‘', 'å‰¯æ€»ç›‘', 'ç»ç†', 'å‰¯ç»ç†', 'ä¸»ç®¡',
        'ä¸»ä»»åŒ»å¸ˆ', 'å‰¯ä¸»ä»»åŒ»å¸ˆ', 'ä¸»æ²»åŒ»å¸ˆ', 'ä¸»ç®¡åŒ»å¸ˆ', 'ä½é™¢åŒ»å¸ˆ',
        'å§”å‘˜', 'æŠ¤å£«é•¿', 'åŒ»ç”Ÿ', 'è¯å¸ˆ', 'é™¢é•¿åŠ©ç†', 'æŠ¤å£«', 'æŠ¤å¸ˆ', 'å·¥ç¨‹å¸ˆ', 'ä¼šè®¡å¸ˆ'
    ]
    
    for keyword in position_keywords:
        if keyword in mixed_text:
            return keyword
    
    return ""

def extract_hospital(text):
    """æ”¹è¿›çš„æœºæ„æå–å‡½æ•°"""
    institutions = []
    
    # ä¼˜å…ˆå¤„ç†è“è‰²å­—ä½“æ ‡è®°ä¸­çš„æœºæ„ä¿¡æ¯
    blue_text_institutions = re.findall(r'ã€æœºæ„/èŒä½ã€‘([^ã€ã€‘\n]+)', text)
    if blue_text_institutions:
        for inst in blue_text_institutions:
            inst = inst.strip()
            if any(keyword in inst for keyword in ['åŒ»é™¢', 'å§”å‘˜ä¼š', 'å±€', 'å…', 'å…¬å¸', 'é›†å›¢', 'ä¸­å¿ƒ', 'ç§‘', 'å¤„', 'éƒ¨']):
                clean_inst = extract_institution_from_mixed_text(inst)
                if clean_inst and len(clean_inst) > 3:
                    institutions.append(clean_inst)
        if institutions:
            return select_best_institution(institutions)
    
    # å„ç§æœºæ„åŒ¹é…æ¨¡å¼
    patterns = [
        # æ”¿åºœæœºæ„
        r'([\u4e00-\u9fa5]{2,8}[çœå¸‚å¿åŒº][\u4e00-\u9fa5]{2,15}(?:å§”å‘˜ä¼š|å«å¥å§”|å‘æ”¹å§”|å±€|å…|éƒ¨é—¨|ä¸­å¿ƒ|åŠå…¬å®¤|ç®¡ç†å±€|ç›‘ç£å±€))',
        # ä¼ä¸šæœºæ„
        r'([\u4e00-\u9fa5]{2,15}(?:æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|æœ‰é™è´£ä»»å…¬å¸|é›†å›¢å…¬å¸|æ§è‚¡é›†å›¢|æŠ•èµ„é›†å›¢))',
        r'([\u4e00-\u9fa5]{3,15}(?:é›†å›¢|å…¬å¸))',
        # åŒ»ç–—æœºæ„
        r'([\u4e00-\u9fa5]{2,8}[çœå¸‚å¿åŒº][\u4e00-\u9fa5]{2,15}(?:äººæ°‘åŒ»é™¢|ä¸­å¿ƒåŒ»é™¢|ä¸­åŒ»é™¢|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+åŒ»é™¢|åŒ»é™¢))',
        r'([\u4e00-\u9fa5]{2,8}(?:å¤§å­¦|åŒ»ç§‘å¤§å­¦|åŒ»å­¦é™¢)é™„å±[\u4e00-\u9fa5]{0,10}åŒ»é™¢)',
        r'([\u4e00-\u9fa5]{3,12}åŒ»é™¢)',
        # å…¶ä»–æœºæ„
        r'([\u4e00-\u9fa5]{2,12}(?:å¤§å­¦|å­¦é™¢|ç ”ç©¶é™¢|ç ”ç©¶æ‰€|ç§‘å­¦é™¢))',
        r'([\u4e00-\u9fa5]{2,12}(?:åä¼š|å­¦ä¼š|åŸºé‡‘ä¼š|è”åˆä¼š|å•†ä¼š))'
    ]
    
    # æ’é™¤å…³é”®è¯
    exclude_keywords = [
        'åŒ»è¯æ…§', 'èµ›æŸè“', 'è¢«æŸ¥', 'è¢«å¤„', 'è°ƒæŸ¥', 'å®˜æ–¹', 'ç»“æœ', 'å…¬å¸ƒ',
        'è¿æ³•', 'è¿çºª', 'è´ªæ±¡', 'è…è´¥', 'å—è´¿', 'åŒå¼€', 'å…èŒ', 'æ’¤èŒ'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for institution in matches:
            if is_valid_institution(institution, exclude_keywords):
                institutions.append(institution)
    
    return select_best_institution(institutions)

def extract_institution_from_mixed_text(mixed_text):
    """ä»æ··åˆçš„æœºæ„/èŒä½æ–‡æœ¬ä¸­æå–æœºæ„åç§°"""
    position_keywords = [
        'é™¢é•¿', 'å‰¯é™¢é•¿', 'ä¸»ä»»', 'å‰¯ä¸»ä»»', 'ä¹¦è®°', 'å‰¯ä¹¦è®°', 'å±€é•¿', 'å‰¯å±€é•¿', 
        'å…é•¿', 'å‰¯å…é•¿', 'å§”å‘˜', 'ä¸»å¸­', 'å‰¯ä¸»å¸­', 'ç§‘é•¿', 'å¤„é•¿', 'éƒ¨é•¿',
        'æ€»ç»ç†', 'å‰¯æ€»ç»ç†', 'è‘£äº‹é•¿', 'å‰¯è‘£äº‹é•¿', 'æ€»è£', 'å‰¯æ€»è£', 'ç»ç†',
        'ä¸»ä»»åŒ»å¸ˆ', 'å‰¯ä¸»ä»»åŒ»å¸ˆ', 'ä¸»æ²»åŒ»å¸ˆ', 'åŒ»å¸ˆ', 'æŠ¤å£«é•¿', 'æŠ¤å¸ˆ'
    ]
    
    for keyword in position_keywords:
        if keyword in mixed_text:
            parts = mixed_text.split(keyword)
            if parts[0]:
                institution = parts[0].strip()
                if any(org_keyword in institution for org_keyword in 
                      ['åŒ»é™¢', 'å§”å‘˜ä¼š', 'å±€', 'å…', 'å…¬å¸', 'é›†å›¢', 'ä¸­å¿ƒ', 'ç§‘', 'å¤„', 'éƒ¨']):
                    return institution
    
    return mixed_text.strip()

def is_valid_institution(institution, exclude_keywords):
    """éªŒè¯æœºæ„åç§°æ˜¯å¦æœ‰æ•ˆ"""
    if not institution or len(institution) < 3 or len(institution) > 40:
        return False
    
    if any(keyword in institution for keyword in exclude_keywords):
        return False
    
    invalid_starts = ['è¢«', 'æ¶‰', 'æŸ¥', 'å¤„', 'è¿', 'æ³•', 'çºª', 'å¼€', 'å…', 'æ’¤']
    if any(institution.startswith(start) for start in invalid_starts):
        return False
    
    return True

def select_best_institution(institutions):
    """ä»å€™é€‰æœºæ„ä¸­é€‰æ‹©æœ€ä½³çš„"""
    if not institutions:
        return ""
    
    unique_institutions = list(set(institutions))
    if len(unique_institutions) == 1:
        return unique_institutions[0]
    
    # ä¼˜å…ˆçº§å…³é”®è¯
    priority_keywords = [
        ['å§”å‘˜ä¼š', 'çœæ”¿åºœ', 'å¸‚æ”¿åºœ', 'å¿æ”¿åºœ'],
        ['çºªå§”', 'ç›‘å§”', 'ç»„ç»‡éƒ¨', 'å®£ä¼ éƒ¨'],
        ['å«å¥å§”', 'å‘æ”¹å§”', 'å±€', 'å…', 'éƒ¨é—¨'],
        ['é›†å›¢å…¬å¸', 'æœ‰é™å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸'],
        ['äººæ°‘åŒ»é™¢', 'ä¸­å¿ƒåŒ»é™¢', 'ä¸­åŒ»é™¢', 'åŒ»é™¢'],
        ['å¤§å­¦', 'å­¦é™¢', 'ç ”ç©¶é™¢', 'ç ”ç©¶æ‰€']
    ]
    
    for priority_group in priority_keywords:
        for institution in unique_institutions:
            if any(keyword in institution for keyword in priority_group):
                return institution
    
    return max(unique_institutions, key=len)

def extract_multiple_violations(text):
    """æå–ä¸€ç¯‡æ–‡ç« ä¸­çš„å¤šä¸ªè¿æ³•äº‹ä»¶"""
    results = []
    
    # æå–æ‰€æœ‰å¯èƒ½çš„äººå
    names = extract_name(text)
    
    # ç”¨äºå»é‡çš„é›†åˆ
    seen_combinations = set()
    
    if not names:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äººåï¼Œä½†åŒ…å«å…³é”®è¯ï¼Œä»ç„¶è®°å½•
        if any(kw in text for kw in KEYWORDS):
            province = extract_first(PROVINCE_RE, text)
            hospital = extract_hospital(text)
            desc = extract_description(text)
            date = extract_first(DATE_RE, text)
            position = extract_position(text)
            
            if desc:
                unique_key = f"{province}_{hospital}_{position}_{desc[:50]}"
                if unique_key not in seen_combinations:
                    seen_combinations.add(unique_key)
                    results.append({
                        "å§“å": "",
                        "çœä»½": province,
                        "åŒ»é™¢": hospital,
                        "èŒä½": position,
                        "æè¿°": desc,
                        "æ—¥æœŸ": date
                    })
    else:
        # ä¸ºæ¯ä¸ªäººåæå–ä¿¡æ¯
        for name in names:
            context_sentences = []
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
            
            for i, sentence in enumerate(sentences):
                if name in sentence:
                    start = max(0, i-1)
                    end = min(len(sentences), i+2)
                    context_sentences.extend(sentences[start:end])
            
            context = 'ã€‚'.join(context_sentences)
            
            province = extract_first(PROVINCE_RE, context) or extract_first(PROVINCE_RE, text)
            hospital = extract_hospital(context) or extract_hospital(text)
            desc = extract_description(context, name)
            date = extract_first(DATE_RE, context) or extract_first(DATE_RE, text)
            position = extract_position(context, name)
            
            unique_key = f"{name}_{province}_{hospital}_{position}_{desc[:50]}"
            if unique_key not in seen_combinations:
                seen_combinations.add(unique_key)
                results.append({
                    "å§“å": name,
                    "çœä»½": province,
                    "åŒ»é™¢": hospital,
                    "èŒä½": position,
                    "æè¿°": desc,
                    "æ—¥æœŸ": date
                })
    
    return results

def filter_and_extract(articles, gzh_name, existing_results=None, max_workers=None):
    """ä¼˜åŒ–çš„ç­›é€‰å’Œæå–å‡½æ•° - ä½¿ç”¨å¹¶å‘å¤„ç†"""
    print(f"ğŸ“Š æœ¬æ¬¡å…±è·å–{len(articles)}ç¯‡æ–‡ç« ï¼Œå¼€å§‹æ™ºèƒ½ç­›é€‰...")
    results = existing_results or []
    
    if max_workers is None:
        max_workers = PERFORMANCE_CONFIG['max_workers']
    
    # ç”¨äºå…¨å±€å»é‡çš„é›†åˆ
    global_seen_combinations = set()
    if existing_results:
        for result in existing_results:
            description = result.get('Description', '')
            if pd.isna(description):
                description = ''
            elif isinstance(description, str):
                description = description[:50]
            else:
                description = str(description)[:50]
            
            unique_key = f"{result.get('å§“å', '')}_{result.get('çœä»½', '')}_{result.get('åŒ»é™¢', '')}_{result.get('èŒä½', '')}_{description}"
            global_seen_combinations.add(unique_key)
    
    # å‡†å¤‡å¹¶å‘å¤„ç†çš„æ•°æ®
    article_tasks = []
    for article in articles:
        try:
            info = json.loads(article.get("publish_info", "{}"))
            if info.get("appmsgex"):
                title = info["appmsgex"][0].get("title", "")
                link = info["appmsgex"][0].get("link", "")
                create_time = info["appmsgex"][0].get("create_time")
                if isinstance(create_time, int):
                    create_time = time.strftime("%Y-%m-%d", time.localtime(create_time))
                
                if quick_title_filter(title):
                    article_tasks.append((
                        {"title": title, "link": link}, 
                        gzh_name, 
                        create_time
                    ))
        except Exception:
            continue
    
    print(f"ğŸ” æ ‡é¢˜é¢„ç­›é€‰ï¼š{len(articles)} -> {len(article_tasks)} ç¯‡æ–‡ç« ")
    
    if not article_tasks:
        print("âŒ æ²¡æœ‰åŒ¹é…çš„æ–‡ç« ")
        return results
    
    # å¹¶å‘è·å–æ–‡ç« å†…å®¹
    print(f"ğŸš€ ä½¿ç”¨ {max_workers} çº¿ç¨‹å¹¶å‘å¤„ç†æ–‡ç« å†…å®¹...")
    valid_articles = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_article = {
            executor.submit(process_single_article, task): task 
            for task in article_tasks
        }
        
        completed = 0
        for future in as_completed(future_to_article):
            completed += 1
            if completed % 10 == 0:
                print(f"â³ å·²å¤„ç† {completed}/{len(article_tasks)} ç¯‡æ–‡ç« ")
                
            try:
                result = future.result()
                if result:
                    valid_articles.append(result)
            except Exception as e:
                pass
    
    print(f"âœ… å†…å®¹ç­›é€‰ï¼š{len(article_tasks)} -> {len(valid_articles)} ç¯‡æœ‰æ•ˆæ–‡ç« ")
    
    if not valid_articles:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ç« å†…å®¹")
        return results
    
    # ä¸²è¡Œå¤„ç†è¿æ³•äº‹ä»¶æå–
    matched_count = 0
    for article_data in valid_articles:
        try:
            matched_count += 1
            title = article_data['title']
            link = article_data['link']
            content = article_data['content']
            create_time = article_data['create_time']
            
            print(f"ğŸ“„ å¤„ç†æ–‡ç«  {matched_count}: {title[:50]}...")
            
            # æå–å¤šä¸ªè¿æ³•äº‹ä»¶
            violations = extract_multiple_violations(content)
            
            if matched_count <= 5:
                print(f"   ğŸ“‹ æå–åˆ° {len(violations)} ä¸ªè¿æ³•äº‹ä»¶")
                for j, v in enumerate(violations):
                    print(f"      äº‹ä»¶{j+1}: å§“å={v['å§“å']}, æœºæ„={v['åŒ»é™¢'][:20]}{'...' if len(v['åŒ»é™¢']) > 20 else ''}, èŒä½={v['èŒä½']}")
            
            for violation in violations:
                violation["é€šæŠ¥æ—¶é—´"] = violation["æ—¥æœŸ"] or create_time
                violation["Resource"] = gzh_name
                violation["æ–‡ç« é“¾æ¥"] = link
                violation["æ ‡é¢˜"] = title
                
                description = violation['æè¿°']
                if isinstance(description, str):
                    description_slice = description[:50]
                else:
                    description_slice = str(description)[:50]
                
                global_unique_key = f"{violation['å§“å']}_{violation['çœä»½']}_{violation['åŒ»é™¢']}_{violation['èŒä½']}_{description_slice}"
                
                if global_unique_key not in global_seen_combinations:
                    global_seen_combinations.add(global_unique_key)
                    
                    result = {
                        "é€šæŠ¥æ—¶é—´": violation["é€šæŠ¥æ—¶é—´"],
                        "çœä»½": violation["çœä»½"],
                        "åŒ»é™¢": violation["åŒ»é™¢"],
                        "èŒä½": violation["èŒä½"],
                        "å§“å": violation["å§“å"],
                        "Resource": violation["Resource"],
                        "Description": violation["æè¿°"],
                        "æ–‡ç« é“¾æ¥": violation["æ–‡ç« é“¾æ¥"],
                        "æ ‡é¢˜": violation["æ ‡é¢˜"]
                    }
                    results.append(result)
                else:
                    if matched_count <= 5:
                        print(f"      âš ï¸ è·³è¿‡é‡å¤è®°å½•: {violation['å§“å']} - {violation['åŒ»é™¢'][:20]}{'...' if len(violation['åŒ»é™¢']) > 20 else ''}")
                    
        except Exception as e:
            print(f"âŒ [{gzh_name}] æ–‡ç« å¤„ç†å¤±è´¥: {e}")
    
    print(f"ğŸ¯ å¤„ç†å®Œæˆï¼šåŒ¹é…åˆ° {len(valid_articles)} ç¯‡ç›¸å…³æ–‡ç« ï¼Œæå–åˆ° {len(results) - len(existing_results or [])} æ¡æ–°è®°å½•")
    return results

def save_progress(current_page, gzh_name, all_results):
    """ä¿å­˜çˆ¬å–è¿›åº¦å’Œæ•°æ®"""
    os.makedirs('å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ', exist_ok=True)
    
    progress_info = {
        'last_page': current_page,
        'gzh_name': gzh_name,
        'total_records': len(all_results),
        'last_update': time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    progress_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /åŒ»è¯æ…§_çˆ¬å–è¿›åº¦.json'
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_info, f, ensure_ascii=False, indent=2)
    
    if all_results:
        df = pd.DataFrame(all_results)
        output_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /åŒ»è¯æ…§è¿æ³•äº‹ä»¶_ä¼˜åŒ–ç‰ˆ.xlsx'
        df.to_excel(output_file, index=False)
        print(f"å·²ä¿å­˜ {len(all_results)} æ¡è®°å½•åˆ° {output_file}")
        print(f"è¿›åº¦å·²ä¿å­˜ï¼šå½“å‰é¡µé¢{current_page}")

def load_progress():
    """åŠ è½½çˆ¬å–è¿›åº¦"""
    progress_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /åŒ»è¯æ…§_çˆ¬å–è¿›åº¦.json'
    data_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /åŒ»è¯æ…§è¿æ³•äº‹ä»¶_ä¼˜åŒ–ç‰ˆ.xlsx'
    
    start_page = 0
    existing_results = []
    
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_info = json.load(f)
            start_page = progress_info.get('last_page', 0)
            print(f"å‘ç°è¿›åº¦æ–‡ä»¶ï¼šä¸Šæ¬¡çˆ¬å–åˆ°ç¬¬{start_page}é¡µ")
            print(f"ä¸Šæ¬¡æ›´æ–°æ—¶é—´ï¼š{progress_info.get('last_update', 'æœªçŸ¥')}")
        except Exception as e:
            print(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    if os.path.exists(data_file):
        try:
            existing_df = pd.read_excel(data_file)
            existing_results = existing_df.to_dict('records')
            print(f"å‘ç°å·²æœ‰æ•°æ®æ–‡ä»¶ï¼ŒåŒ…å« {len(existing_results)} æ¡è®°å½•")
        except Exception as e:
            print(f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
    
    return start_page, existing_results

def main():
    # åŠ è½½å·²æœ‰è¿›åº¦å’Œæ•°æ®
    start_page, all_results = load_progress()
    
    if start_page > 0 or all_results:
        print(f"\næ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼Ÿ")
        print(f"- ä¸Šæ¬¡çˆ¬å–åˆ°ç¬¬{start_page}é¡µ")
        print(f"- å·²æœ‰{len(all_results)}æ¡è®°å½•")
        response = input("ç»§ç»­ä¸Šæ¬¡è¿›åº¦(y) è¿˜æ˜¯é‡æ–°å¼€å§‹(n)ï¼Ÿ[y/n]: ").lower()
        
        if response != 'y':
            start_page = 0
            all_results = []
            print("å°†é‡æ–°å¼€å§‹çˆ¬å–")
        else:
            print(f"å°†ä»ç¬¬{start_page + 1}é¡µå¼€å§‹ç»§ç»­çˆ¬å–")
    
    for gzh in GZH_LIST:
        print(f"\nå¼€å§‹æŠ“å–å…¬ä¼—å·: {gzh['name']}")
        print(f"ä½¿ç”¨é¢„è®¾fakeid: {gzh['fakeid']}")
        
        current_page = start_page
        max_pages = 1000
        batch_size = 20
        
        try:
            while current_page < max_pages:
                print(f"\n=== å¼€å§‹å¤„ç†ç¬¬{current_page//batch_size + 1}æ‰¹æ¬¡ (ç¬¬{current_page+1}-{min(current_page+batch_size, max_pages)}é¡µ) ===")
                
                batch_articles, success = fetch_articles_batch(gzh, current_page, batch_size, page_size=10)
                
                if not success or not batch_articles:
                    print(f"ç¬¬{current_page//batch_size + 1}æ‰¹æ¬¡æŠ“å–å¤±è´¥æˆ–æ— æ–‡ç« ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                print(f"ç¬¬{current_page//batch_size + 1}æ‰¹æ¬¡å…±è·å– {len(batch_articles)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹ç­›é€‰...")
                
                batch_results = filter_and_extract(batch_articles, gzh['name'], all_results)
                all_results = batch_results
                
                print(f"ç¬¬{current_page//batch_size + 1}æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç´¯è®¡è·å¾— {len(all_results)} æ¡è®°å½•")
                
                current_page += batch_size
                save_progress(current_page, gzh['name'], all_results)
                
                if current_page < max_pages:
                    print(f"æ‰¹æ¬¡é—´ä¼‘æ¯2ç§’...")
                    time.sleep(2)
                    
        except KeyboardInterrupt:
            print(f"\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
            save_progress(current_page, gzh['name'], all_results)
            print(f"è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯ä»ç¬¬{current_page + 1}é¡µç»§ç»­")
            return
        except Exception as e:
            print(f"\nçˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            save_progress(current_page, gzh['name'], all_results)
            print(f"è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯ä»ç¬¬{current_page + 1}é¡µç»§ç»­")
            return
    
    # æœ€ç»ˆä¿å­˜å’Œç»Ÿè®¡
    if all_results:
        save_progress(current_page, gzh['name'], all_results)
        
        df = pd.DataFrame(all_results)
        print(f"\n=== çˆ¬å–å®Œæˆï¼æ•°æ®ç»Ÿè®¡ ===")
        print(f"æ€»è®°å½•æ•°: {len(df)}")
        print(f"æœ‰åŒ»é™¢ä¿¡æ¯çš„è®°å½•: {len(df[df['åŒ»é™¢'].notna() & (df['åŒ»é™¢'] != '')])}")
        print(f"æœ‰çœä»½ä¿¡æ¯çš„è®°å½•: {len(df[df['çœä»½'].notna() & (df['çœä»½'] != '')])}")
        print(f"æœ‰äººåä¿¡æ¯çš„è®°å½•: {len(df[df['å§“å'].notna() & (df['å§“å'] != '')])}")
        
        print(f"\n=== å‰5æ¡è®°å½•é¢„è§ˆ ===")
        for i, row in df.head().iterrows():
            print(f"è®°å½• {i+1}:")
            print(f"  æ ‡é¢˜: {row['æ ‡é¢˜']}")
            print(f"  å§“å: {row['å§“å']}")
            print(f"  çœä»½: {row['çœä»½']}")
            print(f"  åŒ»é™¢: {row['åŒ»é™¢']}")
            print(f"  èŒä½: {row['èŒä½']}")
            print(f"  æè¿°: {row['Description'][:100]}...")
            print()
    else:
        print("æ— æœ‰æ•ˆæ•°æ®")

if __name__ == "__main__":
    main()