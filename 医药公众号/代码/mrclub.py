#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
import json
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from threading import Lock
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('mrclub_spider.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# å…¨å±€é”å’Œè®¡æ•°å™¨
results_lock = Lock()
progress_lock = Lock()
processed_count = 0
matched_count = 0

# é‡ç‚¹å…³é”®è¯ - ä¸“æ³¨è¿æ³•çŠ¯ç½ªè¢«æŠ“è¢«æŸ¥
VIOLATION_KEYWORDS = [
    # è¿æ³•çŠ¯ç½ªæ ¸å¿ƒè¯æ±‡
    "è¢«æŸ¥", "è¢«æŠ“", "è¢«æ•", "è¢«æ‹˜", "è½é©¬", "åŒå¼€", "è¢«å¼€é™¤", "è¢«å…èŒ", "è¢«æ’¤èŒ", "è¢«å¤„åˆ†",
    "æ¶‰å«Œ", "è¿æ³•", "è¿çºª", "çŠ¯ç½ª", "è…è´¥", "è´ªæ±¡", "å—è´¿", "è¡Œè´¿", "æŒªç”¨", "æ»¥ç”¨èŒæƒ",
    "ä¸»åŠ¨æŠ•æ¡ˆ", "è‡ªé¦–", "è¢«è¯‰", "è¢«åˆ¤", "è¢«ç«‹æ¡ˆ", "è¢«è°ƒæŸ¥", "çºªæ£€ç›‘å¯Ÿ", "ç›‘å§”", "çºªå§”",
    
    # åŒ»ç–—ç›¸å…³è¿æ³•
    "åŒ»ç–—è…è´¥", "åŒ»è¯è…è´¥", "å›æ‰£", "çº¢åŒ…", "éª—ä¿", "å¥—ä¿", "è™šå¼€å‘ç¥¨", "è¯å“è¿æ³•",
    "åŒ»ä¿è¿æ³•", "æ”¶å—è´¿èµ‚", "æ»¥å¼€è¯å“", "è¿‡åº¦åŒ»ç–—", "åŒ»ç–—æ¬ºè¯ˆ", "è¯å“å›æ‰£",
    
    # å¤„ç½šæªæ–½
    "ä¸¥é‡è¿çºªè¿æ³•", "å¼€é™¤å…šç±", "å¼€é™¤å…¬èŒ", "ç§»é€å¸æ³•æœºå…³", "å…šçºªæ”¿åŠ¡å¤„åˆ†",
    "è­¦å‘Š", "è®°è¿‡", "è®°å¤§è¿‡", "é™çº§", "æ’¤èŒ", "ç•™å…šå¯Ÿçœ‹", "å–æ¶ˆèµ„æ ¼",
    
    # è°ƒæŸ¥ç›¸å…³
    "çºªå¾‹å®¡æŸ¥", "ç›‘å¯Ÿè°ƒæŸ¥", "ç«‹æ¡ˆå®¡æŸ¥", "å®¡æŸ¥è°ƒæŸ¥", "æ¥å—è°ƒæŸ¥", "é…åˆè°ƒæŸ¥"
]

# MRCLUBå…¬ä¼—å·é…ç½® - ä½¿ç”¨æœ€æ–°çš„è®¤è¯ä¿¡æ¯
GZH_LIST = [
    {
        "name": "MRCLUB",
        "token": "840103440",
        "cookie": "RK=DC2Uq4Wf9P; ptcz=c9f4dcf0c0fb279d2316b228ce1d2d7a6b107f591ae8bbce0eac0ce98bc9de36; wxuin=51340895845860; mm_lang=zh_CN; _hp2_id.1405110977=%7B%22userId%22%3A%226801447023479475%22%2C%22pageviewId%22%3A%228306667787246811%22%2C%22sessionId%22%3A%224504468753015668%22%2C%22identity%22%3Anull%2C%22trackerVersion%22%3A%224.0%22%7D; ua_id=mxGDXOVuOo8d0D2hAAAAACdqUxp53FqemlDjGf2eSLM=; rewardsn=; wxtokenkey=777; poc_sid=HBg3iGijlmGc_2ocHEPN26JgrEcR59UETkMwwy7P; _clck=3911425635|1|fy3|0; uuid=0f23747e8a4ce4803ac4c2e81813d9c3; rand_info=CAESICRG+nL2+PnQWtbjYd6JuRPOT89alJ8x3l0VMgP1oYnS; slave_bizuin=3911425635; data_bizuin=3911425635; bizuin=3911425635; data_ticket=0s37cmjlBOpA+6yyl1Vmc2ZL5TY2yMPaZb8t5y2aenlcBIOvu0qMjhWGtWAn5OiS; slave_sid=eFlBWmtDMTdWZUJGMFRpUUNyRGp6TFEyd2czYTRSa1NQY1RiWWtxQVY2dExTMDl4QVhGNHZGamZwUzJ1djJlelpnTjlTcmdsNGVFaXBWSXNnVDJvcEdQUGJoMWFQTzU1MzVlbnpXRXdLQWk5a2FQWWpwNXkyMk1sb0NDMGhPTkpjanF0N2dsVlR3b2prWkts; slave_user=gh_b3cdf815ccbf; xid=e437bfb7c69a0dd7d1b3c2fa393774e4; _clsk=1vy8tkg|1754033023118|3|1|mp.weixin.qq.com/weheat-agent/payload/record"
    }
]

# æ€§èƒ½é…ç½® - å¤§å¹…æå‡å¹¶å‘æ€§èƒ½
PERFORMANCE_CONFIG = {
    'max_workers': 12,     # å¢åŠ çº¿ç¨‹æ•°
    'timeout': 15,         # è¯·æ±‚è¶…æ—¶æ—¶é—´
    'page_delay': 0.3,     # å‡å°‘é¡µé¢å»¶è¿Ÿ
    'cache_size': 256,     # å¢åŠ ç¼“å­˜å¤§å°
    'batch_size': 50       # å¢åŠ æ‰¹æ¬¡å¤§å°ä»¥è·å–æ›´å¤šæ•°æ®
}

# åˆ›å»ºå…¨å±€ä¼šè¯
SESSION = requests.Session()
SESSION.headers.update({
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
})

@lru_cache(maxsize=PERFORMANCE_CONFIG['cache_size'])
def quick_violation_filter(title):
    """å¿«é€Ÿè¿æ³•å…³é”®è¯è¿‡æ»¤"""
    return any(keyword in title for keyword in VIOLATION_KEYWORDS)

def normalize_date_format(date_str):
    """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º xxxx/xx/xx"""
    if not date_str:
        return ''
    
    date_str = str(date_str).strip()
    
    # å¦‚æœå·²ç»æ˜¯ç›®æ ‡æ ¼å¼
    if re.match(r'^\d{4}/\d{2}/\d{2}$', date_str):
        return date_str
    
    # å¤„ç†æ—¶é—´æˆ³
    if isinstance(date_str, (int, float)) or date_str.isdigit():
        timestamp = int(date_str)
        return time.strftime("%Y/%m/%d", time.localtime(timestamp))
    
    # å¤„ç†å„ç§æ—¥æœŸæ ¼å¼
    date_patterns = [
        (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥?', r'\1/\2/\3'),
        (r'(\d{4})-(\d{1,2})-(\d{1,2})', r'\1/\2/\3'),
        (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', r'\1/\2/\3'),
        (r'(\d{4})/(\d{1,2})/(\d{1,2})', r'\1/\2/\3'),
    ]
    
    for pattern, replacement in date_patterns:
        match = re.search(pattern, date_str)
        if match:
            year, month, day = match.groups()
            month = month.zfill(2)
            day = day.zfill(2)
            return f"{year}/{month}/{day}"
    
    return date_str

def extract_precise_names(text):
    """ç²¾ç¡®çš„å§“åæå–é€»è¾‘ - ä¸¥æ ¼ç²¾ç¡®ç‰ˆ"""
    names = []
    
    # å¸¸è§å§“æ°ï¼ˆä¿æŒåŸæœ‰ï¼‰
    common_surnames = {
        'ç‹', 'æ', 'å¼ ', 'åˆ˜', 'é™ˆ', 'æ¨', 'èµµ', 'é»„', 'å‘¨', 'å´',
        'å¾', 'å­™', 'èƒ¡', 'æœ±', 'é«˜', 'æ—', 'ä½•', 'éƒ­', 'é©¬', 'ç½—',
        'æ¢', 'å®‹', 'éƒ‘', 'è°¢', 'éŸ©', 'å”', 'å†¯', 'è‘£', 'è§', 'ç¨‹',
        'æ›¹', 'è¢', 'é‚“', 'è®¸', 'å‚…', 'æ²ˆ', 'æ›¾', 'å½­', 'å•', 'è‹',
        'å¢', 'è’‹', 'è”¡', 'è´¾', 'ä¸', 'é­', 'è–›', 'å¶', 'é˜', 'ä½™',
        'æ½˜', 'æœ', 'æˆ´', 'å¤', 'é’Ÿ', 'æ±ª', 'ç”°', 'å§œ', 'èŒƒ', 'æ–¹',
        'çŸ³', 'å§š', 'è°­', 'å»–', 'é‚¹', 'ç†Š', 'é‡‘', 'é™†', 'éƒ', 'å­”',
        'ç™½', 'å´”', 'åº·', 'æ¯›', 'é‚±', 'ç§¦', 'æ±Ÿ', 'å²', 'é¡¾', 'ä¾¯',
        'é‚µ', 'å­Ÿ', 'é¾™', 'ä¸‡', 'æ®µ', 'é’±', 'æ±¤', 'å°¹', 'é»', 'æ˜“',
        'å¸¸', 'æ­¦', 'ä¹”', 'è´º', 'èµ–', 'é¾š', 'æ–‡', 'åº', 'æ¨Š', 'å…°',
        'æ®·', 'æ–½', 'é™¶', 'æ´ª', 'ç¿Ÿ', 'å®‰', 'é¢œ', 'å€ª', 'ä¸¥', 'ç‰›',
        'æ¸©', 'å­£', 'ä¿', 'ç« ', 'é²', 'è‘›', 'ä¼', 'éŸ¦', 'ç”³', 'å°¤',
        'æ¯•', 'è‚', 'ç„¦', 'å‘', 'æŸ³', 'é‚¢', 'è·¯', 'å²³', 'é½', 'æ¢…',
        'è«', 'åº„', 'è¾›', 'ç®¡', 'ç¥', 'å·¦', 'æ¶‚', 'è°·', 'ç¥', 'æ—¶',
        'èˆ’', 'è€¿', 'ç‰Ÿ', 'åœ', 'è©¹', 'å…³', 'è‹—', 'å‡Œ', 'è´¹', 'çºª',
        'é³', 'ç››', 'ç«¥', 'æ¬§', 'ç”„', 'é¡¹', 'æ›²', 'æˆ', 'æ¸¸', 'é˜³',
        'è£´', 'å¸­', 'å«', 'æŸ¥', 'å±ˆ', 'é²', 'è¦ƒ', 'éœ', 'ç¿', 'éš‹',
        'ç”˜', 'æ™¯', 'è–„', 'å•', 'åŒ…', 'æŸ', 'å®', 'æŸ¯', 'é˜®', 'æ¡‚'
    }
    
    # å¤§å¹…æ‰©å±•ç¦æ­¢è¯æ±‡ - ä¸¥æ ¼è¿‡æ»¤
    invalid_words = {
        # åŸºç¡€ç¦æ­¢è¯
        'MRCLUB', 'èµ›æŸè“', 'åŒ»é™¢', 'å§”å‘˜ä¼š', 'ç®¡ç†å±€', 'ç›‘ç£å±€',
        'è¢«æŸ¥', 'è¢«æŠ“', 'è½é©¬', 'åŒå¼€', 'æ¶‰å«Œ', 'è¿æ³•', 'è¿çºª', 'çŠ¯ç½ª',
        'è´ªæ±¡', 'è…è´¥', 'å—è´¿', 'å¤„åˆ†', 'å…èŒ', 'æ’¤èŒ', 'å¼€é™¤',
        'è°ƒæŸ¥', 'å®¡æŸ¥', 'ç«‹æ¡ˆ', 'èµ·è¯‰', 'åˆ¤å†³', 'é€®æ•', 'æ‹˜ç•™',
        
        # èŒä½ç›¸å…³ç¦æ­¢è¯
        'è‘£äº‹é•¿', 'æ€»ç»ç†', 'å‰¯æ€»', 'é«˜ç®¡', 'ç»ç†', 'ä¸»ç®¡', 'åŠ©ç†',
        'åŸé™¢é•¿', 'å‰é™¢é•¿', 'æ—¶ä»»', 'ç°ä»»', 'åŸä»»', 'æ›¾ä»»',
        'é™¢é•¿', 'å‰¯é™¢é•¿', 'ä¸»ä»»', 'å‰¯ä¸»ä»»', 'ä¹¦è®°', 'å§”å‘˜', 'å±€é•¿', 'å‰¯å±€é•¿',
        
        # æ˜æ˜¾é”™è¯¯çš„ç»„åˆè¯
        'é«˜ç®¡åŠ ', 'åº·ä¸­å›½', 'çºªè¿æ³•', 'å«å¥å§”', 'æ–¹ç»“æœ', 'æå¥',
        'ç®¡åŠ ', 'ä¸­å›½', 'è¿æ³•', 'å¥å§”', 'ç»“æœ', 'æŸæŸ', 'æŸäºº',
        
        # åœ°åå’Œæœºæ„å
        'åŒ—äº¬', 'ä¸Šæµ·', 'å¤©æ´¥', 'é‡åº†', 'å®‰å¾½', 'å±±ä¸œ', 'æ±Ÿè‹', 'æµ™æ±Ÿ',
        'å¹¿ä¸œ', 'æ¹–å—', 'æ¹–åŒ—', 'æ²³å—', 'æ²³åŒ—', 'å››å·', 'äº‘å—', 'è´µå·',
        'åŒ»è¯', 'è¯å“', 'åŒ»ç–—', 'å«ç”Ÿ', 'å¥åº·', 'ä¿å¥', 'è¯ç›‘',
        
        # å…¶ä»–æ— æ•ˆè¯æ±‡
        'æœ‰å…³', 'ç›¸å…³', 'ç­‰äºº', 'ç­‰ç­‰', 'é€šæŠ¥', 'å…¬å¸ƒ', 'æ¶ˆæ¯',
        'ä¸¥é‡', 'çºªå§”', 'ç›‘å§”', 'å¤„é•¿', 'ç§‘é•¿', 'ä¸»æ²»', 'æŠ¤å£«',
        'è¯å¸ˆ', 'æŠ€å¸ˆ', 'æ£€éªŒ', 'å½±åƒ', 'ä¸´åºŠ', 'é—¨è¯Š'
    }
    
    # æ— æ•ˆçš„ç»“å°¾å­—ç¬¦
    invalid_endings = {'è¢«', 'ä¹Ÿ', 'è¿˜', 'åˆ', 'å°±', 'éƒ½', 'å´', 'ä½†', 'è€Œ', 'åˆ™', 'å³', 'æ—¢', 'å·²', 'æ­£', 'åœ¨', 'åˆ°', 'ä»', 'å‘', 'äº', 'å¯¹', 'ä¸º', 'ä¸', 'å’Œ', 'æˆ–', 'åŠ', 'ä»¥', 'å°†', 'ä¼š', 'è¦', 'èƒ½', 'å¯', 'åº”', 'è¯¥', 'å½“', 'è‹¥', 'å¦‚', 'å› ', 'ç”±', 'ç»', 'è¿‡', 'é€š', 'æ¥', 'å—', 'ç»™', 'è®©', 'ä½¿', 'ä»¤', 'å«', 'è¯´', 'è®²', 'å‘Š', 'æŠ¥', 'çŸ¥', 'é“', 'è§£', 'äº†', 'çš„', 'åœ°', 'å¾—'}
    
    # ç²¾ç¡®ä½†ä¸è¿‡äºä¸¥æ ¼çš„åŒ¹é…æ¨¡å¼
    position_name_patterns = [
        # åŸèŒä½ + å§“å + è¿æ³•å…³é”®è¯
        r'(åŸ|å‰|æ—¶ä»»|æ›¾ä»»)?\s*(ä¸»ä»»åŒ»å¸ˆ|å‰¯ä¸»ä»»åŒ»å¸ˆ|ä¸»æ²»åŒ»å¸ˆ|é™¢é•¿|å‰¯é™¢é•¿|å…šå§”ä¹¦è®°|çºªå§”ä¹¦è®°|ä¸»ä»»|å‰¯ä¸»ä»»|å±€é•¿|å‰¯å±€é•¿|å¤„é•¿|å‰¯å¤„é•¿|ç§‘é•¿|å‰¯ç§‘é•¿|ä¸»å¸­|å‰¯ä¸»å¸­|è‘£äº‹é•¿|æ€»ç»ç†|å‰¯æ€»ç»ç†)\s*([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯è‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡åºæ¨Šå…°æ®·æ–½é™¶æ´ªç¿Ÿå®‰é¢œå€ªä¸¥ç‰›æ¸©å­£ä¿ç« é²è‘›ä¼éŸ¦ç”³å°¤æ¯•è‚ç„¦å‘æŸ³é‚¢è·¯å²³é½æ¢…è«åº„è¾›ç®¡ç¥å·¦æ¶‚è°·ç¥æ—¶èˆ’è€¿ç‰Ÿåœè©¹å…³è‹—å‡Œè´¹çºªé³ç››ç«¥æ¬§ç”„é¡¹æ›²æˆæ¸¸é˜³è£´å¸­å«æŸ¥å±ˆé²è¦ƒéœç¿éš‹ç”˜æ™¯è–„å•åŒ…æŸå®æŸ¯é˜®æ¡‚][\u4e00-\u9fff]{1,2})\s*(?=è¢«æŸ¥|è¢«æŠ“|è¢«åŒå¼€|è¢«å…èŒ|è¢«æ’¤èŒ|è¢«å¤„åˆ†|æ¶‰å«Œ|æ¥å—.*?è°ƒæŸ¥|ä¸¥é‡è¿çºª|ä¸¥é‡è¿æ³•)',
        
        # å§“å + èŒä½ + è¿æ³•å…³é”®è¯  
        r'([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯è‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡åºæ¨Šå…°æ®·æ–½é™¶æ´ªç¿Ÿå®‰é¢œå€ªä¸¥ç‰›æ¸©å­£ä¿ç« é²è‘›ä¼éŸ¦ç”³å°¤æ¯•è‚ç„¦å‘æŸ³é‚¢è·¯å²³é½æ¢…è«åº„è¾›ç®¡ç¥å·¦æ¶‚è°·ç¥æ—¶èˆ’è€¿ç‰Ÿåœè©¹å…³è‹—å‡Œè´¹çºªé³ç››ç«¥æ¬§ç”„é¡¹æ›²æˆæ¸¸é˜³è£´å¸­å«æŸ¥å±ˆé²è¦ƒéœç¿éš‹ç”˜æ™¯è–„å•åŒ…æŸå®æŸ¯é˜®æ¡‚][\u4e00-\u9fff]{1,2})\s*(?:ï¼Œ|,)?\s*(ä¸»ä»»åŒ»å¸ˆ|å‰¯ä¸»ä»»åŒ»å¸ˆ|ä¸»æ²»åŒ»å¸ˆ|é™¢é•¿|å‰¯é™¢é•¿|å…šå§”ä¹¦è®°|çºªå§”ä¹¦è®°|ä¸»ä»»|å‰¯ä¸»ä»»|å±€é•¿|å‰¯å±€é•¿|å¤„é•¿|å‰¯å¤„é•¿|ç§‘é•¿|å‰¯ç§‘é•¿|ä¸»å¸­|å‰¯ä¸»å¸­|è‘£äº‹é•¿|æ€»ç»ç†|å‰¯æ€»ç»ç†)\s*(?=è¢«æŸ¥|è¢«æŠ“|è¢«åŒå¼€|è¢«å…èŒ|è¢«æ’¤èŒ|è¢«å¤„åˆ†|æ¶‰å«Œ|æ¥å—.*?è°ƒæŸ¥|ä¸¥é‡è¿çºª|ä¸¥é‡è¿æ³•)',
        
        # ç²¾ç¡®çš„ä¸Šä¸‹æ–‡å§“ååŒ¹é…
        r'(?:è¿æ³•|è¿çºª|æ¶‰å«Œ|è¢«æŸ¥|è¢«æŠ“|è¢«è°ƒæŸ¥|æ¥å—è°ƒæŸ¥)[^ã€‚]{0,20}([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯è‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡åºæ¨Šå…°æ®·æ–½é™¶æ´ªç¿Ÿå®‰é¢œå€ªä¸¥ç‰›æ¸©å­£ä¿ç« é²è‘›ä¼éŸ¦ç”³å°¤æ¯•è‚ç„¦å‘æŸ³é‚¢è·¯å²³é½æ¢…è«åº„è¾›ç®¡ç¥å·¦æ¶‚è°·ç¥æ—¶èˆ’è€¿ç‰Ÿåœè©¹å…³è‹—å‡Œè´¹çºªé³ç››ç«¥æ¬§ç”„é¡¹æ›²æˆæ¸¸é˜³è£´å¸­å«æŸ¥å±ˆé²è¦ƒéœç¿éš‹ç”˜æ™¯è–„å•åŒ…æŸå®æŸ¯é˜®æ¡‚][\u4e00-\u9fff]{1,2})',
        
        # åå‘åŒ¹é…ï¼šå§“ååœ¨è¿æ³•å…³é”®è¯å‰é¢
        r'([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯è‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡åºæ¨Šå…°æ®·æ–½é™¶æ´ªç¿Ÿå®‰é¢œå€ªä¸¥ç‰›æ¸©å­£ä¿ç« é²è‘›ä¼éŸ¦ç”³å°¤æ¯•è‚ç„¦å‘æŸ³é‚¢è·¯å²³é½æ¢…è«åº„è¾›ç®¡ç¥å·¦æ¶‚è°·ç¥æ—¶èˆ’è€¿ç‰Ÿåœè©¹å…³è‹—å‡Œè´¹çºªé³ç››ç«¥æ¬§ç”„é¡¹æ›²æˆæ¸¸é˜³è£´å¸­å«æŸ¥å±ˆé²è¦ƒéœç¿éš‹ç”˜æ™¯è–„å•åŒ…æŸå®æŸ¯é˜®æ¡‚][\u4e00-\u9fff]{1,2})[^ã€‚]{0,20}(?=è¢«æŸ¥|è¢«æŠ“|è¢«åŒå¼€|è¢«å…èŒ|è¢«æ’¤èŒ|è¢«å¤„åˆ†|æ¶‰å«Œ|æ¥å—.*?è°ƒæŸ¥|ä¸¥é‡è¿çºª|ä¸¥é‡è¿æ³•)'
    ]
    
    for pattern in position_name_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            # æå–å§“åéƒ¨åˆ†
            name_candidate = None
            if isinstance(match, tuple):
                # æ ¹æ®åŒ¹é…ç»„æ•°ç¡®å®šå§“åä½ç½®
                if len(match) == 4:  # åŸèŒä½ + èŒä½ + å§“å
                    name_candidate = match[2]
                elif len(match) == 3:  # èŒä½ + å§“å æˆ– åŸ+èŒä½+å§“å
                    name_candidate = match[2]
                elif len(match) == 2:  # å§“å + èŒä½
                    name_candidate = match[0]
                elif len(match) == 1:  # å•ä¸ªå§“å
                    name_candidate = match[0]
            elif isinstance(match, str):
                name_candidate = match
            
            if (name_candidate and
                2 <= len(name_candidate) <= 3 and 
                name_candidate not in invalid_words and
                name_candidate[0] in common_surnames and
                all('\u4e00' <= c <= '\u9fff' for c in name_candidate) and
                name_candidate[-1] not in invalid_endings and
                                    # ä¸¥æ ¼éªŒè¯ï¼šä¸èƒ½åŒ…å«ä»»ä½•èŒä½ã€è¿æ³•å…³é”®è¯ï¼ˆç§»é™¤å¸¸è§åå­—ç”¨å­—ï¼‰
                    not any(bad in name_candidate for bad in ['é™¢é•¿', 'ä¸»ä»»', 'ä¹¦è®°', 'å§”å‘˜', 'å±€é•¿', 'å¤„é•¿', 'ç§‘é•¿', 'è¢«', 'ä¹Ÿ', 'æ¥', 'æŸ¥', 'åŒ»', 'è¯', 'ç®¡', 'ç†', 'å«']) and
                # éªŒè¯å§“åçš„åˆç†æ€§
                is_valid_chinese_name(name_candidate)):
                names.append(name_candidate)
    
    # ç§»é™¤æ‰€æœ‰ä¸ç²¾ç¡®çš„åŒ¹é…æ–¹æ³•ï¼Œåªä¿ç•™æœ€å¯é çš„
    
    # å»é‡å¹¶éªŒè¯
    valid_names = []
    for name in set(names):
        if (len(name) >= 2 and len(name) <= 4 and
            name[0] in common_surnames and
            name not in invalid_words and
            name[-1] not in invalid_endings and
            all('\u4e00' <= c <= '\u9fff' for c in name)):
            valid_names.append(name)
    
    return valid_names

def is_valid_chinese_name(name):
    """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä¸­æ–‡å§“å"""
    if not name or len(name) < 2 or len(name) > 3:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„éå§“åå­—ç¬¦ (ç§»é™¤å¸¸è§åå­—ç”¨å­—)
    invalid_chars = {'åŠ ', 'ç†', 'ç®¡', 'å«', 'åŒ»', 'è¯', 'é™¢', 'å§”', 'ä¼š', 'å±€', 'å¤„', 'ç§‘', 'æ³•', 'æŸ¥', 'è¢«', 'æŠ“', 'å®¡', 'è°ƒ', 'è¿', 'çºª', 'æŸ', 'ç­‰', 'ä¹‹', 'å…¶', 'æœ‰', 'æ— ', 'æ­¤', 'è¯¥'}
    if any(char in name for char in invalid_chars):
        return False
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§çš„é”™è¯¯ç»„åˆ
    error_combinations = ['é«˜ç®¡', 'ç®¡åŠ ', 'æ–¹ç»“', 'ç»“æœ', 'åº·ä¸­', 'ä¸­å›½', 'çºªè¿', 'è¿æ³•', 'å«å¥', 'å¥å§”', 'ä¸»ä»»', 'é™¢é•¿', 'å±€é•¿', 'å¤„é•¿', 'ç§‘é•¿', 'ä¹¦è®°', 'å§”å‘˜', 'ä¸¥é‡']
    if any(combo in name for combo in error_combinations):
        return False
    
    # ç‰¹æ®Šæ£€æŸ¥ï¼šåŒ…å«"æŸ"çš„ä¸€å¾‹è¿‡æ»¤
    if 'æŸ' in name:
        return False
    
    return True

def extract_comprehensive_institutions(text):
    """ç²¾ç¡®çš„æœºæ„æå– - ä¸¥æ ¼è¾¹ç•Œè¯†åˆ«"""
    institutions = []
    
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„è¾¹ç•ŒåŒ¹é…
    precise_patterns = [
        # å®Œæ•´çš„æ”¿åºœæœºæ„åç§°ï¼ˆä»è¾¹ç•Œå¼€å§‹åŒ¹é…ï¼‰
        r'(?:^|[ã€‚ï¼ï¼Ÿï¼›\nï¼Œ,ã€])\s*([A-Z]*[\u4e00-\u9fff]{2,8}(?:çœ|å¸‚|å¿|åŒº)[\u4e00-\u9fff]{2,15}(?:å§”å‘˜ä¼š|ç®¡ç†å±€|ç›‘ç£å±€|è¯ç›‘å±€|åŒ»ä¿å±€|å«å¥å§”|çºªå§”ç›‘å§”|å«ç”Ÿå±€|å«ç”Ÿå…|äººæ°‘æ”¿åºœ))',
        
        # å®Œæ•´çš„åŒ»é™¢åç§°
        r'(?:^|[ã€‚ï¼ï¼Ÿï¼›\nï¼Œ,ã€])\s*([\u4e00-\u9fff]{2,15}(?:äººæ°‘åŒ»é™¢|ä¸­å¿ƒåŒ»é™¢|ä¸­åŒ»åŒ»?é™¢|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+åŒ»é™¢|å¦‡å¹¼ä¿å¥é™¢|å„¿ç«¥åŒ»é™¢|è‚¿ç˜¤åŒ»é™¢|ä¸“ç§‘åŒ»é™¢|æ€»åŒ»é™¢))',
        
        # å¤§å­¦é™„å±åŒ»é™¢
        r'(?:^|[ã€‚ï¼ï¼Ÿï¼›\nï¼Œ,ã€])\s*([\u4e00-\u9fff]{2,15}(?:åŒ»ç§‘å¤§å­¦|åŒ»å­¦é™¢|å¤§å­¦)é™„å±[\u4e00-\u9fff]{0,10}åŒ»é™¢)',
        
        # å›½å®¶çº§æœºæ„
        r'(?:^|[ã€‚ï¼ï¼Ÿï¼›\nï¼Œ,ã€])\s*(å›½å®¶[\u4e00-\u9fff]{2,15}(?:å§”å‘˜ä¼š|ç®¡ç†å±€|ç›‘ç£å±€|è¯ç›‘å±€|åŒ»ä¿å±€|å«å¥å§”))',
    ]
    
    # ä¸¥æ ¼çš„æ’é™¤è¯æ±‡
    strict_invalid_keywords = [
        'MRCLUB', 'èµ›æŸè“', 'è¢«æŸ¥', 'è¢«æŠ“', 'è½é©¬', 'åŒå¼€', 'æ¶‰å«Œ',
        'è¿æ³•', 'è¿çºª', 'çŠ¯ç½ª', 'è´ªæ±¡', 'è…è´¥', 'å—è´¿', 'å¤„åˆ†',
        'è°ƒæŸ¥', 'å®¡æŸ¥', 'ç«‹æ¡ˆ', 'èµ·è¯‰', 'åˆ¤å†³', 'é€®æ•', 'æ‹˜ç•™',
        'æŸæŸ', 'æœ‰å…³', 'ç›¸å…³', 'ç­‰ç­‰', 'æ—¶ä»»', 'åŸä»»', 'æ›¾ä»»', 'å‰ä»»',
        'æœˆæˆ‘åœ¨', 'é‡åŒ»ç”Ÿ', 'æˆ‘ä»¬ä¸»ä»»', 'å¸‚åŒ»å‰¯', 'å¸‚è¯', 'ä»»å±±ä¸œ'
    ]
    
    for pattern in precise_patterns:
        matches = re.findall(pattern, text)
        for institution in matches:
            # æ¸…ç†æœºæ„åç§°
            institution = institution.strip()
            
            # éªŒè¯æœºæ„åç§°çš„æœ‰æ•ˆæ€§
            if (4 <= len(institution) <= 30 and
                not any(invalid in institution for invalid in strict_invalid_keywords) and
                all('\u4e00' <= c <= '\u9fff' or c in '()ï¼ˆï¼‰ç¬¬ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å0123456789' for c in institution) and
                # å¿…é¡»ä»¥æœ‰æ•ˆçš„æœºæ„ç±»å‹ç»“å°¾
                any(institution.endswith(suffix) for suffix in ['åŒ»é™¢', 'å§”å‘˜ä¼š', 'ç®¡ç†å±€', 'ç›‘ç£å±€', 'è¯ç›‘å±€', 'åŒ»ä¿å±€', 'å«å¥å§”', 'çºªå§”ç›‘å§”', 'å«ç”Ÿå±€', 'å«ç”Ÿå…', 'äººæ°‘æ”¿åºœ']) and
                # ä¸èƒ½åŒ…å«æ˜æ˜¾çš„é”™è¯¯æ¨¡å¼
                not any(error in institution for error in ['å‰¯å±€é•¿', 'ä¸»ä»»ä¸»', 'é™¢é•¿é™¢', 'è¢«æŸ¥', 'è¿æ³•'])):
                institutions.append(institution)
    
    if institutions:
        # è¿”å›æœ€é•¿æœ€å®Œæ•´çš„æœºæ„åç§°
        best_institution = max(institutions, key=lambda x: (
            # ä¼˜å…ˆçº§ï¼šæ”¿åºœæœºæ„ > åŒ»é™¢ > å…¶ä»–
            100 if any(keyword in x for keyword in ['å§”å‘˜ä¼š', 'ç®¡ç†å±€', 'ç›‘ç£å±€', 'è¯ç›‘å±€', 'åŒ»ä¿å±€', 'å«å¥å§”', 'çºªå§”ç›‘å§”']) else
            50 if 'åŒ»é™¢' in x else 10,
            # é•¿åº¦å¥–åŠ±
            len(x)
        ))
        return best_institution
    
    return ""

def extract_comprehensive_positions(text, person_name=""):
    """å…¨é¢çš„èŒä½æå–"""
    if not text:
        return ""
    
    # åŒ»ç–—èŒä½
    medical_positions = [
        'ä¸»ä»»åŒ»å¸ˆ', 'å‰¯ä¸»ä»»åŒ»å¸ˆ', 'ä¸»æ²»åŒ»å¸ˆ', 'ä½é™¢åŒ»å¸ˆ', 'åŒ»å¸ˆ', 'åŒ»ç”Ÿ',
        'æŠ¤å£«é•¿', 'ä¸»ç®¡æŠ¤å¸ˆ', 'æŠ¤å¸ˆ', 'æŠ¤å£«',
        'è¯å¸ˆ', 'ä¸»ç®¡è¯å¸ˆ', 'è¯å‰‚å¸ˆ', 'ä¸´åºŠè¯å¸ˆ',
        'æŠ€å¸ˆ', 'ä¸»ç®¡æŠ€å¸ˆ', 'æ£€éªŒå¸ˆ', 'å½±åƒå¸ˆ'
    ]
    
    # è¡Œæ”¿èŒä½
    admin_positions = [
        'é™¢é•¿', 'å‰¯é™¢é•¿', 'å…šå§”ä¹¦è®°', 'çºªå§”ä¹¦è®°', 'é™¢é•¿åŠ©ç†',
        'ä¸»ä»»', 'å‰¯ä¸»ä»»', 'ä¸»ä»»å§”å‘˜', 'å§”å‘˜',
        'å±€é•¿', 'å‰¯å±€é•¿', 'å…é•¿', 'å‰¯å…é•¿',
        'å¤„é•¿', 'å‰¯å¤„é•¿', 'ç§‘é•¿', 'å‰¯ç§‘é•¿',
        'éƒ¨é•¿', 'å‰¯éƒ¨é•¿', 'å¸é•¿', 'å‰¯å¸é•¿',
        'ä¸»å¸­', 'å‰¯ä¸»å¸­', 'ç§˜ä¹¦é•¿', 'å‰¯ç§˜ä¹¦é•¿'
    ]
    
    # ä¼ä¸šèŒä½
    business_positions = [
        'è‘£äº‹é•¿', 'å‰¯è‘£äº‹é•¿', 'æ€»ç»ç†', 'å‰¯æ€»ç»ç†',
        'æ€»è£', 'å‰¯æ€»è£', 'æ€»ç›‘', 'å‰¯æ€»ç›‘',
        'ç»ç†', 'å‰¯ç»ç†', 'ä¸»ç®¡', 'æ€»åŠ©'
    ]
    
    # åˆå¹¶æ‰€æœ‰èŒä½
    all_positions = medical_positions + admin_positions + business_positions
    
    # å¦‚æœæŒ‡å®šäº†äººåï¼Œä¼˜å…ˆåœ¨äººåé™„è¿‘æŸ¥æ‰¾
    if person_name:
        context_sentences = []
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
        for sentence in sentences:
            if person_name in sentence:
                context_sentences.append(sentence)
        
        if context_sentences:
            context_text = 'ã€‚'.join(context_sentences)
            # åœ¨ä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾èŒä½
            for position in all_positions:
                if position in context_text:
                    return position
    
    # å…¨æ–‡æŸ¥æ‰¾èŒä½
    found_positions = []
    for position in all_positions:
        if position in text:
            found_positions.append(position)
    
    if found_positions:
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šåŒ»ç–—èŒä½ > è¡Œæ”¿èŒä½ > ä¼ä¸šèŒä½
        for position_list in [medical_positions, admin_positions, business_positions]:
            for position in found_positions:
                if position in position_list:
                    return position
    
    return ""

def extract_province(text):
    """æå–çœä»½ä¿¡æ¯"""
    provinces = [
        'åŒ—äº¬', 'å¤©æ´¥', 'ä¸Šæµ·', 'é‡åº†', 'æ²³åŒ—', 'å±±è¥¿', 'è¾½å®', 'å‰æ—', 'é»‘é¾™æ±Ÿ',
        'æ±Ÿè‹', 'æµ™æ±Ÿ', 'å®‰å¾½', 'ç¦å»º', 'æ±Ÿè¥¿', 'å±±ä¸œ', 'æ²³å—', 'æ¹–åŒ—', 'æ¹–å—',
        'å¹¿ä¸œ', 'æµ·å—', 'å››å·', 'è´µå·', 'äº‘å—', 'é™•è¥¿', 'ç”˜è‚ƒ', 'é’æµ·', 'å°æ¹¾',
        'å†…è’™å¤', 'å¹¿è¥¿', 'è¥¿è—', 'å®å¤', 'æ–°ç–†', 'é¦™æ¸¯', 'æ¾³é—¨'
    ]
    
    for province in provinces:
        if province in text:
            return province
    
    return ""

def clean_and_deduplicate_results(results):
    """æ¸…æ´—å’Œå»é‡è¿æ³•äº‹ä»¶æ•°æ®"""
    if not results:
        return []
    
    cleaned_results = []
    seen_combinations = set()
    
    for result in results:
        # æ•°æ®æ¸…æ´—
        cleaned_result = {}
        for key, value in result.items():
            # æ¸…ç†å­—ç¬¦ä¸²å€¼
            if isinstance(value, str):
                value = value.strip()
                # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                value = re.sub(r'\s+', ' ', value)
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                value = re.sub(r'[^\u4e00-\u9fff\w\s\-.,()ï¼ˆï¼‰ï¼š:ï¼›;ï¼!ï¼Ÿ?ã€‚ã€/]', '', value)
            cleaned_result[key] = value
        
        # å»é‡æ£€æŸ¥ - ä½¿ç”¨å¤šä¸ªå­—æ®µç»„åˆ
        unique_key = f"{cleaned_result.get('å§“å', '')}_{cleaned_result.get('æœºæ„', '')}_{cleaned_result.get('èŒä½', '')}_{cleaned_result.get('Description', '')[:100]}"
        
        # é¢å¤–çš„ç›¸ä¼¼æ€§æ£€æŸ¥
        is_duplicate = False
        for seen_key in seen_combinations:
            # æ£€æŸ¥å§“åå’Œæè¿°æ˜¯å¦é«˜åº¦ç›¸ä¼¼
            if (cleaned_result.get('å§“å') and 
                cleaned_result.get('å§“å') in seen_key and 
                len(cleaned_result.get('Description', '')) > 20):
                # è®¡ç®—æè¿°ç›¸ä¼¼åº¦
                desc1 = cleaned_result.get('Description', '')[:200]
                desc2 = [k for k in seen_combinations if cleaned_result.get('å§“å') in k]
                if desc2:
                    desc2 = desc2[0].split('_')[-1] if '_' in desc2[0] else ''
                    if desc1 and desc2:
                        # ç®€å•çš„é‡å æ£€æŸ¥
                        overlap = len(set(desc1) & set(desc2)) / max(len(set(desc1)), len(set(desc2)), 1)
                        if overlap > 0.7:  # 70%é‡å è®¤ä¸ºæ˜¯é‡å¤
                            is_duplicate = True
                            break
        
        if not is_duplicate and unique_key not in seen_combinations:
            # éªŒè¯æ•°æ®è´¨é‡
            if (cleaned_result.get('å§“å') or 
                (cleaned_result.get('æœºæ„') and len(cleaned_result.get('æœºæ„', '')) > 3) or
                len(cleaned_result.get('Description', '')) > 30):
                
                seen_combinations.add(unique_key)
                cleaned_results.append(cleaned_result)
    
    logging.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼šåŸå§‹ {len(results)} æ¡ -> æ¸…æ´—å {len(cleaned_results)} æ¡")
    return cleaned_results

def extract_violation_description(text, person_name=""):
    """æå–è¿æ³•äº‹ä»¶æè¿°"""
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
    relevant_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 15 or len(sentence) > 300:
            continue
        
        # å¿…é¡»åŒ…å«è¿æ³•å…³é”®è¯
        has_violation_keyword = any(keyword in sentence for keyword in VIOLATION_KEYWORDS)
        
        # å¦‚æœæŒ‡å®šäº†äººåï¼Œä¼˜å…ˆé€‰æ‹©åŒ…å«è¯¥äººåçš„å¥å­
        has_person_name = not person_name or person_name in sentence
        
        # æ’é™¤æ— æ„ä¹‰çš„å¥å­
        invalid_sentence_patterns = [
            r'^\s*$',  # ç©ºå¥å­
            r'^.*ç¼–è¾‘.*$', r'^.*ä½œè€….*$', r'^.*æ¥æº.*$', r'^.*è½¬è½½.*$',
            r'^.*å…³æ³¨.*$', r'^.*ç‚¹å‡».*$', r'^.*æ‰«ç .*$', r'^.*å¾®ä¿¡.*$',
            r'^.*å£°æ˜.*$', r'^.*å…è´£.*$', r'^.*ç‰ˆæƒ.*$'
        ]
        
        is_invalid_sentence = any(re.match(pattern, sentence) for pattern in invalid_sentence_patterns)
        
        if has_violation_keyword and has_person_name and not is_invalid_sentence:
            # è®¡ç®—è¿æ³•å…³é”®è¯å¯†åº¦
            keyword_count = sum(1 for keyword in VIOLATION_KEYWORDS if keyword in sentence)
            
            # é¢å¤–åŠ åˆ†ï¼šåŒ…å«å…·ä½“è¿æ³•è¡Œä¸ºæè¿°
            specific_violation_keywords = [
                'åˆ©ç”¨èŒåŠ¡ä¾¿åˆ©', 'æ”¶å—è´¢ç‰©', 'éæ³•æ”¶å—', 'ç´¢è¦è´¢ç‰©', 'æŒªç”¨èµ„é‡‘',
                'æ»¥ç”¨èŒæƒ', 'ç©å¿½èŒå®ˆ', 'å¾‡ç§èˆå¼Š', 'ä»¥æƒè°‹ç§', 'æƒé’±äº¤æ˜“',
                'åŒ»ç–—è…è´¥', 'è¯å“å›æ‰£', 'åŒ»ä¿è¯ˆéª—', 'è™šå¼€å‘ç¥¨', 'å¥—å–èµ„é‡‘'
            ]
            specific_score = sum(1 for keyword in specific_violation_keywords if keyword in sentence)
            
            relevant_sentences.append({
                'sentence': sentence,
                'keyword_count': keyword_count,
                'specific_score': specific_score,
                'length': len(sentence)
            })
    
    if relevant_sentences:
        # é€‰æ‹©æœ€ä½³å¥å­ï¼šä¼˜å…ˆè€ƒè™‘å…·ä½“è¿æ³•è¡Œä¸ºï¼Œç„¶åæ˜¯å…³é”®è¯æ•°é‡ï¼Œæœ€åæ˜¯é•¿åº¦
        best_sentence = max(relevant_sentences, 
                          key=lambda s: (s['specific_score'], s['keyword_count'], min(s['length'], 200)))
        return best_sentence['sentence']
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ…å«äººåçš„å¥å­ï¼Œå°è¯•æ‰¾åˆ°åŒ…å«è¿æ³•å…³é”®è¯çš„å¥å­
    if person_name:
        for sentence in sentences:
            sentence = sentence.strip()
            if (15 <= len(sentence) <= 300 and 
                any(keyword in sentence for keyword in VIOLATION_KEYWORDS)):
                return sentence
    
    return ""

def get_article_content_optimized(link, session=None):
    """ä¼˜åŒ–çš„æ–‡ç« å†…å®¹è·å–"""
    if session is None:
        session = SESSION
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«é™åˆ¶
            time.sleep(random.uniform(0.5, 1.5))
            
            headers = {
                'referer': 'https://mp.weixin.qq.com/',
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'accept-language': 'zh-CN,zh;q=0.9',
                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
            }
            
            resp = session.get(link, headers=headers, timeout=PERFORMANCE_CONFIG['timeout'])
            
            if resp.status_code != 200:
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(1, 3))
                    continue
                return ''
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # å¤šç§æ–¹å¼è·å–å†…å®¹
            content_selectors = [
                {'id': 'js_content'},
                {'class_': 'rich_media_content'},
                {'class_': 'content'},
                {'tag': 'article'}
            ]
            
            content_div = None
            for selector in content_selectors:
                if 'id' in selector:
                    content_div = soup.find('div', {'id': selector['id']})
                elif 'class_' in selector:
                    content_div = soup.find('div', class_=selector['class_'])
                elif 'tag' in selector:
                    content_div = soup.find(selector['tag'])
                
                if content_div:
                    break
            
            if content_div:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in content_div(["script", "style"]):
                    script.decompose()
                
                text = content_div.get_text(separator='\n', strip=True)
                return text
            
            return ''
            
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(random.uniform(2, 5))
                continue
            return ''
    
    return ''

def process_single_article_optimized(article_data):
    """ä¼˜åŒ–çš„å•ç¯‡æ–‡ç« å¤„ç† - ç§»é™¤æ ‡é¢˜é¢„ç­›é€‰ï¼Œè·å–æ‰€æœ‰æ–‡ç« å†…å®¹"""
    global processed_count, matched_count
    
    try:
        article, gzh_name, create_time = article_data
        title = article.get("title", "")
        link = article.get("link", "")
        
        # è·å–æ–‡ç« å†…å®¹
        content = get_article_content_optimized(link)
        if not content or len(content) < 20:  # é™ä½å†…å®¹é•¿åº¦é™åˆ¶
            with progress_lock:
                processed_count += 1
            return None
        
        # åªè¿›è¡Œå†…å®¹ç­›é€‰ï¼Œä¸è¿›è¡Œæ ‡é¢˜é¢„ç­›é€‰
        if not any(keyword in content for keyword in VIOLATION_KEYWORDS):
            with progress_lock:
                processed_count += 1
            return None
        
        with progress_lock:
            processed_count += 1
            matched_count += 1
            if processed_count % 50 == 0:  # å¢åŠ ç»Ÿè®¡é¢‘ç‡
                logging.info(f"å·²å¤„ç† {processed_count} ç¯‡ï¼ŒåŒ¹é… {matched_count} ç¯‡")
        
        return {
            'title': title,
            'link': link,
            'content': content,
            'create_time': create_time
        }
        
    except Exception as e:
        with progress_lock:
            processed_count += 1
        return None

def extract_violation_events(article_info, gzh_name):
    """ä»æ–‡ç« ä¸­æå–è¿æ³•äº‹ä»¶ä¿¡æ¯"""
    title = article_info['title']
    link = article_info['link']
    content = article_info['content']
    create_time = article_info['create_time']
    
    # æ ‡å‡†åŒ–å‘å¸ƒæ—¶é—´
    formatted_time = normalize_date_format(create_time)
    
    # æå–åŸºæœ¬ä¿¡æ¯
    names = extract_precise_names(content)
    province = extract_province(content)
    institution = extract_comprehensive_institutions(content)
    
    results = []
    
    if names:
        # ä¸ºæ¯ä¸ªäººååˆ›å»ºè®°å½•
        for name in names:
            position = extract_comprehensive_positions(content, name)
            description = extract_violation_description(content, name)
            
            if description:  # åªæœ‰æœ‰æè¿°çš„æ‰è®°å½•
                result = {
                    "å‘å¸ƒæ—¶é—´": formatted_time,
                    "çœä»½": province,
                    "æœºæ„": institution,
                    "èŒä½": position,
                    "å§“å": name,
                    "Resource": gzh_name,
                    "Description": description,
                    "æ–‡ç« é“¾æ¥": link,
                    "æ ‡é¢˜": title
                }
                results.append(result)
    else:
        # å³ä½¿æ²¡æœ‰äººåï¼Œå¦‚æœæœ‰è¿æ³•äº‹ä»¶æè¿°ä¹Ÿè®°å½•
        description = extract_violation_description(content)
        if description:
            position = extract_comprehensive_positions(content)
            result = {
                "å‘å¸ƒæ—¶é—´": formatted_time,
                "çœä»½": province,
                "æœºæ„": institution,
                "èŒä½": position,
                "å§“å": "",
                "Resource": gzh_name,
                "Description": description,
                "æ–‡ç« é“¾æ¥": link,
                "æ ‡é¢˜": title
            }
            results.append(result)
    
    return results

def fetch_articles_batch_optimized(gzh, start_page=0, batch_size=30, page_size=10):
    """ä¼˜åŒ–çš„æ‰¹é‡æ–‡ç« è·å–"""
    batch_articles = []
    consecutive_empty_pages = 0
    max_consecutive_empty = 10  # å¢åŠ è¿ç»­ç©ºé¡µå®¹å¿åº¦
    
    end_page = start_page + batch_size
    logging.info(f"å¼€å§‹æŠ“å–ç¬¬{start_page+1}é¡µåˆ°ç¬¬{end_page}é¡µ...")
    
    for page in range(start_page, end_page):
        params = {
            'sub': 'list',
            'search_field': 'null',
            "begin": page * page_size,
            "count": page_size,
            'query': '',
            'fakeid': gzh['fakeid'],
            'type': '101_1',
            'free_publish_type': '1',
            'sub_action': 'list_ex',
            'fingerprint': 'a1075c4cfacf4c13a46dad10285e6122',
            'token': gzh['token'],
            'lang': 'zh_CN',
            'f': 'json',
            'ajax': '1',
        }
        
        headers = {
            'cookie': gzh['cookie'],
            'referer': f'https://mp.weixin.qq.com/cgi-bin/appmsg?t=media/appmsg_edit_v2&action=edit&isNew=1&type=77&createType=0&token={gzh["token"]}&lang=zh_CN&timestamp=1754033020095',
            'accept': '*/*',
            'accept-language': 'zh-CN,zh;q=0.9',
            'x-requested-with': 'XMLHttpRequest'
        }
        
        try:
            resp = SESSION.get("https://mp.weixin.qq.com/cgi-bin/appmsgpublish", 
                              params=params, headers=headers, 
                              timeout=PERFORMANCE_CONFIG['timeout'])
            
            if resp.status_code != 200:
                logging.warning(f"ç¬¬{page+1}é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    break
                continue
            
            result = resp.json()
            if result.get("base_resp", {}).get("ret") != 0:
                logging.warning(f"ç¬¬{page+1}é¡µAPIè¿”å›é”™è¯¯: {result.get('base_resp', {})}")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    break
                continue
            
            publish_page = result.get("publish_page")
            if not publish_page:
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    break
                continue
            
            articleList = json.loads(publish_page)
            articles = articleList.get("publish_list", [])
            
            if not articles:
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    break
                continue
            
            consecutive_empty_pages = 0
            batch_articles.extend(articles)
            logging.info(f"ç¬¬{page+1}é¡µè·å–åˆ°{len(articles)}ç¯‡æ–‡ç« ")
            
            time.sleep(PERFORMANCE_CONFIG['page_delay'])
            
        except Exception as e:
            logging.error(f"ç¬¬{page+1}é¡µæŠ“å–å¼‚å¸¸: {e}")
            consecutive_empty_pages += 1
            if consecutive_empty_pages >= max_consecutive_empty:
                break
            time.sleep(2)
    
    logging.info(f"æœ¬æ‰¹æ¬¡è·å–åˆ° {len(batch_articles)} ç¯‡æ–‡ç« ")
    return batch_articles, len(batch_articles) > 0

def filter_and_extract_optimized(articles, gzh_name, existing_results=None):
    """ä¼˜åŒ–çš„æ–‡ç« ç­›é€‰å’Œä¿¡æ¯æå– - å¤„ç†æ‰€æœ‰æ–‡ç« """
    global processed_count, matched_count
    processed_count = 0
    matched_count = 0
    
    results = existing_results or []
    
    # å‡†å¤‡æ‰€æœ‰æ–‡ç« ä»»åŠ¡ï¼Œä¸è¿›è¡Œæ ‡é¢˜é¢„ç­›é€‰
    article_tasks = []
    for article in articles:
        try:
            info = json.loads(article.get("publish_info", "{}"))
            if info.get("appmsgex"):
                title = info["appmsgex"][0].get("title", "")
                link = info["appmsgex"][0].get("link", "")
                create_time = info["appmsgex"][0].get("create_time")
                
                # ç§»é™¤æ ‡é¢˜é¢„ç­›é€‰ï¼Œå¤„ç†æ‰€æœ‰æ–‡ç« 
                article_tasks.append((
                    {"title": title, "link": link}, 
                    gzh_name, 
                    create_time
                ))
        except Exception:
            continue
    
    logging.info(f"å…± {len(article_tasks)} ç¯‡æ–‡ç« éœ€è¦å¤„ç†ï¼ˆå·²ç§»é™¤æ ‡é¢˜é¢„ç­›é€‰ï¼‰")
    
    # å¹¶å‘å¤„ç†æ–‡ç« 
    matched_articles = []
    
    with ThreadPoolExecutor(max_workers=PERFORMANCE_CONFIG['max_workers']) as executor:
        future_to_article = {
            executor.submit(process_single_article_optimized, task): task 
            for task in article_tasks
        }
        
        for future in as_completed(future_to_article):
            try:
                result = future.result()
                if result:
                    matched_articles.append(result)
            except Exception:
                continue
    
    logging.info(f"å†…å®¹ç­›é€‰ååŒ¹é…åˆ° {len(matched_articles)} ç¯‡ç›¸å…³æ–‡ç« ")
    
    # ä¸²è¡Œæå–ä¿¡æ¯é¿å…æ•°æ®ç«äº‰
    batch_violations = []
    
    for article_info in matched_articles:
        try:
            violations = extract_violation_events(article_info, gzh_name)
            batch_violations.extend(violations)
        except Exception as e:
            logging.error(f"ä¿¡æ¯æå–å¤±è´¥: {e}")
    
    # å¯¹æœ¬æ‰¹æ¬¡æ•°æ®è¿›è¡Œæ¸…æ´—å’Œå»é‡
    if batch_violations:
        cleaned_batch = clean_and_deduplicate_results(batch_violations)
        results.extend(cleaned_batch)
    
    # å¯¹å…¨é‡æ•°æ®è¿›è¡Œæœ€ç»ˆå»é‡
    if len(results) > 100:  # å½“æ•°æ®é‡è¾ƒå¤§æ—¶è¿›è¡Œå…¨é‡å»é‡
        results = clean_and_deduplicate_results(results)
    
    new_records = len(batch_violations) if batch_violations else 0
    final_records = len(cleaned_batch) if 'cleaned_batch' in locals() else 0
    logging.info(f"æœ¬æ‰¹æ¬¡æå– {new_records} æ¡ -> æ¸…æ´—å {final_records} æ¡ï¼Œç´¯è®¡ {len(results)} æ¡")
    
    return results

def get_fakeid(gzh_name, token, cookie):
    """è·å–å…¬ä¼—å·fakeid"""
    search_url = 'https://mp.weixin.qq.com/cgi-bin/searchbiz'
    params = {
        'action': 'search_biz',
        'token': token,
        'lang': 'zh_CN',
        'f': 'json',
        'ajax': '1',
        'random': random.random(),
        'query': gzh_name,
        'begin': 0,
        'count': 5
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Cookie': cookie
    }
    
    try:
        resp = requests.get(search_url, params=params, headers=headers, timeout=15)
        data = resp.json()
        if data.get('list'):
            return data['list'][0]['fakeid']
        else:
            logging.error(f"æœªæ‰¾åˆ°å…¬ä¼—å· {gzh_name} çš„fakeid")
            return None
    except Exception as e:
        logging.error(f"è·å–fakeidå¤±è´¥: {e}")
        return None

def save_progress_optimized(current_page, gzh_name, all_results):
    """ä¿å­˜è¿›åº¦å’Œæ•°æ®"""
    os.makedirs('å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ', exist_ok=True)
    
    # ä¿å­˜è¿›åº¦
    progress_info = {
        'last_page': current_page,
        'gzh_name': gzh_name,
        'total_records': len(all_results),
        'last_update': time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    progress_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /MRCLUB_çˆ¬å–è¿›åº¦.json'
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_info, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ•°æ®
    if all_results:
        df = pd.DataFrame(all_results)
        output_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /MRCLUBè¿æ³•äº‹ä»¶_ä¼˜åŒ–ç‰ˆ.xlsx'
        df.to_excel(output_file, index=False)
        logging.info(f"å·²ä¿å­˜ {len(all_results)} æ¡è®°å½•åˆ° {output_file}")

def load_progress_optimized():
    """åŠ è½½è¿›åº¦"""
    progress_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /MRCLUB_çˆ¬å–è¿›åº¦.json'
    data_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /MRCLUBè¿æ³•äº‹ä»¶_ä¼˜åŒ–ç‰ˆ.xlsx'
    
    start_page = 0
    existing_results = []
    
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_info = json.load(f)
            start_page = progress_info.get('last_page', 0)
            logging.info(f"å‘ç°è¿›åº¦æ–‡ä»¶ï¼šä¸Šæ¬¡çˆ¬å–åˆ°ç¬¬{start_page}é¡µ")
        except Exception as e:
            logging.error(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    if os.path.exists(data_file):
        try:
            existing_df = pd.read_excel(data_file)
            existing_results = existing_df.to_dict('records')
            logging.info(f"å‘ç°å·²æœ‰æ•°æ®æ–‡ä»¶ï¼ŒåŒ…å« {len(existing_results)} æ¡è®°å½•")
        except Exception as e:
            logging.error(f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
    
    return start_page, existing_results

def main():
    """ä¸»å‡½æ•°"""
    logging.info("ğŸš€ å¯åŠ¨MRCLUBè¿æ³•äº‹ä»¶çˆ¬è™«ä¼˜åŒ–ç‰ˆ")
    
    # åŠ è½½è¿›åº¦
    start_page, all_results = load_progress_optimized()
    
    if start_page > 0 or all_results:
        print(f"\nğŸ“‹ å‘ç°å†å²æ•°æ®:")
        print(f"   ä¸Šæ¬¡çˆ¬å–åˆ°ç¬¬{start_page}é¡µ")
        print(f"   å·²æœ‰{len(all_results)}æ¡è®°å½•")
        response = input("æ˜¯å¦ç»§ç»­ä¸Šæ¬¡è¿›åº¦ï¼Ÿ[y/n]: ").lower()
        
        if response != 'y':
            start_page = 0
            all_results = []
            logging.info("é‡æ–°å¼€å§‹çˆ¬å–")
        else:
            logging.info(f"ä»ç¬¬{start_page + 1}é¡µç»§ç»­çˆ¬å–")
    
    for gzh in GZH_LIST:
        logging.info(f"ğŸ¯ å¼€å§‹å¤„ç†å…¬ä¼—å·: {gzh['name']}")
        
        # è·å–fakeid
        fakeid = get_fakeid(gzh['name'], gzh['token'], gzh['cookie'])
        if not fakeid:
            continue
        gzh['fakeid'] = fakeid
        
        current_page = start_page
        max_pages = 1000
        batch_size = PERFORMANCE_CONFIG['batch_size']
        
        try:
            while current_page < max_pages:
                batch_num = current_page // batch_size + 1
                logging.info(f"ğŸ“¦ å¼€å§‹å¤„ç†ç¬¬{batch_num}æ‰¹æ¬¡ (ç¬¬{current_page+1}-{min(current_page+batch_size, max_pages)}é¡µ)")
                
                # æŠ“å–æ–‡ç« 
                batch_articles, success = fetch_articles_batch_optimized(
                    gzh, current_page, batch_size, page_size=10
                )
                
                if not success or not batch_articles:
                    logging.warning(f"ç¬¬{batch_num}æ‰¹æ¬¡æŠ“å–å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                # ç­›é€‰å’Œæå–
                all_results = filter_and_extract_optimized(
                    batch_articles, gzh['name'], all_results
                )
                
                # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”å’Œé¢„ä¼°å‰©ä½™æ—¶é—´
                progress_percent = (current_page + batch_size) / max_pages * 100
                if current_page > 0:
                    avg_time_per_batch = (time.time() - batch_start_time) if 'batch_start_time' in locals() else 3
                    remaining_batches = (max_pages - current_page - batch_size) // batch_size
                    estimated_time = remaining_batches * avg_time_per_batch / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
                    
                    logging.info(f"âœ… ç¬¬{batch_num}æ‰¹æ¬¡å®Œæˆï¼Œç´¯è®¡ {len(all_results)} æ¡è®°å½•")
                    logging.info(f"ğŸ“ˆ è¿›åº¦: {progress_percent:.1f}% | é¢„ä¼°å‰©ä½™: {estimated_time:.1f}åˆ†é’Ÿ")
                else:
                    logging.info(f"âœ… ç¬¬{batch_num}æ‰¹æ¬¡å®Œæˆï¼Œç´¯è®¡ {len(all_results)} æ¡è®°å½•")
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹æ¬¡
                current_page += batch_size
                batch_start_time = time.time()  # è®°å½•æ‰¹æ¬¡å¼€å§‹æ—¶é—´
                
                # ä¿å­˜è¿›åº¦
                save_progress_optimized(current_page, gzh['name'], all_results)
                
                # æ˜¾ç¤ºä¸­æœŸç»Ÿè®¡
                if len(all_results) > 0 and len(all_results) % 50 == 0:
                    temp_df = pd.DataFrame(all_results)
                    name_ratio = len(temp_df[temp_df['å§“å'] != '']) / len(temp_df) * 100
                    logging.info(f"ğŸ“Š ä¸­æœŸç»Ÿè®¡: æœ‰å§“åè®°å½•å æ¯” {name_ratio:.1f}%")
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if current_page < max_pages:
                    time.sleep(3)
        
        except KeyboardInterrupt:
            logging.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...")
            save_progress_optimized(current_page, gzh['name'], all_results)
            return
        except Exception as e:
            logging.error(f"âŒ çˆ¬å–å¼‚å¸¸: {e}")
            save_progress_optimized(current_page, gzh['name'], all_results)
            return
    
    # æœ€ç»ˆç»Ÿè®¡å’Œæ•°æ®æ¸…æ´—
    if all_results:
        # è¿›è¡Œæœ€ç»ˆçš„å…¨é‡æ•°æ®æ¸…æ´—
        logging.info("ğŸ§¹ å¼€å§‹æœ€ç»ˆæ•°æ®æ¸…æ´—...")
        all_results = clean_and_deduplicate_results(all_results)
        
        save_progress_optimized(current_page, gzh['name'], all_results)
        
        df = pd.DataFrame(all_results)
        
        # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        total_records = len(df)
        has_name = len(df[df['å§“å'].notna() & (df['å§“å'] != '')])
        has_institution = len(df[df['æœºæ„'].notna() & (df['æœºæ„'] != '')])
        has_position = len(df[df['èŒä½'].notna() & (df['èŒä½'] != '')])
        has_province = len(df[df['çœä»½'].notna() & (df['çœä»½'] != '')])
        
        logging.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æ€»è®¡ {total_records} æ¡æœ‰æ•ˆè®°å½•")
        logging.info(f"ğŸ“Š æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡:")
        logging.info(f"   æœ‰å§“å: {has_name} æ¡ ({has_name/total_records*100:.1f}%)")
        logging.info(f"   æœ‰æœºæ„: {has_institution} æ¡ ({has_institution/total_records*100:.1f}%)")
        logging.info(f"   æœ‰èŒä½: {has_position} æ¡ ({has_position/total_records*100:.1f}%)")
        logging.info(f"   æœ‰çœä»½: {has_province} æ¡ ({has_province/total_records*100:.1f}%)")
        
        # æŒ‰çœä»½ç»Ÿè®¡
        if has_province > 0:
            province_stats = df[df['çœä»½'] != '']['çœä»½'].value_counts().head(10)
            logging.info(f"ğŸŒ top10çœä»½åˆ†å¸ƒ:")
            for province, count in province_stats.items():
                logging.info(f"   {province}: {count} æ¡")
        
        # æŒ‰æœºæ„ç±»å‹ç»Ÿè®¡
        if has_institution > 0:
            hospital_count = len(df[df['æœºæ„'].str.contains('åŒ»é™¢', na=False)])
            committee_count = len(df[df['æœºæ„'].str.contains('å§”å‘˜ä¼š|ç®¡ç†å±€|ç›‘ç£å±€', na=False)])
            logging.info(f"ğŸ¥ æœºæ„ç±»å‹åˆ†å¸ƒ:")
            logging.info(f"   åŒ»é™¢ç±»: {hospital_count} æ¡")
            logging.info(f"   æ”¿åºœæœºæ„: {committee_count} æ¡")
            logging.info(f"   å…¶ä»–æœºæ„: {has_institution - hospital_count - committee_count} æ¡")
        
        # æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
        print(f"\nğŸ“‹ æ•°æ®ç¤ºä¾‹ï¼ˆå‰10æ¡ï¼‰:")
        sample_cols = ['å‘å¸ƒæ—¶é—´', 'å§“å', 'æœºæ„', 'èŒä½', 'çœä»½']
        available_cols = [col for col in sample_cols if col in df.columns]
        print(df[available_cols].head(10).to_string(index=False))
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
        stats_report = {
            'çˆ¬å–æ—¶é—´': time.strftime("%Y-%m-%d %H:%M:%S"),
            'æ€»è®°å½•æ•°': total_records,
            'æœ‰å§“åè®°å½•æ•°': has_name,
            'æœ‰æœºæ„è®°å½•æ•°': has_institution,
            'æœ‰èŒä½è®°å½•æ•°': has_position,
            'æœ‰çœä»½è®°å½•æ•°': has_province,
            'æ•°æ®å®Œæ•´ç‡': f"{(has_name + has_institution + has_position) / (total_records * 3) * 100:.1f}%"
        }
        
        os.makedirs('å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ', exist_ok=True)
        stats_file = 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« /çˆ¬å–ç»Ÿè®¡æŠ¥å‘Š.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_report, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ğŸ“ˆ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {stats_file}")
        
    else:
        logging.info("âŒ æ— æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥å…³é”®è¯åŒ¹é…æˆ–ç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    main()