#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾·ç‘æ–¯äº§å“ä¸­å¿ƒçˆ¬è™«
çˆ¬å–å˜é¢‘å™¨å’Œæ°¸ç£åŒæ­¥ç”µæœºä¸‹é¢çš„æ‰€æœ‰æ¨¡å—
æ”¯æŒé’‰é’‰é€šçŸ¥å’Œè‡ªåŠ¨æ£€æµ‹æ–°æ–‡ä»¶
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
import pickle
import hmac
import hashlib
import base64
from datetime import datetime
from urllib.parse import urljoin, urlparse, quote_plus
import re
import urllib.request

class DiriseSpider:
    def __init__(self):
        self.base_url = "http://www.dirise.cn"
        self.main_url = "http://www.dirise.cn/product_index.html"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/å¾·ç‘æ–¯"
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "è¯´æ˜ä¹¦ä¸‹è½½")
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "å¾·ç‘æ–¯")
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "è¯´æ˜ä¹¦ä¸‹è½½")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.download_dir, exist_ok=True)
        
        # é’‰é’‰é…ç½®ï¼ˆå†…ç½®ï¼‰
        self.dingtalk_config = {
            "access_token": "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24",
            "secret": "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb",
            "webhook_url": "https://oapi.dingtalk.com/robot/send?access_token=1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        }
        
        # åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•
        self.processed_files = self.load_processed_files()
        self.new_files = []
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_files.pkl'))    
    def load_processed_files(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_files(self):
        """ä¿å­˜å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_files, f)
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥ï¼ˆæ”¯æŒåŠ å¯†ç­¾åï¼‰"""
        if not self.dingtalk_config or not self.dingtalk_config.get('webhook_url'):
            self.log("âš ï¸ é’‰é’‰é…ç½®æœªè®¾ç½®ï¼Œè·³è¿‡é€šçŸ¥")
            return
        
        try:
            # è·å–é…ç½®
            access_token = self.dingtalk_config.get('access_token', '')
            secret = self.dingtalk_config.get('secret', '')
            webhook_url = self.dingtalk_config.get('webhook_url', '')
            
            # å¦‚æœæœ‰secretï¼Œç”Ÿæˆç­¾å
            if secret:
                timestamp = str(round(time.time() * 1000))
                string_to_sign = f'{timestamp}\n{secret}'
                hmac_code = hmac.new(
                    secret.encode('utf-8'),
                    string_to_sign.encode('utf-8'),
                    digestmod=hashlib.sha256
                ).digest()
                sign = quote_plus(base64.b64encode(hmac_code))
                webhook_url = f"{webhook_url}&timestamp={timestamp}&sign={sign}"
            
            # æ„å»ºæ¶ˆæ¯
            data = {
                "msgtype": "text",
                "text": {
                    "content": f"ğŸ¤– å¾·ç‘æ–¯çˆ¬è™«é€šçŸ¥\n{message}"
                }
            }
            
            # å‘é€é€šçŸ¥
            response = requests.post(
                webhook_url,
                json=data,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    self.log("âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
                else:
                    self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {result.get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
            else:
                self.log(f"âŒ é’‰é’‰é€šçŸ¥HTTPé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¼‚å¸¸: {str(e)}")
    
    def get_page(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 502 and attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(3 * (attempt + 1))
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
        return None
    
    def parse_main_page(self):
        """è§£æä¸»é¡µé¢ï¼Œè·å–æ‰€æœ‰æ¨¡å—é“¾æ¥"""
        print("æ­£åœ¨è§£æä¸»é¡µé¢...")
        html = self.get_page(self.main_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        modules = []
        
        # æŸ¥æ‰¾å˜é¢‘å™¨å’Œæ°¸ç£åŒæ­¥ç”µæœºæ¨¡å—
        nav_div = soup.find('div', class_='p102-fdh-1-nav-one')
        if nav_div:
            # è·å–ä¸»æ ‡é¢˜
            title_elem = nav_div.find('h3')
            if title_elem:
                main_title = title_elem.get_text(strip=True).replace('ï¼š', '')
                print(f"æ‰¾åˆ°ä¸»æ¨¡å—: {main_title}")
            
            # è·å–æ‰€æœ‰å­æ¨¡å—
            dd_elements = nav_div.find_all('dd')
            for dd in dd_elements:
                link = dd.find('a')
                if link:
                    module_name = link.get('title', '').strip()
                    module_url = link.get('href', '').strip()
                    if module_name and module_url:
                        full_url = urljoin(self.base_url, module_url)
                        modules.append({
                            'name': module_name,
                            'url': full_url,
                            'type': 'å˜é¢‘å™¨æ¨¡å—'
                        })
                        print(f"æ‰¾åˆ°æ¨¡å—: {module_name} - {full_url}")
        
        return modules
    
    def parse_module_page(self, module_info):
        """è§£æå…·ä½“æ¨¡å—é¡µé¢"""
        print(f"\næ­£åœ¨è§£ææ¨¡å—: {module_info['name']}")
        html = self.get_page(module_info['url'])
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        products = []
        
        # æŸ¥æ‰¾äº§å“åˆ—è¡¨ - æ ¹æ®<dl>ç»“æ„è§£æ
        dl_elements = soup.find_all('dl')
        
        for dl in dl_elements:
            # æŸ¥æ‰¾äº§å“é“¾æ¥å’Œæ ‡é¢˜
            dt = dl.find('dt')
            dd = dl.find('dd')
            
            if dt and dd:
                # ä»dtä¸­è·å–äº§å“é“¾æ¥å’Œå›¾ç‰‡ä¿¡æ¯
                link_elem = dt.find('a')
                if link_elem:
                    product_url = link_elem.get('href', '')
                    product_title = link_elem.get('title', '')
                    
                    # å¦‚æœæ²¡æœ‰titleå±æ€§ï¼Œå°è¯•ä»imgçš„altæˆ–titleè·å–
                    if not product_title:
                        img = link_elem.find('img')
                        if img:
                            product_title = img.get('alt', '') or img.get('title', '')
                    
                    # ä»ddä¸­è·å–äº§å“æè¿°
                    product_desc = ""
                    desc_div = dd.find('div', class_='p102-pros-1-desc')
                    if desc_div:
                        product_desc = desc_div.get_text(strip=True)
                    
                    # è·å–äº§å“åç§°ï¼ˆä»h4æ ‡ç­¾ï¼‰
                    product_name = ""
                    h4_elem = dd.find('h4')
                    if h4_elem:
                        h4_link = h4_elem.find('a')
                        if h4_link:
                            product_name = h4_link.get('title', '') or h4_link.get_text(strip=True)
                    
                    # å¦‚æœè¿˜æ²¡æœ‰äº§å“åç§°ï¼Œä½¿ç”¨title
                    if not product_name:
                        product_name = product_title
                    
                    # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                    if product_url and not product_url.startswith('http'):
                        product_url = urljoin(module_info['url'], product_url)
                    
                    if product_name and product_url:
                        product_info = {
                            'name': product_name,
                            'title': product_title,
                            'url': product_url,
                            'description': product_desc,
                            'module': module_info['name']
                        }
                        products.append(product_info)
                        print(f"  æ‰¾åˆ°äº§å“: {product_name}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°<dl>ç»“æ„çš„äº§å“ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if not products:
            print("  æœªæ‰¾åˆ°<dl>ç»“æ„çš„äº§å“ï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„äº§å“é“¾æ¥
            product_links = soup.find_all('a', href=True)
            
            for link in product_links:
                href = link.get('href', '')
                title = link.get('title', '') or link.get_text(strip=True)
                
                # è¿‡æ»¤äº§å“é“¾æ¥
                if href and title and ('product' in href or 'detail' in href):
                    if not href.startswith('http'):
                        href = urljoin(module_info['url'], href)
                    
                    products.append({
                        'name': title,
                        'title': title,
                        'url': href,
                        'description': '',
                        'module': module_info['name']
                    })
        
        print(f"  å…±æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
        return products
    
    def parse_product_page(self, product_info):
        """è§£æäº§å“è¯¦æƒ…é¡µï¼ŒæŸ¥æ‰¾è¯´æ˜ä¹¦ä¸‹è½½é“¾æ¥"""
        print(f"    æ­£åœ¨è§£æäº§å“: {product_info['name']}")
        html = self.get_page(product_info['url'])
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        download_links = []
        
        # 1. æŸ¥æ‰¾ç‰¹æ®Šçš„è¯´æ˜ä¹¦ä¸‹è½½åŒºåŸŸ (g_sms æˆ–å…¶ä»–å¯èƒ½çš„class)
        sms_divs = soup.find_all('div', class_=['g_sms', 'sms', 'download-area'])
        
        for sms_div in sms_divs:
            if sms_div:
                print(f"      æ‰¾åˆ°è¯´æ˜ä¹¦ä¸‹è½½åŒºåŸŸ: {sms_div.get('class')}")
                
                # æŸ¥æ‰¾æ‰€æœ‰çš„liå…ƒç´ ï¼ˆæ¯ä¸ªliåŒ…å«ä¸€ä¸ªè¯´æ˜ä¹¦ï¼‰
                li_elements = sms_div.find_all('li')
                
                for li in li_elements:
                    # è·å–è¯´æ˜ä¹¦æ ‡é¢˜ (v2)
                    v2_div = li.find('div', class_='v2')
                    manual_title = ""
                    if v2_div:
                        manual_title = v2_div.get_text(strip=True)
                    
                    # è·å–ç‰ˆæœ¬ä¿¡æ¯ (v3)
                    v3_div = li.find('div', class_='v3')
                    version_info = ""
                    if v3_div:
                        version_info = v3_div.get_text(strip=True)
                    
                    # è·å–æ—¥æœŸä¿¡æ¯ (v4)
                    v4_div = li.find('div', class_='v4')
                    date_info = ""
                    if v4_div:
                        date_info = v4_div.get_text(strip=True)
                    
                    # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥ (v6 ä¸­çš„ a æ ‡ç­¾)
                    v6_div = li.find('div', class_='v6')
                    if v6_div:
                        # æŸ¥æ‰¾aæ ‡ç­¾ï¼Œä¸ä¸€å®šæœ‰attach class
                        attach_link = v6_div.find('a', class_='attach') or v6_div.find('a')
                        if attach_link:
                            path = attach_link.get('path', '')
                            href = attach_link.get('href', '')
                            
                            download_url = ""
                            if path:
                                download_url = urljoin(self.base_url, path)
                            elif href and not href.startswith('javascript:'):
                                download_url = urljoin(self.base_url, href) if not href.startswith('http') else href
                            
                            # å³ä½¿æ²¡æœ‰æœ‰æ•ˆçš„ä¸‹è½½URLï¼Œä¹Ÿè¦è®°å½•è¿™ä¸ªè¯´æ˜ä¹¦ä¿¡æ¯ï¼Œä¾›åé¢çš„pathé“¾æ¥å¤„ç†ä½¿ç”¨
                            if manual_title:  # åªè¦æœ‰è¯´æ˜ä¹¦æ ‡é¢˜å°±è®°å½•
                                # ç”Ÿæˆæ–‡ä»¶åï¼ˆä¸åŒ…å«æ—¥æœŸï¼‰
                                filename = manual_title if manual_title else "è¯´æ˜ä¹¦"
                                if version_info:
                                    # åªä¿ç•™ç‰ˆæœ¬å·ï¼Œä¸è¦æ—¥æœŸ
                                    version = version_info.replace('ç‰ˆæœ¬å·ï¼š', '').replace(':', '_').strip()
                                    if version:
                                        filename += f"_{version}"
                                
                                # ç¡®ä¿æ–‡ä»¶åä»¥.pdfç»“å°¾
                                if not filename.lower().endswith('.pdf'):
                                    filename += '.pdf'
                                
                                # å°†è¯´æ˜ä¹¦ä¿¡æ¯å­˜å‚¨èµ·æ¥ï¼Œä¾›åé¢åŒ¹é…ä½¿ç”¨
                                if 'manual_info' not in product_info:
                                    product_info['manual_info'] = []
                                product_info['manual_info'].append({
                                    'title': manual_title,
                                    'filename': filename,
                                    'version': version_info,
                                    'date': date_info
                                })
                                
                                if download_url:
                                    download_links.append({
                                        'url': download_url,
                                        'title': filename,
                                        'manual_title': manual_title,
                                        'version': version_info,
                                        'date': date_info,
                                        'product_name': product_info['name'],
                                        'module': product_info['module'],
                                        'is_manual': True  # æ ‡è®°ä¸ºè¯´æ˜ä¹¦æ–‡ä»¶
                                    })
                                    print(f"      æ‰¾åˆ°è¯´æ˜ä¹¦: {manual_title} - {download_url}")
        
        # 1.5 å¦‚æœæ²¡æœ‰æ‰¾åˆ°g_smsï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–è¯´æ˜ä¹¦ä¸‹è½½ç»“æ„
        if not download_links:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«pathå±æ€§å’Œv2ã€v3ã€v4ç»“æ„çš„åŒºåŸŸ
            all_lis = soup.find_all('li')
            for li in all_lis:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«v2, v6ç»“æ„
                v2_div = li.find('div', class_='v2')
                v6_div = li.find('div', class_='v6')
                
                if v2_div and v6_div:
                    manual_title = v2_div.get_text(strip=True)
                    
                    # è·å–ç‰ˆæœ¬ä¿¡æ¯ (v3)
                    v3_div = li.find('div', class_='v3')
                    version_info = ""
                    if v3_div:
                        version_info = v3_div.get_text(strip=True)
                    
                    # è·å–æ—¥æœŸä¿¡æ¯ (v4)
                    v4_div = li.find('div', class_='v4')
                    date_info = ""
                    if v4_div:
                        date_info = v4_div.get_text(strip=True)
                    
                    # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
                    attach_link = v6_div.find('a', class_='attach')
                    if attach_link:
                        path = attach_link.get('path', '')
                        if path:
                            download_url = urljoin(self.base_url, path)
                            
                            # ç”Ÿæˆæ–‡ä»¶å
                            filename = manual_title if manual_title else "è¯´æ˜ä¹¦"
                            if version_info:
                                filename += f"_{version_info.replace('ç‰ˆæœ¬å·ï¼š', '').replace(':', '_')}"
                            if date_info:
                                filename += f"_{date_info}"
                            
                            if not filename.lower().endswith('.pdf'):
                                filename += '.pdf'
                            
                            download_links.append({
                                'url': download_url,
                                'title': filename,
                                'manual_title': manual_title,
                                'version': version_info,
                                'date': date_info,
                                'product_name': product_info['name'],
                                'module': product_info['module'],
                                'is_manual': True
                            })
                            print(f"      æ‰¾åˆ°è¯´æ˜ä¹¦(æ— g_sms): {manual_title} - {download_url}")
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ä¸‹è½½é“¾æ¥ï¼ˆåŒ…æ‹¬pathå±æ€§çš„é“¾æ¥ï¼‰
        all_links = soup.find_all('a', href=True)
        seen_urls = set()  # ç”¨äºå»é‡
        
        for link in all_links:
            # æ£€æŸ¥æ˜¯å¦æœ‰pathå±æ€§ï¼ˆJavaScriptä¸‹è½½é“¾æ¥ï¼‰
            path = link.get('path', '')
            if path:
                # æ„å»ºå®Œæ•´çš„ä¸‹è½½URL
                download_url = urljoin(self.base_url, path)
                
                # å°è¯•ä»å‘¨å›´çš„HTMLç»“æ„è·å–æ›´å¥½çš„æ–‡ä»¶å
                title = link.get('title', '') or link.get_text(strip=True)
                is_manual_file = False
                
                # é¦–å…ˆå°è¯•ä»å­˜å‚¨çš„è¯´æ˜ä¹¦ä¿¡æ¯ä¸­åŒ¹é…
                if 'manual_info' in product_info and product_info['manual_info']:
                    # å¦‚æœæœ‰å­˜å‚¨çš„è¯´æ˜ä¹¦ä¿¡æ¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªï¼ˆæˆ–è€…å¯ä»¥æ ¹æ®URLåŒ¹é…ï¼‰
                    manual_info = product_info['manual_info'][0]  # ç®€å•èµ·è§ä½¿ç”¨ç¬¬ä¸€ä¸ª
                    title = manual_info['filename']
                    is_manual_file = True
                    print(f"      ä½¿ç”¨å­˜å‚¨çš„è¯´æ˜ä¹¦ä¿¡æ¯: {title}")
                else:
                    # æŸ¥æ‰¾çˆ¶çº§liå…ƒç´ ï¼Œçœ‹æ˜¯å¦æœ‰v2æ ‡ç­¾
                    parent_li = link.find_parent('li')
                    if parent_li:
                        v2_div = parent_li.find('div', class_='v2')
                        if v2_div:
                            manual_title = v2_div.get_text(strip=True)
                            if manual_title:
                                # è·å–ç‰ˆæœ¬ä¿¡æ¯
                                v3_div = parent_li.find('div', class_='v3')
                                version_info = ""
                                if v3_div:
                                    version_info = v3_div.get_text(strip=True)
                                
                                # è·å–æ—¥æœŸä¿¡æ¯
                                v4_div = parent_li.find('div', class_='v4')
                                date_info = ""
                                if v4_div:
                                    date_info = v4_div.get_text(strip=True)
                                
                                # ç”Ÿæˆæ›´å¥½çš„æ–‡ä»¶åï¼ˆä¸åŒ…å«æ—¥æœŸï¼‰
                                filename = manual_title
                                if version_info:
                                    version = version_info.replace('ç‰ˆæœ¬å·ï¼š', '').replace(':', '_').strip()
                                    if version:
                                        filename += f"_{version}"
                                
                                if not filename.lower().endswith('.pdf'):
                                    filename += '.pdf'
                                
                                title = filename
                                is_manual_file = True
                                print(f"      æ‰¾åˆ°æ­£ç¡®å‘½åçš„è¯´æ˜ä¹¦: {title}")
                
                if download_url not in seen_urls:
                    seen_urls.add(download_url)
                    download_links.append({
                        'url': download_url,
                        'title': title if title else "ç‚¹å‡»ä¸‹è½½",
                        'product_name': product_info['name'],
                        'module': product_info['module'],
                        'is_manual': is_manual_file
                    })
                    print(f"      æ‰¾åˆ°pathä¸‹è½½é“¾æ¥: {title} - {download_url}")
        
        # 3. å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°ä¸‹è½½é“¾æ¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if not download_links:
            print(f"      æœªæ‰¾åˆ°pathä¸‹è½½é“¾æ¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
            
            # æŸ¥æ‰¾åŒ…å«"ä¸‹è½½"ã€"è¯´æ˜ä¹¦"ã€"æ‰‹å†Œ"ç­‰å…³é”®è¯çš„é“¾æ¥
            download_keywords = ['ä¸‹è½½', 'è¯´æ˜ä¹¦', 'æ‰‹å†Œ', 'æ–‡æ¡£', 'èµ„æ–™', 'download', 'manual', 'guide']
            
            for link in all_links:
                link_text = link.get_text(strip=True).lower()
                link_href = link.get('href', '').lower()
                link_title = link.get('title', '').lower()
                
                # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æˆ–hrefæ˜¯å¦åŒ…å«ä¸‹è½½å…³é”®è¯
                is_download_link = any(keyword in link_text or keyword in link_href or keyword in link_title 
                                      for keyword in download_keywords)
                
                if is_download_link:
                    href = link.get('href', '')
                    title = link.get('title', '') or link.get_text(strip=True)
                    
                    # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                    if href and not href.startswith('http'):
                        href = urljoin(product_info['url'], href)
                    
                    # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸æ˜¯æ–‡ä»¶çš„é“¾æ¥
                    if href and not any(skip in href.lower() for skip in ['javascript:', 'mailto:', '#']):
                        if href not in seen_urls:
                            seen_urls.add(href)
                            download_links.append({
                                'url': href,
                                'title': title,
                                'product_name': product_info['name'],
                                'module': product_info['module'],
                                'is_manual': False
                            })
                            print(f"      æ‰¾åˆ°ä¸‹è½½é“¾æ¥: {title} - {href}")
            
            # æŸ¥æ‰¾æ–‡ä»¶æ‰©å±•åé“¾æ¥ï¼ˆ.pdf, .doc, .docxç­‰ï¼‰
            file_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar']
            for link in all_links:
                href = link.get('href', '')
                if any(ext in href.lower() for ext in file_extensions):
                    title = link.get('title', '') or link.get_text(strip=True)
                    
                    if not href.startswith('http'):
                        href = urljoin(product_info['url'], href)
                    
                    # é¿å…é‡å¤æ·»åŠ 
                    if href not in seen_urls:
                        seen_urls.add(href)
                        download_links.append({
                            'url': href,
                            'title': title,
                            'product_name': product_info['name'],
                            'module': product_info['module'],
                            'is_manual': False
                        })
                        print(f"      æ‰¾åˆ°æ–‡ä»¶é“¾æ¥: {title} - {href}")
        
        return download_links
    
    def download_file(self, download_info):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            url = download_info['url']
            filename = download_info['title']
            
            # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
            filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
            
            # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼ï¼Œè®¾ç½®é»˜è®¤åç§°
            if not filename or not filename.strip():
                filename = "ä¸‹è½½æ–‡ä»¶"
            
            # å¦‚æœæ–‡ä»¶åæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•ä»URLè·å–
            if '.' not in filename.split('/')[-1]:
                parsed_url = urlparse(url)
                path = parsed_url.path
                if '.' in path:
                    ext = path.split('.')[-1]
                    if ext.lower() in ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'zip', 'rar']:
                        filename += f'.{ext}'
                else:
                    # å¦‚æœURLä¸­ä¹Ÿæ²¡æœ‰æ‰©å±•åï¼Œé»˜è®¤ä¸ºPDF
                    filename += '.pdf'
            
            # æ¸…ç†æ¨¡å—åå’Œäº§å“åä¸­çš„éæ³•å­—ç¬¦
            module_name = re.sub(r'[<>:"/\\|?*]', '_', download_info['module'])
            product_name = re.sub(r'[<>:"/\\|?*]', '_', download_info['product_name'])
            
            # åˆ›å»ºæ¨¡å—ç›®å½•
            module_dir = os.path.join(self.download_dir, module_name)
            if not os.path.exists(module_dir):
                os.makedirs(module_dir)
            
            # åˆ›å»ºäº§å“ç›®å½•
            product_dir = os.path.join(module_dir, product_name)
            if not os.path.exists(product_dir):
                os.makedirs(product_dir)
            
            # æ‰€æœ‰æ–‡ä»¶éƒ½æ”¾åœ¨äº§å“ç›®å½•ä¸‹çš„è¯´æ˜ä¹¦æ–‡ä»¶å¤¹
            manual_dir = os.path.join(product_dir, "è¯´æ˜ä¹¦")
            if not os.path.exists(manual_dir):
                os.makedirs(manual_dir)
            target_dir = manual_dir
            
            # å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            filepath = os.path.join(target_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            if os.path.exists(filepath):
                print(f"        æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return True
            
            print(f"        æ­£åœ¨ä¸‹è½½: {filename}")
            
            # ä½¿ç”¨requestsä¸‹è½½æ–‡ä»¶
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' in content_type:
                print(f"        è­¦å‘Š: ä¸‹è½½çš„å¯èƒ½ä¸æ˜¯æ–‡ä»¶ï¼Œè€Œæ˜¯HTMLé¡µé¢")
                return False
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filepath)
            if file_size < 1024:  # å°äº1KBå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                print(f"        è­¦å‘Š: æ–‡ä»¶å¤§å°å¼‚å¸¸ ({file_size} bytes)")
                # å¯ä»¥é€‰æ‹©åˆ é™¤å°æ–‡ä»¶
                # os.remove(filepath)
                # return False
            
            print(f"        ä¸‹è½½å®Œæˆ: {filename} ({file_size} bytes)")
            
            # è®°å½•æ–°æ–‡ä»¶
            file_key = f"{download_info['module']}_{download_info['product_name']}_{filename}"
            if file_key not in self.processed_files:
                self.new_files.append({
                    'filename': filename,
                    'path': filepath,
                    'url': download_info['url'],
                    'size': file_size,
                    'module': download_info['module'],
                    'product': download_info['product_name']
                })
                self.processed_files.add(file_key)
            
            return True
            
        except Exception as e:
            print(f"        ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def download_manuals(self, products):
        """ä¸‹è½½æ‰€æœ‰äº§å“çš„è¯´æ˜ä¹¦"""
        if not products:
            print("æ²¡æœ‰äº§å“éœ€è¦ä¸‹è½½è¯´æ˜ä¹¦")
            return
        
        print(f"\nå¼€å§‹ä¸‹è½½è¯´æ˜ä¹¦...")
        print(f"å…±æœ‰ {len(products)} ä¸ªäº§å“éœ€è¦å¤„ç†")
        
        total_downloads = 0
        successful_downloads = 0
        
        for i, product in enumerate(products, 1):
            print(f"\nè¿›åº¦: {i}/{len(products)} - {product['module']} - {product['name']}")
            
            # è§£æäº§å“é¡µé¢ï¼ŒæŸ¥æ‰¾ä¸‹è½½é“¾æ¥
            download_links = self.parse_product_page(product)
            
            if download_links:
                print(f"    æ‰¾åˆ° {len(download_links)} ä¸ªä¸‹è½½é“¾æ¥")
                
                # ä¸‹è½½æ¯ä¸ªæ–‡ä»¶
                for download_info in download_links:
                    total_downloads += 1
                    if self.download_file(download_info):
                        successful_downloads += 1
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)
            else:
                print(f"    æœªæ‰¾åˆ°ä¸‹è½½é“¾æ¥")
            
            # äº§å“é—´å»¶è¿Ÿ
            time.sleep(2)
        
        print(f"\nè¯´æ˜ä¹¦ä¸‹è½½å®Œæˆï¼")
        print(f"æ€»å°è¯•ä¸‹è½½: {total_downloads} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸä¸‹è½½: {successful_downloads} ä¸ªæ–‡ä»¶")
        print(f"å¤±è´¥: {total_downloads - successful_downloads} ä¸ªæ–‡ä»¶")
    
    def save_data(self, data, filename):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        filepath = os.path.join(self.output_dir, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.log("ğŸš€ å¼€å§‹çˆ¬å–å¾·ç‘æ–¯äº§å“ä¸­å¿ƒ...")
        
        # 1. è·å–æ‰€æœ‰æ¨¡å—
        modules = self.parse_main_page()
        if not modules:
            self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å—")
            return
        
        self.log(f"ğŸ“‹ å…±æ‰¾åˆ° {len(modules)} ä¸ªæ¨¡å—")
        
        # 2. ä¿å­˜æ¨¡å—ä¿¡æ¯
        self.save_data(modules, 'modules.json')
        
        # 3. çˆ¬å–æ¯ä¸ªæ¨¡å—çš„äº§å“
        all_products = []
        for i, module in enumerate(modules, 1):
            self.log(f"ğŸ”„ è¿›åº¦: {i}/{len(modules)} - {module['name']}")
            products = self.parse_module_page(module)
            all_products.extend(products)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        # 4. ä¿å­˜æ‰€æœ‰äº§å“ä¿¡æ¯
        if all_products:
            self.save_data(all_products, 'products.json')
            self.log(f"âœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_products)} ä¸ªäº§å“")
        else:
            self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•äº§å“ä¿¡æ¯")
            return
        
        # 5. ä¸‹è½½è¯´æ˜ä¹¦
        self.download_manuals(all_products)
        
        # 6. ä¿å­˜å¤„ç†è®°å½•
        self.save_processed_files()
        
        # 7. å‘é€é’‰é’‰é€šçŸ¥
        self.send_completion_notification()
        
        # 8. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(modules, all_products)
    
    def send_completion_notification(self):
        """å‘é€å®Œæˆé€šçŸ¥"""
        if not self.new_files:
            if not self.is_first_run:
                self.log("ğŸ“¢ æ— æ–°æ–‡ä»¶ï¼Œä¸å‘é€é€šçŸ¥")
            return
        
        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
        message_parts = []
        message_parts.append(f"ğŸ“Š å¾·ç‘æ–¯çˆ¬è™«å®Œæˆ")
        message_parts.append(f"ğŸ•’ æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        message_parts.append(f"ğŸ“ æ–°ä¸‹è½½æ–‡ä»¶: {len(self.new_files)} ä¸ª")
        
        if self.is_first_run:
            message_parts.append("ğŸ†• é¦–æ¬¡è¿è¡Œï¼Œå·²å»ºç«‹åŸºçº¿")
        
        # æŒ‰æ¨¡å—åˆ†ç»„æ˜¾ç¤ºæ–°æ–‡ä»¶
        module_files = {}
        for file_info in self.new_files:
            module = file_info['module']
            if module not in module_files:
                module_files[module] = []
            module_files[module].append(file_info)
        
        message_parts.append("\nğŸ“‹ æ–°æ–‡ä»¶è¯¦æƒ…:")
        for module, files in module_files.items():
            message_parts.append(f"  ğŸ“‚ {module}: {len(files)} ä¸ªæ–‡ä»¶")
            for file_info in files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                size_mb = file_info['size'] / 1024 / 1024
                message_parts.append(f"    ğŸ“„ {file_info['filename']} ({size_mb:.1f}MB)")
            if len(files) > 3:
                message_parts.append(f"    ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")
        
        message = "\n".join(message_parts)
        self.send_dingtalk_notification(message)
    
    def generate_report(self, modules, products):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            'çˆ¬å–æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S'),
            'æ€»æ¨¡å—æ•°': len(modules),
            'æ€»äº§å“æ•°': len(products),
            'æ¨¡å—åˆ—è¡¨': [m['name'] for m in modules],
            'å„æ¨¡å—äº§å“æ•°é‡': {}
        }
        
        # ç»Ÿè®¡å„æ¨¡å—äº§å“æ•°é‡
        for module in modules:
            module_products = [p for p in products if p['module'] == module['name']]
            report['å„æ¨¡å—äº§å“æ•°é‡'][module['name']] = len(module_products)
        
        # ä¿å­˜æŠ¥å‘Š
        self.save_data(report, 'çˆ¬å–æŠ¥å‘Š.json')
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "="*50)
        print("çˆ¬å–æŠ¥å‘Š")
        print("="*50)
        print(f"çˆ¬å–æ—¶é—´: {report['çˆ¬å–æ—¶é—´']}")
        print(f"æ€»æ¨¡å—æ•°: {report['æ€»æ¨¡å—æ•°']}")
        print(f"æ€»äº§å“æ•°: {report['æ€»äº§å“æ•°']}")
        print("\nå„æ¨¡å—äº§å“æ•°é‡:")
        for module_name, count in report['å„æ¨¡å—äº§å“æ•°é‡'].items():
            print(f"  {module_name}: {count} ä¸ª")
        print("="*50)
    
    def download_manuals_only(self):
        """åªä¸‹è½½è¯´æ˜ä¹¦ï¼Œä¸é‡æ–°çˆ¬å–äº§å“ä¿¡æ¯"""
        print("å¼€å§‹ä¸‹è½½è¯´æ˜ä¹¦ï¼ˆä½¿ç”¨ç°æœ‰äº§å“æ•°æ®ï¼‰...")
        
        # åŠ è½½ç°æœ‰äº§å“æ•°æ®
        products_file = os.path.join(self.output_dir, 'products.json')
        if not os.path.exists(products_file):
            print("æœªæ‰¾åˆ°äº§å“æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´çˆ¬å–")
            return
        
        try:
            with open(products_file, 'r', encoding='utf-8') as f:
                products = json.load(f)
            
            print(f"åŠ è½½äº† {len(products)} ä¸ªäº§å“ä¿¡æ¯")
            self.download_manuals(products)
            
        except Exception as e:
            print(f"åŠ è½½äº§å“æ•°æ®å¤±è´¥: {e}")

if __name__ == "__main__":
    spider = DiriseSpider()
    try:
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        import sys
        if len(sys.argv) > 1 and sys.argv[1] == '--download-only':
            spider.download_manuals_only()
        else:
            spider.run()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
