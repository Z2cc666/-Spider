#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import json
import pickle
import platform
import re
import requests
from datetime import datetime, date
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote, parse_qs
from bs4 import BeautifulSoup
import hashlib

class DexwellSpider:
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.base_url = "https://www.welllinkio.com"
        self.download_url = "https://www.welllinkio.com/download"
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/å¾·å…‹å¨å°”"
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "downloads", "å¾·å…‹å¨å°”")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.base_dir, exist_ok=True)
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.dingtalk_webhook = os.getenv('DINGTALK_WEBHOOK', '')
        
        # æ•°æ®å­˜å‚¨
        self.processed_files = self.load_processed_files()
        self.module_structure = self.load_module_structure()
        self.new_files = []
        self.new_modules = []
        
        # æ—¶é—´è¿‡æ»¤
        self.filter_date = datetime(2024, 11, 1).date()
        
        # è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # åˆå§‹åŒ–é¡µé¢å¤„ç†çŠ¶æ€
        self._processed_pages = set()
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.ACCESS_TOKEN = "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        self.SECRET = "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb"
        
        # æœåŠ¡å™¨ä¿å­˜åœ°å€
        self.server_save_path = "/srv/downloads/approved/å¾·å…‹å¨å°”"
        
    def clean_folder_name(self, name):
        """æ¸…ç†æ–‡ä»¶å¤¹åç§°ï¼Œæ›¿æ¢ç‰¹æ®Šå­—ç¬¦"""
        if not name:
            return "æœªçŸ¥"
        
        # æ›¿æ¢æ–œæ ä¸ºä¸‹åˆ’çº¿
        cleaned = name.replace('/', '_').replace('\\', '_')
        
        # æ›¿æ¢å…¶ä»–å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'[<>:"|?*]', '_', cleaned)
        
        # æ¸…ç†å¤šä½™çš„ä¸‹åˆ’çº¿
        cleaned = re.sub(r'_+', '_', cleaned)
        
        # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
        cleaned = cleaned.strip('_')
        
        return cleaned if cleaned else "æœªçŸ¥"

    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def load_processed_files(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_files(self):
        """ä¿å­˜å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_files, f)
    
    def load_module_structure(self):
        """åŠ è½½æ¨¡å—ç»“æ„"""
        structure_file = os.path.join(self.base_dir, 'module_structure.json')
        if os.path.exists(structure_file):
            try:
                with open(structure_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def save_module_structure(self):
        """ä¿å­˜æ¨¡å—ç»“æ„"""
        structure_file = os.path.join(self.base_dir, 'module_structure.json')
        with open(structure_file, 'w', encoding='utf-8') as f:
            json.dump(self.module_structure, f, ensure_ascii=False, indent=2)
    
    def get_page_content(self, url, retry_count=3):
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(retry_count):
            try:
                self.log(f"ğŸ”„ è®¿é—®é¡µé¢ (å°è¯•{attempt+1}/{retry_count}): {url}")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                self.log(f"âŒ é¡µé¢è®¿é—®å¤±è´¥ (å°è¯•{attempt+1}): {str(e)}")
                if attempt < retry_count - 1:
                    time.sleep(5)
        
        self.log(f"âŒ é¡µé¢è®¿é—®å®Œå…¨å¤±è´¥: {url}")
        return None
    
    def scan_module_structure(self):
        """æ‰«ææ¨¡å—ç»“æ„"""
        self.log("ğŸ” å¼€å§‹æ‰«ææ¨¡å—ç»“æ„")
        
        try:
            # è·å–ä¸»é¡µé¢
            page_content = self.get_page_content(self.download_url)
            if not page_content:
                return {}
            
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # æŸ¥æ‰¾å·¦ä¾§æ¨¡å—å¯¼èˆª
            left_nav = soup.find('div', class_='newleft')
            if not left_nav:
                self.log("âŒ æœªæ‰¾åˆ°å·¦ä¾§å¯¼èˆª")
                return {}
            
            # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å—
            modules = left_nav.find_all('li', class_='cur')
            current_structure = {}
            
            for module in modules:
                # è·å–æ¨¡å—åç§°
                module_header = module.find('h4')
                if not module_header:
                    continue
                
                module_name = module_header.get_text().strip()
                self.log(f"ğŸ“‹ å‘ç°æ¨¡å—: {module_name}")
                
                # æŸ¥æ‰¾å­æ¨¡å—
                sub_modules = module.find('div', class_='newtoolsnav')
                if sub_modules:
                    sub_module_links = sub_modules.find_all('a')
                    sub_modules_list = []
                    
                    for link in sub_module_links:
                        onclick = link.get('onclick', '')
                        checkbox = link.find('input')
                        if checkbox and onclick:
                            # æå–å­æ¨¡å—åç§°
                            sub_name = link.get_text().strip()
                            # æå–URLå‚æ•°
                            url_params = self.extract_url_params(onclick)
                            
                            sub_modules_list.append({
                                'name': sub_name,
                                'onclick': onclick,
                                'url_params': url_params
                            })
                            self.log(f"   ğŸ“ å­æ¨¡å—: {sub_name}")
                    
                    current_structure[module_name] = sub_modules_list
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨¡å—
            self.check_new_modules(current_structure)
            
            # ä¿å­˜å½“å‰ç»“æ„
            self.module_structure = current_structure
            self.save_module_structure()
            
            return current_structure
            
        except Exception as e:
            self.log(f"âŒ æ‰«ææ¨¡å—ç»“æ„å¤±è´¥: {str(e)}")
            return {}
    
    def extract_url_params(self, onclick):
        """ä»onclickä¸­æå–URLå‚æ•°"""
        try:
            # æå–routerWayå‡½æ•°ä¸­çš„å‚æ•°
            match = re.search(r"routerWay\('([^']+)'\)", onclick)
            if match:
                url = match.group(1)
                if url.startswith('/download'):
                    # è§£ææŸ¥è¯¢å‚æ•°
                    parsed = urlparse(url)
                    params = parse_qs(parsed.query)
                    return params
            return {}
        except:
            return {}
    
    def check_new_modules(self, current_structure):
        """æ£€æŸ¥æ–°æ¨¡å—"""
        if not self.module_structure:
            # é¦–æ¬¡è¿è¡Œï¼Œè®°å½•æ‰€æœ‰æ¨¡å—
            self.log("ğŸ‰ é¦–æ¬¡è¿è¡Œï¼Œè®°å½•æ‰€æœ‰æ¨¡å—")
            return
        
        for module_name, sub_modules in current_structure.items():
            if module_name not in self.module_structure:
                self.log(f"ğŸ†• å‘ç°æ–°æ¨¡å—: {module_name}")
                self.new_modules.append(module_name)
                self.notify_dingtalk(f"å‘ç°æ–°æ¨¡å—: {module_name}")
            
            # æ£€æŸ¥å­æ¨¡å—
            if module_name in self.module_structure:
                existing_sub_names = {sm['name'] for sm in self.module_structure[module_name]}
                current_sub_names = {sm['name'] for sm in sub_modules}
                
                new_sub_modules = current_sub_names - existing_sub_names
                if new_sub_modules:
                    for new_sub in new_sub_modules:
                        self.log(f"ğŸ†• å‘ç°æ–°å­æ¨¡å—: {module_name} -> {new_sub}")
                        self.new_modules.append(f"{module_name} -> {new_sub}")
                        self.notify_dingtalk(f"å‘ç°æ–°å­æ¨¡å—: {module_name} -> {new_sub}")
    
    def notify_dingtalk(self, message):
        """é’‰é’‰é€šçŸ¥"""
        if not self.dingtalk_webhook:
            self.log(f"ğŸ“¢ é’‰é’‰é€šçŸ¥: {message}")
            return
        
        try:
            data = {
                "msgtype": "text",
                "text": {
                    "content": f"å¾·å…‹å¨å°”çˆ¬è™«é€šçŸ¥: {message}"
                }
            }
            
            response = requests.post(self.dingtalk_webhook, json=data, timeout=10)
            if response.status_code == 200:
                self.log(f"âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ: {message}")
            else:
                self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {response.status_code}")
        except Exception as e:
            self.log(f"âŒ é’‰é’‰é€šçŸ¥å¼‚å¸¸: {str(e)}")
    
    def should_download_file(self, title, publish_date, module_name):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸‹è½½æ–‡ä»¶"""
        # äº§å“æ‰‹å†Œéœ€è¦æ—¶é—´è¿‡æ»¤ï¼ˆåªä¸‹è½½2024.11.1ä¹‹åçš„æ–‡ä»¶ï¼‰
        if module_name in ["é€‰å‹æ‰‹å†Œ", "ä½¿ç”¨æ‰‹å†Œ", "å®£ä¼ æ‰‹å†Œ"]:
            try:
                # è§£ææ—¥æœŸ
                if publish_date:
                    file_date = datetime.strptime(publish_date.strip(), "%Y-%m-%d").date()
                    if file_date < self.filter_date:
                        self.log(f"â­ï¸ è·³è¿‡è¿‡æœŸæ–‡ä»¶: {title} ({publish_date})")
                        return False
            except:
                # æ—¥æœŸè§£æå¤±è´¥ï¼Œé»˜è®¤ä¸‹è½½
                pass
        
        # å…¶ä»–æ¨¡å—ï¼ˆé…ç½®æ–‡ä»¶ã€å›¾çº¸ã€è½¯ä»¶ä¸è°ƒè¯•å·¥å…·ï¼‰å…¨éƒ¨ä¸‹è½½
        return True
    
    def download_file(self, url, filename, folder_path):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            # åˆ›å»ºç›®å½•
            os.makedirs(folder_path, exist_ok=True)
            file_path = os.path.join(folder_path, filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                if file_size > 1024:  # å¤§äº1KBè®¤ä¸ºæœ‰æ•ˆ
                    self.log(f"ğŸ“ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                    return True
                else:
                    self.log(f"ğŸ”„ æ–‡ä»¶å­˜åœ¨ä½†å¤§å°å¼‚å¸¸ï¼Œé‡æ–°ä¸‹è½½: {filename}")
                    os.remove(file_path)
            
            # ä¸‹è½½æ–‡ä»¶
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(file_path)
            self.log(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} ({file_size} bytes)")
            
            self.new_files.append({
                'filename': filename,
                'path': file_path,
                'url': url,
                'size': file_size
            })
            
            return True
            
        except Exception as e:
            self.log(f"âŒ ä¸‹è½½å¤±è´¥ {filename}: {str(e)}")
            return False
    
    def process_download_page(self, url, module_name, sub_module_name):
        """å¤„ç†ä¸‹è½½é¡µé¢"""
        # æ£€æŸ¥é¡µé¢æ˜¯å¦å·²å¤„ç†
        if not hasattr(self, '_processed_pages'):
            self._processed_pages = set()
        
        if url in self._processed_pages:
            self.log(f"â­ï¸ é¡µé¢å·²å¤„ç†ï¼Œè·³è¿‡: {url}")
            return
        
        self.log(f"ğŸ“„ å¤„ç†ä¸‹è½½é¡µé¢: {sub_module_name}")
        self._processed_pages.add(url)
        
        try:
            page_content = self.get_page_content(url)
            if not page_content:
                return
            
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # æŸ¥æ‰¾ä¸‹è½½åˆ—è¡¨
            download_list = soup.find('div', class_='dllist')
            if not download_list:
                self.log(f"âŒ æœªæ‰¾åˆ°ä¸‹è½½åˆ—è¡¨: {url}")
                return
            
            # æŸ¥æ‰¾æ‰€æœ‰ä¸‹è½½é¡¹
            download_items = download_list.find_all('div', class_='list')
            self.log(f"ğŸ” æ‰¾åˆ° {len(download_items)} ä¸ªä¸‹è½½é¡¹")
            
            # åˆ›å»ºæ¨¡å—ç›®å½•
            clean_module_name = self.clean_folder_name(module_name)
            clean_sub_module_name = self.clean_folder_name(sub_module_name)
            module_dir = os.path.join(self.base_dir, clean_module_name, clean_sub_module_name)
            
            for item in download_items:
                try:
                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    title_element = item.find('h3')
                    if not title_element:
                        continue
                    
                    title = title_element.get_text().strip()
                    
                    # è·å–å‘å¸ƒæ—¥æœŸ
                    date_element = item.find('p')
                    publish_date = date_element.get_text().strip() if date_element else ""
                    
                    # è·å–ä¸‹è½½é“¾æ¥
                    download_link = item.find('a', target='_blank')
                    if not download_link:
                        continue
                    
                    file_url = download_link.get('href', '')
                    if not file_url:
                        continue
                    
                    # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸‹è½½
                    if not self.should_download_file(title, publish_date, sub_module_name):
                        continue
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    filename = self.generate_filename(title, file_url)
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                    file_hash = hashlib.md5(f"{file_url}_{title}".encode()).hexdigest()
                    if file_hash in self.processed_files:
                        self.log(f"â­ï¸ æ–‡ä»¶å·²å¤„ç†ï¼Œè·³è¿‡: {title}")
                        continue
                    
                    # ä¸‹è½½æ–‡ä»¶
                    if self.download_file(file_url, filename, module_dir):
                        self.processed_files.add(file_hash)
                    
                    time.sleep(1)  # ä¸‹è½½é—´éš”
                    
                except Exception as e:
                    self.log(f"âŒ å¤„ç†ä¸‹è½½é¡¹æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é¡µ
            self.check_pagination(soup, url, module_name, sub_module_name)
            
        except Exception as e:
            self.log(f"âŒ å¤„ç†ä¸‹è½½é¡µé¢å¤±è´¥: {str(e)}")
    
    def check_pagination(self, soup, current_url, module_name, sub_module_name):
        """æ£€æŸ¥åˆ†é¡µ"""
        try:
            pagination = soup.find('div', class_='pagee')
            if not pagination:
                return
            
            # æŸ¥æ‰¾æ‰€æœ‰é¡µç é“¾æ¥
            page_links = pagination.find_all('a')
            if not page_links:
                return
            
            # è·å–å½“å‰é¡µç 
            current_page = 1
            if 'page=' in current_url:
                page_match = re.search(r'page=(\d+)', current_url)
                if page_match:
                    current_page = int(page_match.group(1))
            
            # è®°å½•å·²å¤„ç†çš„é¡µé¢ï¼Œé¿å…é‡å¤
            if not hasattr(self, '_processed_pages'):
                self._processed_pages = set()
            
            # æ”¶é›†æ‰€æœ‰åˆ†é¡µURLï¼Œé¿å…é€’å½’è°ƒç”¨
            page_urls = []
            for link in page_links:
                href = link.get('href', '')
                if href and 'page=' in href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        page_url = urljoin(self.base_url, href)
                    else:
                        page_url = urljoin(current_url, href)
                    
                    # é¿å…é‡å¤å¤„ç†å½“å‰é¡µå’Œå·²å¤„ç†çš„é¡µé¢
                    if page_url != current_url and page_url not in self._processed_pages:
                        page_urls.append(page_url)
            
            # æ‰¹é‡å¤„ç†åˆ†é¡µï¼Œé¿å…é€’å½’
            for page_url in page_urls:
                if page_url not in self._processed_pages:
                    self.log(f"ğŸ“„ å‘ç°åˆ†é¡µ: {page_url}")
                    self._processed_pages.add(page_url)
                    # ä½¿ç”¨å»¶è¿Ÿå¤„ç†ï¼Œé¿å…é€’å½’è°ƒç”¨
                    self.process_download_page_delayed(page_url, module_name, sub_module_name)
                    time.sleep(2)  # é¡µé¢é—´å»¶è¿Ÿ
            
        except Exception as e:
            self.log(f"âŒ æ£€æŸ¥åˆ†é¡µæ—¶å‡ºé”™: {str(e)}")
    
    def process_download_page_delayed(self, url, module_name, sub_module_name):
        """å»¶è¿Ÿå¤„ç†ä¸‹è½½é¡µé¢ï¼Œé¿å…é€’å½’"""
        try:
            # ç›´æ¥å¤„ç†é¡µé¢ï¼Œä¸æ£€æŸ¥åˆ†é¡µ
            page_content = self.get_page_content(url)
            if not page_content:
                return
            
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # æŸ¥æ‰¾ä¸‹è½½åˆ—è¡¨
            download_list = soup.find('div', class_='dllist')
            if not download_list:
                self.log(f"âŒ æœªæ‰¾åˆ°ä¸‹è½½åˆ—è¡¨: {url}")
                return
            
            # æŸ¥æ‰¾æ‰€æœ‰ä¸‹è½½é¡¹
            download_items = download_list.find_all('div', class_='list')
            self.log(f"ğŸ” æ‰¾åˆ° {len(download_items)} ä¸ªä¸‹è½½é¡¹")
            
            # åˆ›å»ºæ¨¡å—ç›®å½•
            clean_module_name = self.clean_folder_name(module_name)
            clean_sub_module_name = self.clean_folder_name(sub_module_name)
            module_dir = os.path.join(self.base_dir, clean_module_name, clean_sub_module_name)
            
            for item in download_items:
                try:
                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    title_element = item.find('h3')
                    if not title_element:
                        continue
                    
                    title = title_element.get_text().strip()
                    
                    # è·å–å‘å¸ƒæ—¥æœŸ
                    date_element = item.find('p')
                    publish_date = date_element.get_text().strip() if date_element else ""
                    
                    # è·å–ä¸‹è½½é“¾æ¥
                    download_link = item.find('a', target='_blank')
                    if not download_link:
                        continue
                    
                    file_url = download_link.get('href', '')
                    if not file_url:
                        continue
                    
                    # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸‹è½½
                    if not self.should_download_file(title, publish_date, sub_module_name):
                        continue
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    filename = self.generate_filename(title, file_url)
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                    file_hash = hashlib.md5(f"{file_url}_{title}".encode()).hexdigest()
                    if file_hash in self.processed_files:
                        self.log(f"â­ï¸ æ–‡ä»¶å·²å¤„ç†ï¼Œè·³è¿‡: {title}")
                        continue
                    
                    # ä¸‹è½½æ–‡ä»¶
                    if self.download_file(file_url, filename, module_dir):
                        self.processed_files.add(file_hash)
                    
                    time.sleep(1)  # ä¸‹è½½é—´éš”
                    
                except Exception as e:
                    self.log(f"âŒ å¤„ç†ä¸‹è½½é¡¹æ—¶å‡ºé”™: {str(e)}")
                    continue
            
        except Exception as e:
            self.log(f"âŒ å»¶è¿Ÿå¤„ç†ä¸‹è½½é¡µé¢å¤±è´¥: {str(e)}")
    
    def generate_filename(self, title, url):
        """ç”Ÿæˆæ–‡ä»¶å"""
        try:
            # æ¸…ç†æ ‡é¢˜ï¼Œå°†æ–œæ æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
            clean_title = re.sub(r'[^\w\s\-\u4e00-\u9fff]', '', title)
            clean_title = clean_title.replace('/', '_').replace('\\', '_')  # æ›¿æ¢æ–œæ 
            clean_title = re.sub(r'\s+', '_', clean_title.strip())
            
            # ä»URLè·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            path = parsed_url.path
            filename = os.path.basename(path)
            
            if filename and '.' in filename:
                ext = os.path.splitext(filename)[1]
                return f"{clean_title}{ext}"
            else:
                # æ ¹æ®URLåˆ¤æ–­æ‰©å±•å
                if '.pdf' in url.lower():
                    return f"{clean_title}.pdf"
                elif '.zip' in url.lower():
                    return f"{clean_title}.zip"
                elif '.exe' in url.lower():
                    return f"{clean_title}.exe"
                else:
                    return f"{clean_title}.pdf"  # é»˜è®¤PDF
            
        except Exception as e:
            self.log(f"âš ï¸ æ–‡ä»¶åç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"document_{int(time.time())}.pdf"
    
    def crawl_all_modules(self):
        """çˆ¬å–æ‰€æœ‰æ¨¡å—"""
        self.log("ğŸš€ å¼€å§‹çˆ¬å–æ‰€æœ‰æ¨¡å—")
        
        try:
            # æ‰«ææ¨¡å—ç»“æ„
            module_structure = self.scan_module_structure()
            
            if not module_structure:
                self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å—")
                return
            
            # çˆ¬å–æ¯ä¸ªæ¨¡å—
            for module_name, sub_modules in module_structure.items():
                self.log(f"ğŸ“‹ å¤„ç†æ¨¡å—: {module_name}")
                
                for sub_module in sub_modules:
                    sub_name = sub_module['name']
                    onclick = sub_module['onclick']
                    
                    self.log(f"ğŸ”„ å¤„ç†å­æ¨¡å—: {sub_name}")
                    
                    # æ„å»ºURL
                    if onclick:
                        # ä»onclickä¸­æå–URL
                        url_match = re.search(r"routerWay\('([^']+)'\)", onclick)
                        if url_match:
                            url_path = url_match.group(1)
                            if url_path.startswith('/download'):
                                full_url = urljoin(self.base_url, url_path)
                                self.process_download_page(full_url, module_name, sub_name)
                                time.sleep(3)  # æ¨¡å—é—´å»¶è¿Ÿ
                            elif url_path.startswith('http'):
                                # å¤–éƒ¨é“¾æ¥ï¼ˆå¦‚3Dæ•°æ®åº“ï¼‰
                                self.log(f"ğŸ”— å¤–éƒ¨é“¾æ¥ï¼Œè·³è¿‡: {url_path}")
                            else:
                                self.log(f"âš ï¸ æœªçŸ¥é“¾æ¥æ ¼å¼: {url_path}")
                    
                    time.sleep(2)  # å­æ¨¡å—é—´å»¶è¿Ÿ
            
            # ä¿å­˜è¿›åº¦
            self.save_processed_files()
            
            # ç»Ÿè®¡ç»“æœ
            total_files = len(self.new_files)
            self.log(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±ä¸‹è½½ {total_files} ä¸ªæ–°æ–‡ä»¶")
            
            if self.new_files:
                self.log("ğŸ“ æ–°ä¸‹è½½çš„æ–‡ä»¶:")
                for file_info in self.new_files[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                    self.log(f"   ğŸ“„ {file_info['filename']} ({file_info['size']} bytes)")
                
                if len(self.new_files) > 10:
                    self.log(f"   ... è¿˜æœ‰ {len(self.new_files) - 10} ä¸ªæ–‡ä»¶")
            
            # å‘é€å®Œæˆé€šçŸ¥
            if self.new_files or self.new_modules:
                summary = f"çˆ¬å–å®Œæˆï¼æ–°æ–‡ä»¶: {len(self.new_files)}ä¸ª, æ–°æ¨¡å—: {len(self.new_modules)}ä¸ª"
                self.notify_dingtalk(summary)
            
        except Exception as e:
            self.log(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}")
            self.notify_dingtalk(f"çˆ¬å–å‡ºé”™: {str(e)}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            self.log("ğŸš€ å¼€å§‹è¿è¡Œå¾·å…‹å¨å°”æ–‡æ¡£çˆ¬è™«")
            self.crawl_all_modules()
        except Exception as e:
            self.log(f"âŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {str(e)}")
        finally:
            self.session.close()

def test_single_module():
    """æµ‹è¯•å•ä¸ªæ¨¡å—"""
    spider = DexwellSpider()
    
    try:
        # æµ‹è¯•é€‰å‹æ‰‹å†Œ
        test_url = "https://www.welllinkio.com/download?cpsc=%E9%80%89%E5%9E%8B%E6%89%8B%E5%86%8C"
        spider.process_download_page(test_url, "äº§å“æ‰‹å†Œ", "é€‰å‹æ‰‹å†Œ")
        
    except Exception as e:
        spider.log(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    finally:
        spider.session.close()

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼
        test_single_module()
    else:
        # æ­£å¸¸è¿è¡Œ
        spider = DexwellSpider()
        spider.run()
