#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ—äº¬å‹æ–¹é‡‘æ³°ï¼ˆå²›ç”µï¼‰ä¸‹è½½ä¸­å¿ƒçˆ¬è™«
ç½‘ç«™ï¼šhttp://www.yhxml.com/articles.php?classid=3
"""

import os
import sys
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import logging
from pathlib import Path
import re
from typing import Dict, List, Tuple, Optional
import json
import pickle
import hashlib
import hmac
import base64
import urllib.parse
from datetime import datetime

class YHXMLSpider:
    def __init__(self, base_dir: str = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            base_dir: ä¸‹è½½æ–‡ä»¶ä¿å­˜çš„åŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ä»£ç æ–‡ä»¶å¤¹
        """
        if base_dir is None:
            # é»˜è®¤ä½¿ç”¨æœåŠ¡å™¨æŒ‡å®šç›®å½•
            base_dir = "/srv/downloads/approved/å²›ç”µ"
        self.base_url = "http://www.yhxml.com"
        self.base_dir = Path(base_dir)
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # åˆ›å»ºåŸºç¡€ç›®å½•
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.ACCESS_TOKEN = "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        self.SECRET = "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb"
        
        # å·²å¤„ç†URLè®°å½•
        self.processed_urls = self.load_processed_urls()
        self.new_files = []  # æ–°å¢æ–‡ä»¶åˆ—è¡¨
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        self.is_first_run = not (self.base_dir / 'processed_urls.pkl').exists()
        
        # ä¸‹è½½ç»Ÿè®¡
        self.stats = {
            'total_files': 0,
            'downloaded_files': 0,
            'failed_files': 0,
            'skipped_files': 0
        }
        
        # æ¨¡å—é…ç½®
        self.modules = {
            'äº§å“é€‰å‹æŒ‡å—': {
                'classid': 30,
                'has_subcategories': False,
                'folder_name': 'äº§å“é€‰å‹æŒ‡å—'
            },
            'äº§å“èµ„æ–™ä¸‹è½½': {
                'classid': 36,
                'has_subcategories': True,  # æœ‰äº§å“åˆ†ç±»
                'folder_name': 'äº§å“èµ„æ–™ä¸‹è½½'
            },
            'è½¯ä»¶ä¸‹è½½': {
                'classid': 34,
                'has_subcategories': False,
                'folder_name': 'è½¯ä»¶ä¸‹è½½'
            },
            'äº§å“è§„æ ¼ä¹¦': {
                'classid': 40,
                'has_subcategories': False,
                'folder_name': 'äº§å“è§„æ ¼ä¹¦'
            },
            'åœäº§äº§å“': {
                'classid': 37,
                'has_subcategories': True,  # æœ‰å¤šä¸ªå­äº§å“åˆ†ç±»
                'folder_name': 'åœäº§äº§å“'
            }
        }

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_file = self.base_dir / 'spider.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URL"""
        urls_file = self.base_dir / 'processed_urls.pkl'
        if urls_file.exists():
            try:
                with open(urls_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
        
    def save_processed_urls(self):
        """ä¿å­˜å·²å¤„ç†çš„URL"""
        urls_file = self.base_dir / 'processed_urls.pkl'
        with open(urls_file, 'wb') as f:
            pickle.dump(self.processed_urls, f)

    def safe_request(self, url: str, timeout: int = 30) -> Optional[requests.Response]:
        """
        å®‰å…¨çš„HTTPè¯·æ±‚
        
        Args:
            url: è¯·æ±‚çš„URL
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            Responseå¯¹è±¡æˆ–None
        """
        try:
            self.logger.info(f"è¯·æ±‚URL: {url}")
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None

    def parse_download_page(self, url: str) -> List[Dict]:
        """
        è§£æä¸‹è½½é¡µé¢ï¼Œæå–ä¸‹è½½é“¾æ¥
        
        Args:
            url: é¡µé¢URL
            
        Returns:
            ä¸‹è½½ä¿¡æ¯åˆ—è¡¨
        """
        response = self.safe_request(url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        downloads = []
        
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾åŒ…å«ä¸‹è½½é“¾æ¥çš„åˆ—è¡¨é¡¹
        # é¦–å…ˆå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ—è¡¨é¡¹
        list_items = []
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"ä¸‹è½½"é“¾æ¥çš„å…ƒç´ 
        download_links = soup.find_all('a', text=lambda x: x and 'ä¸‹è½½' in x)
        for link in download_links:
            # å‘ä¸ŠæŸ¥æ‰¾åŒ…å«è¯¥é“¾æ¥çš„åˆ—è¡¨é¡¹
            item = link.find_parent(['li', 'div', 'td'])
            if item and item not in list_items:
                list_items.append(item)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰åˆ—è¡¨é¡¹
        if not list_items:
            list_items = soup.find_all(['li', 'div'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['item', 'list', 'download']))
        
        for item in list_items:
            # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
            download_link = item.find('a', href=True)
            if not download_link:
                continue
                
            href = download_link.get('href', '')
            link_text = download_link.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹è½½é“¾æ¥
            if any(keyword in link_text for keyword in ['ä¸‹è½½', 'download']):
                if href.startswith('http') or href.startswith('/'):
                    full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    
                    # ä»åˆ—è¡¨é¡¹ä¸­æå–æ ‡é¢˜ï¼ˆæ’é™¤ä¸‹è½½é“¾æ¥æ–‡æœ¬ï¼‰
                    title = self.extract_title_from_list_item(item, download_link)
                    
                    if title:
                        downloads.append({
                            'title': title,
                            'url': full_url,
                            'filename': self.extract_filename_from_title(title)
                        })
        
        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1æ²¡æœ‰æ‰¾åˆ°ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹æ³•
        if not downloads:
            download_links = soup.find_all('a', href=True)
            
            for link in download_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # è¿‡æ»¤ä¸‹è½½é“¾æ¥ï¼ˆåŒ…å«ä¸‹è½½ç›¸å…³å…³é”®è¯ï¼‰
                if any(keyword in text for keyword in ['ä¸‹è½½', 'download', 'è¯´æ˜ä¹¦', 'è§„æ ¼', 'è½¯ä»¶', 'åè®®', 'æµç¨‹å›¾']):
                    if href.startswith('http') or href.startswith('/'):
                        full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                        
                        # å°è¯•ä»çˆ¶å…ƒç´ æˆ–å…„å¼Ÿå…ƒç´ è·å–æ›´è¯¦ç»†çš„æ ‡é¢˜ä¿¡æ¯
                        detailed_title = self.extract_detailed_title(link, soup)
                        
                        downloads.append({
                            'title': detailed_title if detailed_title else text,
                            'url': full_url,
                            'filename': self.extract_filename_from_title(detailed_title if detailed_title else text)
                        })
        
        self.logger.info(f"åœ¨ {url} æ‰¾åˆ° {len(downloads)} ä¸ªä¸‹è½½é“¾æ¥")
        return downloads

    def extract_title_from_list_item(self, item, download_link) -> str:
        """
        ä»åˆ—è¡¨é¡¹ä¸­æå–æ ‡é¢˜ï¼Œæ’é™¤ä¸‹è½½é“¾æ¥æ–‡æœ¬
        
        Args:
            item: åˆ—è¡¨é¡¹å…ƒç´ 
            download_link: ä¸‹è½½é“¾æ¥å…ƒç´ 
            
        Returns:
            æ ‡é¢˜æ–‡æœ¬
        """
        # è·å–æ•´ä¸ªåˆ—è¡¨é¡¹çš„æ–‡æœ¬ï¼Œä½†æ’é™¤ä¸‹è½½é“¾æ¥çš„æ–‡æœ¬
        item_text = item.get_text(strip=True)
        link_text = download_link.get_text(strip=True)
        
        # ç§»é™¤ä¸‹è½½é“¾æ¥æ–‡æœ¬ï¼Œå¾—åˆ°æ ‡é¢˜
        if link_text in item_text:
            title = item_text.replace(link_text, '').strip()
            if title:
                return title
        
        # å¦‚æœä¸Šé¢çš„æ–¹æ³•ä¸è¡Œï¼Œå°è¯•æŸ¥æ‰¾ç‰¹å®šçš„æ–‡æœ¬å…ƒç´ 
        # æŸ¥æ‰¾åŒ…å«äº§å“åç§°çš„æ–‡æœ¬èŠ‚ç‚¹
        text_nodes = item.find_all(text=True, recursive=True)
        for text in text_nodes:
            text = text.strip()
            if text and text != link_text and len(text) > 3:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«äº§å“ç›¸å…³ä¿¡æ¯
                if any(keyword in text for keyword in ['å²›ç”µ', 'äº§å“', 'é€‰å‹', 'æŒ‡å—', 'SRS', 'SR', 'FP', 'MR', 'SD', 'HCM', 'PAC', 'è¯´æ˜ä¹¦', 'è§„æ ¼', 'è½¯ä»¶', 'åè®®', 'æµç¨‹å›¾']):
                    return text
        
        # æœ€åå°è¯•ï¼šæŸ¥æ‰¾çˆ¶çº§å…ƒç´ ä¸­çš„æ ‡é¢˜
        parent = item.parent
        if parent:
            # æŸ¥æ‰¾æ ‡é¢˜æ ‡ç­¾
            title_elem = parent.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if title_text and title_text != link_text:
                    return title_text
        
        return ""

    def extract_detailed_title(self, link, soup) -> str:
        """
        å°è¯•ä»é“¾æ¥çš„ä¸Šä¸‹æ–‡è·å–æ›´è¯¦ç»†çš„æ ‡é¢˜ä¿¡æ¯
        
        Args:
            link: ä¸‹è½½é“¾æ¥å…ƒç´ 
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            è¯¦ç»†çš„æ ‡é¢˜ä¿¡æ¯
        """
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾çˆ¶çº§å…ƒç´ ä¸­çš„æ ‡é¢˜
        parent = link.parent
        if parent:
            # æŸ¥æ‰¾çˆ¶çº§å…ƒç´ ä¸­çš„æ ‡é¢˜æ ‡ç­¾
            title_elem = parent.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'strong', 'b'])
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if title_text and len(title_text) > 2:
                    return title_text
            
            # æŸ¥æ‰¾çˆ¶çº§å…ƒç´ ä¸­çš„å…¶ä»–æ–‡æœ¬å†…å®¹
            parent_text = parent.get_text(strip=True)
            if parent_text and len(parent_text) > 10:
                # æå–æœ‰æ„ä¹‰çš„æ–‡æœ¬ç‰‡æ®µ
                lines = [line.strip() for line in parent_text.split('\n') if line.strip()]
                for line in lines:
                    if len(line) > 5 and any(keyword in line for keyword in ['SRS', 'SR', 'FP', 'MR', 'SD', 'HCM', 'PAC', 'è¯´æ˜ä¹¦', 'è§„æ ¼', 'è½¯ä»¶', 'åè®®', 'æµç¨‹å›¾']):
                        return line
        
        # æ–¹æ³•2ï¼šæŸ¥æ‰¾å…„å¼Ÿå…ƒç´ ä¸­çš„æ ‡é¢˜
        siblings = link.find_previous_siblings()
        for sibling in siblings[:3]:  # åªæ£€æŸ¥å‰3ä¸ªå…„å¼Ÿå…ƒç´ 
            if sibling.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'strong', 'b', 'p']:
                sibling_text = sibling.get_text(strip=True)
                if sibling_text and len(sibling_text) > 5:
                    return sibling_text
        
        # æ–¹æ³•3ï¼šæŸ¥æ‰¾é¡µé¢ä¸­çš„è¡¨æ ¼æˆ–åˆ—è¡¨ç»“æ„
        # å¦‚æœé“¾æ¥åœ¨è¡¨æ ¼ä¸­ï¼Œå°è¯•è·å–è¡Œæˆ–åˆ—çš„æ ‡é¢˜
        table = link.find_parent('table')
        if table:
            # æŸ¥æ‰¾å½“å‰è¡Œ
            row = link.find_parent('tr')
            if row:
                cells = row.find_all(['td', 'th'])
                for cell in cells:
                    cell_text = cell.get_text(strip=True)
                    if cell_text and len(cell_text) > 5 and any(keyword in cell_text for keyword in ['SRS', 'SR', 'FP', 'MR', 'SD', 'HCM', 'PAC']):
                        return cell_text
        
        return ""

    def extract_filename_from_title(self, title: str, module_name: str = "") -> str:
        """
        ä»æ ‡é¢˜ä¸­æå–æ–‡ä»¶åï¼Œä¿æŒåŸå§‹æ ¼å¼
        
        Args:
            title: æ ‡é¢˜æ–‡æœ¬
            module_name: æ¨¡å—åç§°ï¼ˆå½“å‰æœªä½¿ç”¨ï¼Œä¿æŒæ¥å£å…¼å®¹æ€§ï¼‰
            
        Returns:
            æ–‡ä»¶å
        """
        # ä¿æŒåŸå§‹æ ‡é¢˜æ ¼å¼ï¼Œåªç§»é™¤æ–‡ä»¶ç³»ç»Ÿä¸å…è®¸çš„ç‰¹æ®Šå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*\[\]{}]', '', title.strip())
        
        # ç¡®ä¿æ–‡ä»¶åä»¥.pdfç»“å°¾
        if not filename.lower().endswith('.pdf'):
            filename += '.pdf'
            
        return filename

    def get_subcategories(self, classid: int) -> List[Dict]:
        """
        è·å–å­åˆ†ç±»ä¿¡æ¯
        
        Args:
            classid: åˆ†ç±»ID
            
        Returns:
            å­åˆ†ç±»åˆ—è¡¨
        """
        url = f"{self.base_url}/articles.php?classid={classid}"
        response = self.safe_request(url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        subcategories = []
        
        # æŸ¥æ‰¾å·¦ä¾§å¯¼èˆªèœå•ä¸­çš„å­åˆ†ç±»
        nav_links = soup.find_all('a', href=True)
        
        for link in nav_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # åŒ¹é…å­åˆ†ç±»é“¾æ¥æ¨¡å¼
            if 'classid=' in href and text and len(text) > 1:
                # æå–classid
                match = re.search(r'classid=(\d+)', href)
                if match:
                    sub_classid = int(match.group(1))
                    if sub_classid != classid:  # æ’é™¤å½“å‰åˆ†ç±»
                        full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                        subcategories.append({
                            'name': text,
                            'classid': sub_classid,
                            'url': full_url
                        })
        
        # å»é‡
        seen = set()
        unique_subcategories = []
        for sub in subcategories:
            key = (sub['name'], sub['classid'])
            if key not in seen:
                seen.add(key)
                unique_subcategories.append(sub)
        
        self.logger.info(f"æ‰¾åˆ° {len(unique_subcategories)} ä¸ªå­åˆ†ç±»")
        return unique_subcategories

    def download_file(self, url: str, filepath: Path, title: str = "", module_name: str = "") -> bool:
        """
        ä¸‹è½½æ–‡ä»¶ï¼ˆå¢é‡ç‰ˆæœ¬ï¼‰
        
        Args:
            url: æ–‡ä»¶URL
            filepath: ä¿å­˜è·¯å¾„
            title: æ–‡ä»¶æ ‡é¢˜
            module_name: æ¨¡å—åç§°ï¼Œç”¨äºç”Ÿæˆæ›´æœ‰æ„ä¹‰çš„æ–‡ä»¶å
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼‰
            if url in self.processed_urls:
                self.logger.info(f"URLå·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {title or 'æœªçŸ¥æ–‡ä»¶'}")
                self.stats['skipped_files'] += 1
                return True
            
            # å¦‚æœæä¾›äº†titleï¼Œé‡æ–°ç”Ÿæˆæ›´å¥½çš„æ–‡ä»¶å
            if title and title.strip():
                better_filename = self.extract_filename_from_title(title, module_name)
                new_filepath = filepath.parent / better_filename
                
                # æ£€æŸ¥åŸå§‹æ–‡ä»¶åæ˜¯å¦å·²å­˜åœ¨ï¼ˆä¸è‡ªåŠ¨ç”Ÿæˆç¼–å·ï¼‰
                if new_filepath.exists():
                    self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {new_filepath.name}")
                    # å³ä½¿è·³è¿‡ï¼Œä¹Ÿè¦è®°å½•URLå·²å¤„ç†ï¼Œé¿å…é‡å¤æ£€æŸ¥
                    self.processed_urls.add(url)
                    self.stats['skipped_files'] += 1
                    return True
                
                filepath = new_filepath
            else:
                # æ£€æŸ¥åŸå§‹è·¯å¾„æ˜¯å¦å·²å­˜åœ¨
                if filepath.exists():
                    self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filepath.name}")
                    # å³ä½¿è·³è¿‡ï¼Œä¹Ÿè¦è®°å½•URLå·²å¤„ç†
                    self.processed_urls.add(url)
                    self.stats['skipped_files'] += 1
                    return True
            
            # åˆ›å»ºç›®å½•
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½æ–‡ä»¶
            self.logger.info(f"å¼€å§‹ä¸‹è½½: {title or filepath.name} -> {filepath.name}")
            response = self.safe_request(url)
            if not response:
                return False
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            file_size = filepath.stat().st_size
            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {filepath.name} ({file_size} bytes)")
            
            # è®°å½•å¤„ç†çš„URL
            self.processed_urls.add(url)
            
            # æ·»åŠ åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
            self.new_files.append({
                'type': 'PDF' if filepath.suffix.lower() == '.pdf' else 'æ–‡æ¡£',
                'title': title or filepath.stem,
                'path': str(filepath),
                'url': url,
                'size': file_size
            })
            
            self.stats['downloaded_files'] += 1
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            self.stats['failed_files'] += 1
            return False

    def crawl_module(self, module_name: str, module_config: Dict):
        """
        çˆ¬å–æŒ‡å®šæ¨¡å—
        
        Args:
            module_name: æ¨¡å—åç§°
            module_config: æ¨¡å—é…ç½®
        """
        self.logger.info(f"å¼€å§‹çˆ¬å–æ¨¡å—: {module_name}")
        
        classid = module_config['classid']
        folder_name = module_config['folder_name']
        has_subcategories = module_config['has_subcategories']
        
        module_dir = self.base_dir / folder_name
        module_dir.mkdir(parents=True, exist_ok=True)
        
        if has_subcategories:
            # å¤„ç†æœ‰å­åˆ†ç±»çš„æ¨¡å—
            if module_name == 'äº§å“èµ„æ–™ä¸‹è½½':
                self.crawl_product_data_download(classid, module_dir)
            elif module_name == 'åœäº§äº§å“':
                self.crawl_discontinued_products(classid, module_dir)
        else:
            # å¤„ç†æ²¡æœ‰å­åˆ†ç±»çš„æ¨¡å—ï¼ˆæ”¯æŒç¿»é¡µï¼‰
            if module_name == 'äº§å“è§„æ ¼ä¹¦':
                # äº§å“è§„æ ¼ä¹¦éœ€è¦ç¿»é¡µå¤„ç†
                self.crawl_module_with_pagination(classid, module_dir, module_name)
            else:
                # å…¶ä»–æ¨¡å—æŒ‰åŸæ¥çš„æ–¹å¼å¤„ç†
                url = f"{self.base_url}/articles.php?classid={classid}"
                downloads = self.parse_download_page(url)
                
                for download in downloads:
                    self.stats['total_files'] += 1
                    # ä½¿ç”¨titleæ¥ç”Ÿæˆæ›´å¥½çš„æ–‡ä»¶åï¼Œä¸å†ä¾èµ–åŸå§‹filename
                    title = download.get('title', 'æ–‡æ¡£')
                    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶è·¯å¾„ï¼Œå®é™…è·¯å¾„ä¼šåœ¨download_fileä¸­é‡æ–°ç”Ÿæˆ
                    temp_filepath = module_dir / "temp.pdf"
                    
                    self.download_file(download['url'], temp_filepath, title, module_name)
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    def crawl_product_data_download(self, classid: int, base_dir: Path):
        """
        çˆ¬å–äº§å“èµ„æ–™ä¸‹è½½æ¨¡å—ï¼ˆæŒ‰äº§å“åˆ†ç±»ä¿å­˜ï¼‰
        
        Args:
            classid: åˆ†ç±»ID
            base_dir: åŸºç¡€ç›®å½•
        """
        self.logger.info("çˆ¬å–äº§å“èµ„æ–™ä¸‹è½½æ¨¡å—")
        
        # æ˜ç¡®å®šä¹‰æ‰€æœ‰äº§å“åˆ†ç±»çš„classidï¼ŒåŸºäºä½ æä¾›çš„HTMLç»“æ„
        known_product_categories = [
            {'name': 'SRS1/3/4/5', 'classid': 27},
            {'name': 'SRS11A/12A/13A/14A', 'classid': 14},
            {'name': 'SR91/92/93/94', 'classid': 15},
            {'name': 'FP93', 'classid': 16},
            {'name': 'MR13', 'classid': 17},
            {'name': 'SR82/83/84', 'classid': 18},
            {'name': 'FP33/34', 'classid': 19},
            {'name': 'SR23A', 'classid': 20},
            {'name': 'FP23A', 'classid': 21},
            {'name': 'æ¨¡å—å‹è°ƒèŠ‚å™¨', 'classid': 22},  # è¿™ä¸ªä¹‹å‰è¢«æ¼æ‰äº†
            {'name': 'SD24A', 'classid': 28},
            {'name': 'SD17', 'classid': 33},
            {'name': 'HCMäººæœºç•Œé¢', 'classid': 29},
            {'name': 'PAC26/35/36/46', 'classid': 31}
        ]
        
        self.logger.info(f"å°†çˆ¬å– {len(known_product_categories)} ä¸ªäº§å“åˆ†ç±»")
        
        # ä¾æ¬¡çˆ¬å–æ¯ä¸ªäº§å“åˆ†ç±»
        for category in known_product_categories:
            self.logger.info(f"å¼€å§‹çˆ¬å–äº§å“åˆ†ç±»: {category['name']} (classid={category['classid']})")
            
            # åˆ›å»ºäº§å“åˆ†ç±»ç›®å½•
            category_dir = base_dir / self.sanitize_folder_name(category['name'])
            category_dir.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºåˆ†ç±»URL
            category_url = f"{self.base_url}/articles.php?classid={category['classid']}"
            
            # çˆ¬å–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰é¡µé¢ï¼ˆåŒ…æ‹¬ç¿»é¡µï¼‰
            self.crawl_category_pages(category_url, category_dir, category['name'])
            
            time.sleep(2)  # åˆ†ç±»é—´æš‚åœ
    
    def crawl_category_pages(self, category_url: str, category_dir: Path, category_name: str):
        """
        çˆ¬å–æŒ‡å®šäº§å“åˆ†ç±»çš„æ‰€æœ‰é¡µé¢ï¼ˆåŒ…æ‹¬ç¿»é¡µï¼‰
        
        Args:
            category_url: åˆ†ç±»é¡µé¢URL
            category_dir: åˆ†ç±»ç›®å½•
            category_name: åˆ†ç±»åç§°
        """
        page_num = 1
        current_url = category_url
        
        while current_url:
            self.logger.info(f"çˆ¬å– {category_name} ç¬¬ {page_num} é¡µ: {current_url}")
            
            # è§£æå½“å‰é¡µé¢
            downloads = self.parse_download_page(current_url)
            
            if not downloads:
                self.logger.info(f"{category_name} ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°ä¸‹è½½é“¾æ¥")
                break
            
            # ä¸‹è½½å½“å‰é¡µé¢çš„æ–‡ä»¶
            for download in downloads:
                self.stats['total_files'] += 1
                title = download.get('title', 'æ–‡æ¡£')
                
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶è·¯å¾„ï¼Œå®é™…è·¯å¾„ä¼šåœ¨download_fileä¸­é‡æ–°ç”Ÿæˆ
                temp_filepath = category_dir / "temp.pdf"
                
                self.download_file(download['url'], temp_filepath, title, category_name)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
            next_page_url = self.find_next_page(current_url)
            if next_page_url and next_page_url != current_url:
                current_url = next_page_url
                page_num += 1
                time.sleep(2)  # é¡µé¢é—´æš‚åœ
            else:
                break
        
        self.logger.info(f"å®Œæˆçˆ¬å–äº§å“åˆ†ç±»: {category_name}ï¼Œå…± {page_num} é¡µ")
    
    def find_next_page(self, current_url: str) -> Optional[str]:
        """
        æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
        
        Args:
            current_url: å½“å‰é¡µé¢URL
            
        Returns:
            ä¸‹ä¸€é¡µURLæˆ–None
        """
        response = self.safe_request(current_url)
        if not response:
            return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾å²›ç”µç½‘ç«™ç‰¹æœ‰çš„åˆ†é¡µå¯¼èˆª
        # æŸ¥æ‰¾åŒ…å«"å½“å‰é¡µ:X/Y/æ€»è®°å½•:Zæ¡"çš„åˆ†é¡µä¿¡æ¯
        pagination_text = soup.find(text=lambda x: x and 'å½“å‰é¡µ:' in x and 'æ€»è®°å½•:' in x)
        if pagination_text:
            self.logger.info(f"æ‰¾åˆ°åˆ†é¡µä¿¡æ¯: {pagination_text.strip()}")
            
            # æå–å½“å‰é¡µå’Œæ€»é¡µæ•°
            page_match = re.search(r'å½“å‰é¡µ:(\d+)/(\d+)', pagination_text)
            if page_match:
                current_page = int(page_match.group(1))
                total_pages = int(page_match.group(2))
                
                if current_page < total_pages:
                    # æ„å»ºä¸‹ä¸€é¡µURL
                    next_page = current_page + 1
                    if 'p=' in current_url:
                        # æ›¿æ¢é¡µç å‚æ•°
                        next_url = re.sub(r'p=\d+', f'p={next_page}', current_url)
                    else:
                        # æ·»åŠ é¡µç å‚æ•°
                        separator = '&' if '?' in current_url else '?'
                        next_url = f"{current_url}{separator}p={next_page}"
                    
                    self.logger.info(f"æ„å»ºä¸‹ä¸€é¡µURL: {next_url}")
                    return next_url
        
        # æ–¹æ³•2ï¼šæŸ¥æ‰¾åˆ†é¡µå¯¼èˆªé“¾æ¥
        pagination = soup.find(['div', 'ul'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['page', 'pagination', 'nav']))
        
        if pagination:
            # æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"æˆ–">"é“¾æ¥
            next_links = pagination.find_all('a', text=lambda x: x and any(keyword in x for keyword in ['ä¸‹ä¸€é¡µ', '>', 'next', 'Â»']))
            
            for link in next_links:
                href = link.get('href', '')
                if href and href != current_url:
                    if href.startswith('http'):
                        return href
                    elif href.startswith('/'):
                        return urljoin(self.base_url, href)
                    else:
                        return urljoin(current_url, href)
        
        # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ•°å­—é¡µç é“¾æ¥
        page_links = soup.find_all('a', href=True)
        current_page_num = self.extract_page_number(current_url)
        
        for link in page_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—é¡µç 
            if text.isdigit():
                link_page_num = int(text)
                if link_page_num == current_page_num + 1:
                    if href.startswith('http'):
                        return href
                    elif href.startswith('/'):
                        return urljoin(self.base_url, href)
                    else:
                        return urljoin(current_url, href)
        
        return None
    
    def extract_page_number(self, url: str) -> int:
        """
        ä»URLä¸­æå–é¡µç 
        
        Args:
            url: URLå­—ç¬¦ä¸²
            
        Returns:
            é¡µç ï¼Œé»˜è®¤ä¸º1
        """
        # å°è¯•ä»URLå‚æ•°ä¸­æå–é¡µç 
        match = re.search(r'[?&]page=(\d+)', url)
        if match:
            return int(match.group(1))
        
        # å°è¯•ä»è·¯å¾„ä¸­æå–é¡µç 
        match = re.search(r'/(\d+)(?:\.html?)?$', url)
        if match:
            return int(match.group(1))
        
        return 1

    def crawl_module_with_pagination(self, classid: int, base_dir: Path, module_name: str):
        """
        çˆ¬å–éœ€è¦ç¿»é¡µçš„æ¨¡å—ï¼ˆå¦‚äº§å“è§„æ ¼ä¹¦ï¼‰
        
        Args:
            classid: åˆ†ç±»ID
            base_dir: åŸºç¡€ç›®å½•
            module_name: æ¨¡å—åç§°
        """
        self.logger.info(f"å¼€å§‹çˆ¬å–å¸¦ç¿»é¡µçš„æ¨¡å—: {module_name}")
        
        page_num = 1
        current_url = f"{self.base_url}/articles.php?classid={classid}"
        total_downloads = 0
        
        while current_url:
            self.logger.info(f"çˆ¬å– {module_name} ç¬¬ {page_num} é¡µ: {current_url}")
            
            # è§£æå½“å‰é¡µé¢
            downloads = self.parse_download_page(current_url)
            
            if not downloads:
                self.logger.info(f"{module_name} ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°ä¸‹è½½é“¾æ¥")
                break
            
            # ä¸‹è½½å½“å‰é¡µé¢çš„æ–‡ä»¶
            for download in downloads:
                self.stats['total_files'] += 1
                title = download.get('title', 'æ–‡æ¡£')
                
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶è·¯å¾„ï¼Œå®é™…è·¯å¾„ä¼šåœ¨download_fileä¸­é‡æ–°ç”Ÿæˆ
                temp_filepath = base_dir / "temp.pdf"
                
                if self.download_file(download['url'], temp_filepath, title, module_name):
                    total_downloads += 1
                
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
            next_page_url = self.find_next_page(current_url)
            if next_page_url and next_page_url != current_url:
                current_url = next_page_url
                page_num += 1
                time.sleep(2)  # é¡µé¢é—´æš‚åœ
            else:
                break
        
        self.logger.info(f"å®Œæˆçˆ¬å–æ¨¡å—: {module_name}ï¼Œå…± {page_num} é¡µï¼Œ{total_downloads} ä¸ªæ–‡ä»¶")

    def crawl_discontinued_products(self, classid: int, base_dir: Path):
        """
        çˆ¬å–åœäº§äº§å“æ¨¡å—ï¼ˆå¤šå±‚çº§ç»“æ„ï¼‰
        
        Args:
            classid: åˆ†ç±»ID
            base_dir: åŸºç¡€ç›®å½•
        """
        self.logger.info("çˆ¬å–åœäº§äº§å“æ¨¡å—")
        
        # æ˜ç¡®å®šä¹‰åœäº§äº§å“çš„åˆ†ç±»IDå’Œåç§°
        discontinued_products = [
            {'name': 'SD16A', 'classid': 23, 'url': 'http://www.yhxml.com/articles.php?classid=23'},
            {'name': 'SR1/3/4', 'classid': 38, 'url': 'http://www.yhxml.com/articles.php?classid=38'},
            {'name': 'SR253', 'classid': 41, 'url': 'http://www.yhxml.com/articles.php?classid=41'},
            {'name': 'FP21', 'classid': 42, 'url': 'http://www.yhxml.com/articles.php?classid=42'}
        ]
        
        for subcat in discontinued_products:
            self.logger.info(f"å¤„ç†åœäº§äº§å“å­åˆ†ç±»: {subcat['name']}")
            
            subcat_dir = base_dir / self.sanitize_folder_name(subcat['name'])
            subcat_dir.mkdir(parents=True, exist_ok=True)
            
            # çˆ¬å–å­åˆ†ç±»é¡µé¢
            downloads = self.parse_download_page(subcat['url'])
            
            for download in downloads:
                self.stats['total_files'] += 1
                # ä½¿ç”¨titleæ¥ç”Ÿæˆæ›´å¥½çš„æ–‡ä»¶å
                title = download.get('title', 'æ–‡æ¡£')
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶è·¯å¾„ï¼Œå®é™…è·¯å¾„ä¼šåœ¨download_fileä¸­é‡æ–°ç”Ÿæˆ
                temp_filepath = subcat_dir / "temp.pdf"
                
                self.download_file(download['url'], temp_filepath, title, 'åœäº§äº§å“')
                time.sleep(1)

    def sanitize_folder_name(self, name: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶å¤¹åç§°
        
        Args:
            name: åŸå§‹åç§°
            
        Returns:
            æ¸…ç†åçš„åç§°
        """
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'[<>:"/\\|?*]', '_', name.strip())
        name = re.sub(r'\s+', '_', name)
        return name

    def save_progress(self):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        progress_file = self.base_dir / 'crawl_progress.json'
        progress_data = {
            'timestamp': time.time(),
            'stats': self.stats,
            'completed_modules': []
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.logger.info("å¼€å§‹çˆ¬å–åŒ—äº¬å‹æ–¹é‡‘æ³°ä¸‹è½½ä¸­å¿ƒ")
        self.logger.info(f"ä¿å­˜ç›®å½•: {self.base_dir}")
        
        start_time = time.time()
        
        try:
            # çˆ¬å–æ‰€æœ‰æ¨¡å—
            for module_name, module_config in self.modules.items():
                self.logger.info(f"=" * 50)
                self.crawl_module(module_name, module_config)
                time.sleep(2)  # æ¨¡å—é—´æš‚åœ
                
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            # ä¿å­˜è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
            self.save_progress()
            
            # ä¿å­˜å·²å¤„ç†çš„URLs
            self.save_processed_urls()
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            duration = end_time - start_time
            
            self.logger.info("=" * 50)
            self.logger.info("çˆ¬å–å®Œæˆ!")
            self.logger.info(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
            self.logger.info(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {self.stats['downloaded_files']}")
            self.logger.info(f"è·³è¿‡æ–‡ä»¶: {self.stats['skipped_files']}")
            self.logger.info(f"ä¸‹è½½å¤±è´¥: {self.stats['failed_files']}")
            self.logger.info(f"ä¿å­˜ç›®å½•: {self.base_dir}")
            
            # å‘é€é€šçŸ¥
            if self.new_files:
                self.send_notifications()

    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥"""
        try:
            timestamp = str(round(time.time() * 1000))
            string_to_sign = f'{timestamp}\n{self.SECRET}'
            hmac_code = hmac.new(self.SECRET.encode(), string_to_sign.encode(), hashlib.sha256).digest()
            sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))

            url = f'https://oapi.dingtalk.com/robot/send?access_token={self.ACCESS_TOKEN}&timestamp={timestamp}&sign={sign}'
            headers = {'Content-Type': 'application/json'}
            data = {
                "msgtype": "text",
                "text": {"content": message},
                "at": {"isAtAll": False}
            }

            response = requests.post(url, json=data, headers=headers)
            self.logger.info(f"é’‰é’‰é€šçŸ¥å“åº”ï¼š{response.status_code} {response.text}")
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {e}")
            return False

    def send_notifications(self):
        """å‘é€æ–°å¢æ–‡ä»¶é€šçŸ¥"""
        try:
            if not self.new_files:
                return
            
            # æ§åˆ¶å°é€šçŸ¥
            self.logger.info(f"\nğŸ‰ çˆ¬å–å®Œæˆé€šçŸ¥:")
            self.logger.info("=" * 60)
            self.logger.info(f"ğŸ“Š å‘ç° {len(self.new_files)} ä¸ªæ–°æ–‡ä»¶:")
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = {}
            for file_info in self.new_files:
                file_type = file_info['type']
                type_counts[file_type] = type_counts.get(file_type, 0) + 1
            
            for file_type, count in type_counts.items():
                self.logger.info(f"  ğŸ“ {file_type}: {count} ä¸ª")
            
            self.logger.info(f"\nğŸ“‚ æœ€æ–°æ–‡ä»¶é¢„è§ˆ:")
            for file_info in self.new_files[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                size_str = f" ({file_info['size']/1024:.1f}KB)" if 'size' in file_info else ""
                self.logger.info(f"  ğŸ“„ {file_info['title']}{size_str}")
            
            if len(self.new_files) > 5:
                self.logger.info(f"  ... è¿˜æœ‰ {len(self.new_files) - 5} ä¸ªæ–‡ä»¶")
                
            self.logger.info(f"\nğŸ’¾ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³: {self.base_dir}")
        
            # é’‰é’‰é€šçŸ¥
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            total_files = len(self.new_files)
            success_rate = 100.0 if self.stats['failed_files'] == 0 else (self.stats['downloaded_files'] / (self.stats['downloaded_files'] + self.stats['failed_files'])) * 100
            
            if self.is_first_run:
                # ç¬¬ä¸€æ¬¡å…¨é‡çˆ¬å–é€šçŸ¥
                message = f"""âœ… å‹æ–¹é‡‘æ³° çˆ¬å–æˆåŠŸï¼Œè¯·åŠæ—¶å®¡æ ¸

ğŸ“Š ä¸‹è½½ç»Ÿè®¡:
  æˆåŠŸä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶
  æ€»æ–‡ä»¶æ•°: {self.stats['total_files']} ä¸ªæ–‡ä»¶
  æˆåŠŸç‡: {success_rate:.1f}%

ğŸ“ æ–‡ä»¶å­˜æ”¾è·¯å¾„: {self.base_dir}
â° å®Œæˆæ—¶é—´: {current_time}"""
            else:
                # å¢é‡çˆ¬å–é€šçŸ¥
                message = f"""âœ… å‹æ–¹é‡‘æ³° å¢é‡çˆ¬å–æˆåŠŸï¼Œè¯·åŠæ—¶å®¡æ ¸

ğŸ“Š ä¸‹è½½ç»Ÿè®¡:
  æˆåŠŸä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶
  æ€»æ–‡ä»¶æ•°: {self.stats['total_files']} ä¸ªæ–‡ä»¶
  æˆåŠŸç‡: {success_rate:.1f}%
æ–‡ä»¶æ˜ç»†ï¼š"""
                
                # æ·»åŠ æ–‡ä»¶æ˜ç»†
                for file_info in self.new_files:
                    # æ„å»ºç›¸å¯¹è·¯å¾„ï¼ˆä»åŸºç¡€ç›®å½•å¼€å§‹ï¼‰
                    relative_path = file_info['path'].replace(str(self.base_dir) + '/', '')
                    message += f"\n{relative_path}"
                
                message += f"""

ğŸ“ æ–‡ä»¶å­˜æ”¾è·¯å¾„: {self.base_dir}
â° å®Œæˆæ—¶é—´: {current_time}"""
            
            # å‘é€é’‰é’‰é€šçŸ¥
            self.send_dingtalk_notification(message)
            
        except Exception as e:
            self.logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„"å‹æ–¹é‡‘æ³°ä¸‹è½½"æ–‡ä»¶å¤¹
    base_dir = sys.argv[1] if len(sys.argv) > 1 else None
    
    spider = YHXMLSpider(base_dir)
    spider.run()


if __name__ == "__main__":
    main()
