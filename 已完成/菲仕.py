#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è²ä»•ç§‘æŠ€ (Physis) äº§å“èµ„æ–™ä¸‹è½½çˆ¬è™«
ç½‘ç«™ï¼šhttps://www.physis.com.cn/ProductCenter3992/index.aspx?lcid=31
åŠŸèƒ½ï¼š
1. çˆ¬å–æ‰€æœ‰äº§å“åˆ—è¡¨
2. è¿›å…¥æ¯ä¸ªäº§å“é¡µé¢ä¸‹è½½è§„æ ¼å‚æ•°PDF
3. ä¸‹è½½èµ„æ–™ä¸‹è½½æ¨¡å—çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆå‹å½•æ‰‹å†Œã€3Då›¾çº¸ç­‰ï¼‰
"""

import os
import sys
import time
import requests
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
import logging
from pathlib import Path
import re
from typing import Dict, List, Tuple, Optional
import json
import pickle
import hashlib
import hmac
import base64
import urllib.parse
from datetime import datetime, date, timedelta
try:
    import html2text
    from weasyprint import HTML, CSS
    HTML2TEXT_AVAILABLE = True
    WEASYPRINT_AVAILABLE = True
except ImportError:
    HTML2TEXT_AVAILABLE = False
    WEASYPRINT_AVAILABLE = False
    print("âš ï¸ html2text æˆ– weasyprint æœªå®‰è£…ï¼ŒPDFç”ŸæˆåŠŸèƒ½å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")

from io import BytesIO
import tempfile

class PhysisSpider:
    def __init__(self, base_dir: str = None, monitor_mode: bool = False, category_filter: str = None):
        """
        åˆå§‹åŒ–è²ä»•ç§‘æŠ€çˆ¬è™«
        
        Args:
            base_dir: ä¸‹è½½æ–‡ä»¶ä¿å­˜çš„åŸºç¡€ç›®å½•
            monitor_mode: ç›‘æ§æ¨¡å¼ï¼Œåªæ£€æµ‹æ–°æ–‡ä»¶è€Œä¸ä¸‹è½½
            category_filter: ç±»åˆ«è¿‡æ»¤å™¨ï¼Œå¦‚æœæŒ‡å®šåˆ™åªçˆ¬å–è¯¥ç±»åˆ«çš„äº§å“
        """
        if base_dir is None:
            # é»˜è®¤ä½¿ç”¨æœåŠ¡å™¨æ ‡å‡†ç›®å½•
            base_dir = "/srv/downloads/approved/è²ä»•"
        
        self.base_url = "https://www.physis.com.cn"
        self.base_dir = Path(base_dir)
        self.session = requests.Session()
        self.monitor_mode = monitor_mode
        self.category_filter = category_filter
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # åˆ›å»ºåŸºç¡€ç›®å½•
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.ACCESS_TOKEN = "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        self.SECRET = "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb"
        
        # å·²å¤„ç†URLè®°å½•
        self.processed_urls = self.load_processed_urls()
        self.new_files = []  # æ–°å¢æ–‡ä»¶åˆ—è¡¨
        
        # ä¸‹è½½ç»Ÿè®¡
        self.stats = {
            'total_products': 0,
            'processed_products': 0,
            'downloaded_files': 0,
            'failed_files': 0,
            'skipped_files': 0
        }
        
        # äº§å“ä¸­å¿ƒURL
        self.product_center_url = "/ProductCenter3992/index.aspx?lcid=31"
        
        # ä¸‹è½½ä¸­å¿ƒURLs
        self.download_center_urls = [
            "/DownloadCenter/list.aspx?lcid=8",   # å‹å½•æ‰‹å†Œ
            "/DownloadCenter/list.aspx?lcid=18",  # 3Då›¾çº¸  
            "/DownloadCenter/list.aspx?lcid=7",   # æ“ä½œè¯´æ˜ä¹¦
            "/DownloadCenter/list.aspx?lcid=6",   # é©±åŠ¨å™¨é…ç½®è½¯ä»¶
        ]
        
        # æ—¥æœŸè¿‡æ»¤ï¼š2024å¹´11æœˆå
        self.filter_date = datetime(2024, 11, 1)

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_file = self.base_dir / 'spider.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URL"""
        urls_file = self.base_dir / 'processed_urls.pkl'
        if urls_file.exists():
            try:
                with open(urls_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
        
    def save_processed_urls(self):
        """ä¿å­˜å·²å¤„ç†çš„URL"""
        urls_file = self.base_dir / 'processed_urls.pkl'
        with open(urls_file, 'wb') as f:
            pickle.dump(self.processed_urls, f)

    def safe_request(self, url: str, timeout: int = 30) -> Optional[requests.Response]:
        """
        å®‰å…¨çš„HTTPè¯·æ±‚
        
        Args:
            url: è¯·æ±‚çš„URL
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            Responseå¯¹è±¡æˆ–None
        """
        try:
            self.logger.info(f"è¯·æ±‚URL: {url}")
            response = self.session.get(url, timeout=timeout, verify=False)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None

    def get_product_categories(self) -> List[Dict]:
        """
        è·å–äº§å“åˆ†ç±»åˆ—è¡¨
        
        Returns:
            äº§å“åˆ†ç±»åˆ—è¡¨
        """
        categories = []
        
        # åŸºäºç½‘ç«™å®é™…ç»“æ„ï¼Œè²ä»•ç§‘æŠ€çš„äº§å“åˆ†ç±»
        category_mapping = {
            'ä¼ºæœç”µæœºç³»åˆ—': {
                'subcategories': [
                    'Ultract â…¢ç³»åˆ—æ ‡å‡†äº¤æµæ°¸ç£åŒæ­¥ä¼ºæœç”µæœº',
                    'TKç³»åˆ—æ°¸ç£åŒæ­¥åŠ›çŸ©ä¼ºæœç”µæœº', 
                    'XTç³»åˆ—ç›´é©±åŠ›çŸ©ç”µæœº',
                    'Expressç³»åˆ—äº¤æµæ°¸ç£åŒæ­¥ä¼ºæœç”µæœº'
                ]
            },
            'ä¼ºæœé©±åŠ¨å™¨ç³»åˆ—': {
                'subcategories': [
                    'PH590ç³»åˆ—é«˜æ€§èƒ½é€šç”¨å˜é¢‘å™¨',
                    'PD120ç³»åˆ—é«˜æ€§èƒ½ä¹¦æœ¬å‹ç½‘ç»œåŒ–ä¼ºæœé©±åŠ¨å™¨',
                    'AxN-PDç³»åˆ—å¤šä¼ ä¼ºæœé©±åŠ¨å™¨',
                    'PH600ç³»åˆ—é«˜æ€§èƒ½ç”µæ¶²ä¼ºæœé©±åŠ¨å™¨',
                    'AxNç³»åˆ—å…¨æ•°å­—äº¤æµä¼ºæœé©±åŠ¨å™¨',
                    'AxN-DCç³»åˆ—å…±ç›´æµæ¯çº¿å¤šè½´é©±åŠ¨å™¨',
                    'PH300ç³»åˆ—é«˜æ€§èƒ½é—­ç¯çŸ¢é‡é©±åŠ¨å™¨'
                ]
            },
            'OSAIæ§åˆ¶ç³»ç»Ÿ': {
                'subcategories': [
                    'OSAIæ•°æ§ç³»ç»Ÿ',
                    'OSAIæ“ä½œé¢æ¿',
                    'å·¥ä¸šPCï¼ˆIPCï¼‰',
                    'è¾“å…¥/è¾“å‡ºæ¨¡å—'
                ]
            },
            'ç›´æµæ— åˆ·ç”µæœºç³»åˆ—': {
                'subcategories': [
                    'BLDCå†…è½¬å­ç³»åˆ—ç›´æµæ— åˆ·ç”µæœº',
                    'BLDCå¤–è½¬å­ç³»åˆ—ç›´æµæ— åˆ·ç”µæœº'
                ]
            },
            'æ–°èƒ½æºæ±½è½¦ç”µé©±ç³»åˆ—': {
                'subcategories': [
                    'ä¸‰åˆä¸€åŠ¨åŠ›æ€»æˆ',
                    'ä¸»é©±ç”µæœºæ§åˆ¶å™¨',
                    'æ–°èƒ½æºæ±½è½¦ä¸»é©±ç”µæœº',
                    'å‡é€Ÿå™¨'
                ]
            }
        }
        
        return category_mapping

    def get_all_products(self) -> List[Dict]:
        """
        ä»äº§å“ä¸­å¿ƒè·å–æ‰€æœ‰äº§å“åˆ—è¡¨
        
        Returns:
            äº§å“åˆ—è¡¨
        """
        products = []
        
        try:
            # è®¿é—®äº§å“ä¸­å¿ƒé¡µé¢
            product_center_url = self.base_url + self.product_center_url
            response = self.safe_request(product_center_url)
            if not response:
                return products
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾äº§å“é“¾æ¥ - åŸºäºå®é™…é¡µé¢ç»“æ„åˆ†æ
            # äº§å“é€šå¸¸åœ¨å¯¼èˆªèœå•æˆ–äº§å“ç½‘æ ¼ä¸­
            product_links = soup.find_all('a', href=True)
            
            for link in product_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # åŒ¹é…äº§å“é¡µé¢çš„URLæ¨¡å¼
                if ('ProductCenter3992/info.aspx' in href and 
                    'itemid=' in href and 
                    text and len(text) > 2):
                    
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = self.base_url + href
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        full_url = urljoin(product_center_url, href)
                    
                    # å°è¯•åˆ†ç±»äº§å“
                    category = self.classify_product(text)
                    
                    products.append({
                        'name': text,
                        'url': full_url,
                        'category': category
                    })
                    
        except Exception as e:
            self.logger.error(f"è·å–äº§å“åˆ—è¡¨å¤±è´¥: {e}")
        
        return products

    def classify_product(self, product_name: str) -> str:
        """
        æ ¹æ®äº§å“åç§°åˆ†ç±»äº§å“
        
        Args:
            product_name: äº§å“åç§°
            
        Returns:
            äº§å“åˆ†ç±»
        """
        product_name_lower = product_name.lower()
        
        # æ ¹æ®äº§å“åç§°å…³é”®è¯åˆ†ç±»
        if any(keyword in product_name_lower for keyword in ['ultract', 'tkç³»åˆ—', 'xtç³»åˆ—', 'express', 'ä¼ºæœç”µæœº']):
            return 'ä¼ºæœç”µæœºç³»åˆ—'
        elif any(keyword in product_name_lower for keyword in ['ph590', 'pd120', 'axn', 'ph600', 'ph300', 'é©±åŠ¨å™¨', 'å˜é¢‘å™¨']):
            return 'ä¼ºæœé©±åŠ¨å™¨ç³»åˆ—'
        elif any(keyword in product_name_lower for keyword in ['osai', 'æ•°æ§', 'ipc', 'æ§åˆ¶']):
            return 'OSAIæ§åˆ¶ç³»ç»Ÿ'
        elif any(keyword in product_name_lower for keyword in ['bldc', 'ç›´æµ', 'æ— åˆ·']):
            return 'ç›´æµæ— åˆ·ç”µæœºç³»åˆ—'
        elif any(keyword in product_name_lower for keyword in ['æ–°èƒ½æº', 'æ±½è½¦', 'ç”µé©±', 'åŠ¨åŠ›æ€»æˆ']):
            return 'æ–°èƒ½æºæ±½è½¦ç”µé©±ç³»åˆ—'
        else:
            return 'å…¶ä»–äº§å“'

    def download_spec_params_as_pdf(self, product_url: str, product_name: str, save_dir: Path) -> bool:
        """
        ä¸‹è½½äº§å“çš„è§„æ ¼å‚æ•°ä¸ºPDF
        
        Args:
            product_url: äº§å“é¡µé¢URL
            product_name: äº§å“åç§°
            save_dir: ä¿å­˜ç›®å½•
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            response = self.safe_request(product_url)
            if not response:
                return False
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾è§„æ ¼å‚æ•°éƒ¨åˆ†
            spec_section = soup.find('div', class_='box_item box4')
            if not spec_section:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                spec_section = soup.find('div', string=re.compile('è§„æ ¼å‚æ•°'))
                if spec_section:
                    spec_section = spec_section.find_parent('div')
            
            if not spec_section:
                self.logger.warning(f"æœªæ‰¾åˆ°è§„æ ¼å‚æ•°éƒ¨åˆ†: {product_name}")
                return False
            
            # æå–è§„æ ¼å‚æ•°å†…å®¹
            spec_content = ""
            
            # æŸ¥æ‰¾è§„æ ¼å‚æ•°çš„å›¾ç‰‡
            spec_images = spec_section.find_all('img')
            if spec_images:
                spec_content += f"<h1>{product_name} - è§„æ ¼å‚æ•°</h1>\n"
                for img in spec_images:
                    img_src = img.get('src')
                    if img_src:
                        # æ„å»ºå®Œæ•´çš„å›¾ç‰‡URL
                        if img_src.startswith('/'):
                            img_url = self.base_url + img_src
                        elif img_src.startswith('http'):
                            img_url = img_src
                        else:
                            img_url = urljoin(product_url, img_src)
                        
                        spec_content += f'<img src="{img_url}" style="max-width: 100%; margin: 10px 0;" />\n'
            
            # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹ï¼Œä¹ŸåŒ…å«è¿›æ¥
            spec_text = spec_section.get_text(strip=True)
            if spec_text and not spec_images:
                spec_content += f"<h1>{product_name} - è§„æ ¼å‚æ•°</h1>\n"
                spec_content += f"<pre>{spec_text}</pre>\n"
            
            if not spec_content:
                self.logger.warning(f"è§„æ ¼å‚æ•°å†…å®¹ä¸ºç©º: {product_name}")
                return False
            
            # ç”ŸæˆPDFæ–‡ä»¶å
            pdf_filename = f"{self.sanitize_filename(product_name)}_è§„æ ¼å‚æ•°.pdf"
            pdf_path = save_dir / pdf_filename
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if pdf_path.exists():
                self.logger.info(f"è§„æ ¼å‚æ•°PDFå·²å­˜åœ¨ï¼Œè·³è¿‡: {pdf_filename}")
                return True
            
            # ç”ŸæˆPDF
            if WEASYPRINT_AVAILABLE:
                # ä½¿ç”¨weasyprintç”ŸæˆPDF
                html_content = f"""
                <!DOCTYPE html>
                <html>
                <head>
                    <meta charset="utf-8">
                    <title>{product_name} - è§„æ ¼å‚æ•°</title>
                    <style>
                        body {{ font-family: "Microsoft YaHei", Arial, sans-serif; margin: 20px; }}
                        h1 {{ color: #333; text-align: center; }}
                        img {{ max-width: 100%; height: auto; }}
                        pre {{ white-space: pre-wrap; }}
                    </style>
                </head>
                <body>
                    {spec_content}
                </body>
                </html>
                """
                
                HTML(string=html_content, base_url=self.base_url).write_pdf(str(pdf_path))
            else:
                # å¦‚æœæ²¡æœ‰weasyprintï¼Œä¿å­˜ä¸ºHTMLæ–‡ä»¶
                html_filename = f"{self.sanitize_filename(product_name)}_è§„æ ¼å‚æ•°.html"
                html_path = save_dir / html_filename
                
                html_content = f"""
                <!DOCTYPE html>
                <html>
                <head>
                    <meta charset="utf-8">
                    <title>{product_name} - è§„æ ¼å‚æ•°</title>
                    <style>
                        body {{ font-family: "Microsoft YaHei", Arial, sans-serif; margin: 20px; }}
                        h1 {{ color: #333; text-align: center; }}
                        img {{ max-width: 100%; height: auto; }}
                        pre {{ white-space: pre-wrap; }}
                    </style>
                </head>
                <body>
                    {spec_content}
                </body>
                </html>
                """
                
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                
                pdf_path = html_path  # æ›´æ–°è·¯å¾„å¼•ç”¨
                self.logger.info(f"è§„æ ¼å‚æ•°HTMLç”ŸæˆæˆåŠŸï¼ˆPDFåº“ä¸å¯ç”¨ï¼‰: {html_filename}")
            
            self.logger.info(f"è§„æ ¼å‚æ•°PDFç”ŸæˆæˆåŠŸ: {pdf_filename}")
            
            # è®°å½•åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
            self.new_files.append({
                'type': 'PDF',
                'title': f"{product_name}_è§„æ ¼å‚æ•°",
                'path': str(pdf_path),
                'url': product_url,
                'size': pdf_path.stat().st_size
            })
            
            self.stats['downloaded_files'] += 1
            return True
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè§„æ ¼å‚æ•°PDFå¤±è´¥ {product_name}: {e}")
            self.stats['failed_files'] += 1
            return False

    def download_product_resources(self, product_url: str, product_name: str, save_dir: Path) -> int:
        """
        ä¸‹è½½äº§å“çš„èµ„æ–™ä¸‹è½½æ¨¡å—æ–‡ä»¶
        
        Args:
            product_url: äº§å“é¡µé¢URL
            product_name: äº§å“åç§°
            save_dir: ä¿å­˜ç›®å½•
            
        Returns:
            ä¸‹è½½æˆåŠŸçš„æ–‡ä»¶æ•°é‡
        """
        downloaded_count = 0
        
        try:
            response = self.safe_request(product_url)
            if not response:
                return 0
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ–¹æ³•1: æŸ¥æ‰¾èµ„æ–™ä¸‹è½½éƒ¨åˆ† - æ–°çš„é€‰æ‹©å™¨ç­–ç•¥
            download_section = None
            
            # å°è¯•å¤šç§æ–¹å¼æ‰¾åˆ°èµ„æ–™ä¸‹è½½éƒ¨åˆ†
            selectors_to_try = [
                'div.auto.auto_1500',
                'div[class*="auto"]',
                'div[id*="download"]',
                'div[class*="download"]'
            ]
            
            for selector in selectors_to_try:
                sections = soup.select(selector)
                for section in sections:
                    if 'èµ„æ–™ä¸‹è½½' in section.get_text():
                        download_section = section
                        break
                if download_section:
                    break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥æœç´¢æ–‡æœ¬
            if not download_section:
                download_texts = soup.find_all(text=re.compile('èµ„æ–™ä¸‹è½½'))
                for text in download_texts:
                    parent = text.parent
                    while parent and parent.name != 'body':
                        if parent.name == 'div':
                            download_section = parent
                            break
                        parent = parent.parent
                    if download_section:
                        break
            
            if not download_section:
                self.logger.warning(f"æœªæ‰¾åˆ°èµ„æ–™ä¸‹è½½éƒ¨åˆ†: {product_name}")
                
                # æ–¹æ³•2: ç›´æ¥æŸ¥æ‰¾ä¸‹è½½é“¾æ¥ï¼ˆåŸºäºå®é™…é¡µé¢ç»“æ„ï¼‰
                # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ä¸‹è½½é“¾æ¥
                all_links = soup.find_all('a', href=True)
                valid_downloads = []
                
                for link in all_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹è½½é“¾æ¥ï¼ˆåŸºäºhrefæ¨¡å¼æˆ–æ–‡æœ¬å†…å®¹ï¼‰
                    if (href and text and 
                        (any(ext in href.lower() for ext in ['.pdf', '.doc', '.zip', '.rar']) or
                         any(keyword in text for keyword in ['ä¸‹è½½', 'æ–‡æ¡£', 'æ‰‹å†Œ', 'å›¾çº¸', 'å‚æ•°']) or
                         'download' in href.lower() or
                         'attachment' in href.lower())):
                        
                        # æ„å»ºå®Œæ•´URL
                        if href.startswith('/'):
                            file_url = self.base_url + href
                        elif href.startswith('http'):
                            file_url = href
                        else:
                            file_url = urljoin(product_url, href)
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
                        if file_url not in self.processed_urls:
                            valid_downloads.append((file_url, text))
                
                # ä¸‹è½½æ‰¾åˆ°çš„æ–‡ä»¶
                if valid_downloads:
                    self.logger.info(f"é€šè¿‡ç›´æ¥é“¾æ¥æœç´¢æ‰¾åˆ° {len(valid_downloads)} ä¸ªä¸‹è½½æ–‡ä»¶")
                    for file_url, title in valid_downloads:
                        if self.download_file(file_url, title, save_dir):
                            downloaded_count += 1
                            self.processed_urls.add(file_url)
                        time.sleep(1)
                
                return downloaded_count
            
            # æ–¹æ³•3: è§£ææ‰¾åˆ°çš„èµ„æ–™ä¸‹è½½éƒ¨åˆ†
            self.logger.info(f"æ‰¾åˆ°èµ„æ–™ä¸‹è½½éƒ¨åˆ†: {product_name}")
            
            # æŸ¥æ‰¾æ‰€æœ‰ä¸‹è½½åˆ†ç±»ï¼ˆå‹å½•æ‰‹å†Œã€3Då›¾çº¸ç­‰ï¼‰
            tab_sections = download_section.find_all('ul', class_='ul clearfix')
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ulç»“æ„ï¼Œå°è¯•å…¶ä»–ç»“æ„
            if not tab_sections:
                # æŸ¥æ‰¾æ‰€æœ‰åˆ—è¡¨é¡¹æˆ–ä¸‹è½½é“¾æ¥
                download_items = download_section.find_all(['li', 'a'], href=True)
                if download_items:
                    tab_sections = [download_section]  # å°†æ•´ä¸ªsectionä½œä¸ºä¸€ä¸ªtabå¤„ç†
            
            for tab_section in tab_sections:
                # è·å–åˆ†ç±»åç§°
                category_name = tab_section.get('data-name', 'äº§å“èµ„æ–™')
                
                # æŸ¥æ‰¾è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰ä¸‹è½½é“¾æ¥
                download_links = tab_section.find_all('a', href=True)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°aæ ‡ç­¾ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ä¸‹è½½å…ƒç´ 
                if not download_links:
                    # æŸ¥æ‰¾å¸¦æœ‰ä¸‹è½½ç›¸å…³æ–‡æœ¬çš„å…ƒç´ 
                    possible_links = tab_section.find_all(text=re.compile(r'[A-Z0-9_-]+\.[a-z]{2,4}$'))
                    for text in possible_links:
                        # ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶ååˆ›å»ºä¸‹è½½é“¾æ¥ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
                        parent = text.parent
                        if parent and parent.name == 'li':
                            # å°è¯•æ„å»ºä¸‹è½½URL
                            filename = text.strip()
                            # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®è²ä»•ç½‘ç«™çš„å®é™…ä¸‹è½½URLæ¨¡å¼è°ƒæ•´
                            potential_url = f"{self.base_url}/download/{filename}"
                            download_links.append(type('obj', (object,), {
                                'get': lambda self, attr, default='': filename if attr == 'text' else potential_url if attr == 'href' else default,
                                'get_text': lambda strip=False: filename
                            })())
                
                # å…ˆæ”¶é›†æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥
                valid_downloads = []
                for link in download_links:
                    href = link.get('href', '') if hasattr(link, 'get') else ''
                    title = link.get_text(strip=True) if hasattr(link, 'get_text') else str(link)
                    
                    if href and title:
                        # æ„å»ºå®Œæ•´URL
                        if href.startswith('/'):
                            file_url = self.base_url + href
                        elif href.startswith('http'):
                            file_url = href
                        else:
                            file_url = urljoin(product_url, href)
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
                        if file_url not in self.processed_urls:
                            valid_downloads.append((file_url, title))
                
                # åªæœ‰åœ¨æœ‰æœ‰æ•ˆä¸‹è½½æ–‡ä»¶æ—¶æ‰åˆ›å»ºç›®å½•
                if valid_downloads:
                    category_dir = save_dir / self.sanitize_filename(category_name)
                    category_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ä¸‹è½½æ–‡ä»¶
                    for file_url, title in valid_downloads:
                        if self.download_file(file_url, title, category_dir):
                            downloaded_count += 1
                            self.processed_urls.add(file_url)
                        else:
                            self.logger.info(f"æ–‡ä»¶å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {title}")
                        
                        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½äº§å“èµ„æ–™å¤±è´¥ {product_name}: {e}")
        
        return downloaded_count

    def clean_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
        
        Args:
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            æ¸…ç†åçš„æ–‡ä»¶å
        """
        # ç§»é™¤éæ³•å­—ç¬¦
        import re
        cleaned = re.sub(r'[<>:"/\\|?*]', '_', filename)
        cleaned = cleaned.strip()
        
        # å¦‚æœæ–‡ä»¶åå¤ªé•¿ï¼Œæˆªæ–­å®ƒ
        if len(cleaned) > 200:
            cleaned = cleaned[:200]
            
        return cleaned

    def download_file(self, url: str, filename: str, save_dir: Path) -> bool:
        """
        ä¸‹è½½å•ä¸ªæ–‡ä»¶
        
        Args:
            url: æ–‡ä»¶URL
            filename: æ–‡ä»¶å
            save_dir: ä¿å­˜ç›®å½•
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            # æ¸…ç†æ–‡ä»¶å
            clean_filename = self.sanitize_filename(filename)
            
            # å¦‚æœURLä¸­æœ‰æ–‡ä»¶æ‰©å±•åï¼Œä½¿ç”¨URLä¸­çš„æ‰©å±•å
            url_path = urlparse(url).path
            if '.' in url_path:
                url_ext = os.path.splitext(url_path)[1]
                if url_ext and not clean_filename.endswith(url_ext):
                    clean_filename += url_ext
            
            file_path = save_dir / clean_filename
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if file_path.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {clean_filename}")
                return True
            
            # ä¸‹è½½æ–‡ä»¶
            self.logger.info(f"å¼€å§‹ä¸‹è½½: {clean_filename}")
            response = self.safe_request(url)
            if not response:
                return False
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            file_size = file_path.stat().st_size
            
            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {clean_filename} ({file_size} bytes)")
            
            # è®°å½•åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
            self.new_files.append({
                'type': 'PDF' if file_path.suffix.lower() == '.pdf' else 'æ–‡æ¡£',
                'title': filename,
                'path': str(file_path),
                'url': url,
                'size': file_size
            })
            
            self.stats['downloaded_files'] += 1
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {url}: {e}")
            self.stats['failed_files'] += 1
            return False

    def sanitize_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        
        Args:
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            æ¸…ç†åçš„æ–‡ä»¶å
        """
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*\[\]{}]', '_', filename.strip())
        filename = re.sub(r'\s+', '_', filename)
        return filename

    def process_product(self, product_info: Dict) -> bool:
        """
        å¤„ç†å•ä¸ªäº§å“
        
        Args:
            product_info: äº§å“ä¿¡æ¯
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        product_name = product_info['name']
        product_url = product_info['url']
        category = product_info['category']
        
        try:
            self.logger.info(f"å¤„ç†äº§å“: {category} -> {product_name}")
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡
            if product_url in self.processed_urls:
                self.logger.info(f"äº§å“å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {product_name}")
                return True
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•æ¥æµ‹è¯•æ˜¯å¦æœ‰å†…å®¹
            category_dir = self.base_dir / self.sanitize_filename(category)
            product_dir = category_dir / self.sanitize_filename(product_name)
            
            # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹éœ€è¦ä¸‹è½½ï¼Œä¸ç«‹å³åˆ›å»ºç›®å½•
            has_content = False
            
            # 1. æ£€æŸ¥è§„æ ¼å‚æ•°PDF
            self.logger.info(f"æ£€æŸ¥è§„æ ¼å‚æ•°PDF: {product_name}")
            spec_success = False
            try:
                # å…ˆä¸ä¼ é€’ç›®å½•ï¼Œåªæ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
                response = self.safe_request(product_url)
                if response:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    spec_section = soup.find('div', class_='box_item box4')
                    if not spec_section:
                        spec_section = soup.find('div', string=re.compile('è§„æ ¼å‚æ•°'))
                        if spec_section:
                            spec_section = spec_section.find_parent('div')
                    
                    if spec_section:
                        # æœ‰è§„æ ¼å‚æ•°å†…å®¹
                        has_content = True
                        spec_success = True
            except Exception as e:
                self.logger.warning(f"æ£€æŸ¥è§„æ ¼å‚æ•°æ—¶å‡ºé”™: {e}")
            
            # 2. æ£€æŸ¥èµ„æ–™ä¸‹è½½æ¨¡å—
            self.logger.info(f"æ£€æŸ¥äº§å“èµ„æ–™: {product_name}")
            resource_count = 0
            try:
                response = self.safe_request(product_url)
                if response:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    download_section = soup.find('div', class_='auto auto_1500')
                    if download_section and 'èµ„æ–™ä¸‹è½½' in download_section.get_text():
                        pass
                    else:
                        download_sections = soup.find_all('div', string=re.compile('èµ„æ–™ä¸‹è½½'))
                        if download_sections:
                            download_section = download_sections[0].find_parent('div')
                    
                    if download_section:
                        tab_sections = download_section.find_all('ul', class_='ul clearfix')
                        for tab_section in tab_sections:
                            download_links = tab_section.find_all('a', href=True)
                            for link in download_links:
                                href = link.get('href', '')
                                title = link.get_text(strip=True)
                                if href and title:
                                    # æœ‰èµ„æ–™ä¸‹è½½å†…å®¹
                                    has_content = True
                                    break
                            if has_content:
                                break
            except Exception as e:
                self.logger.warning(f"æ£€æŸ¥äº§å“èµ„æ–™æ—¶å‡ºé”™: {e}")
            
            # åªæœ‰åœ¨ç¡®å®æœ‰å†…å®¹æ—¶æ‰åˆ›å»ºç›®å½•å¹¶ä¸‹è½½
            if has_content:
                product_dir.mkdir(parents=True, exist_ok=True)
                
                # æ‰§è¡Œå®é™…ä¸‹è½½
                if spec_success:
                    self.logger.info(f"ä¸‹è½½è§„æ ¼å‚æ•°PDF: {product_name}")
                    self.download_spec_params_as_pdf(product_url, product_name, product_dir)
                
                self.logger.info(f"ä¸‹è½½äº§å“èµ„æ–™: {product_name}")
                resource_count = self.download_product_resources(product_url, product_name, product_dir)
                
                self.logger.info(f"äº§å“ {product_name} å¤„ç†å®Œæˆï¼Œä¸‹è½½ {resource_count} ä¸ªèµ„æ–™æ–‡ä»¶")
            else:
                self.logger.info(f"äº§å“ {product_name} æ²¡æœ‰å¯ä¸‹è½½çš„å†…å®¹ï¼Œè·³è¿‡ç›®å½•åˆ›å»º")
            
            # è®°å½•å·²å¤„ç†çš„URL
            self.processed_urls.add(product_url)
            self.stats['processed_products'] += 1
            
            return True
            
        except Exception as e:
            self.logger.error(f"å¤„ç†äº§å“å¤±è´¥ {product_name}: {e}")
            return False

    def save_progress(self):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        progress_file = self.base_dir / 'crawl_progress.json'
        progress_data = {
            'timestamp': time.time(),
            'stats': self.stats,
            'completed_products': self.stats['processed_products']
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)

    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥"""
        try:
            timestamp = str(round(time.time() * 1000))
            string_to_sign = f'{timestamp}\n{self.SECRET}'
            hmac_code = hmac.new(self.SECRET.encode(), string_to_sign.encode(), hashlib.sha256).digest()
            sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))

            url = f'https://oapi.dingtalk.com/robot/send?access_token={self.ACCESS_TOKEN}&timestamp={timestamp}&sign={sign}'
            headers = {'Content-Type': 'application/json'}
            data = {
                "msgtype": "text",
                "text": {"content": message},
                "at": {"isAtAll": False}
            }

            response = requests.post(url, json=data, headers=headers)
            self.logger.info(f"é’‰é’‰é€šçŸ¥å“åº”ï¼š{response.status_code} {response.text}")
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {e}")
            return False

    def send_notifications(self):
        """å‘é€æ–°å¢æ–‡ä»¶é€šçŸ¥"""
        try:
            if not self.new_files:
                return
            
            # æ§åˆ¶å°é€šçŸ¥
            self.logger.info(f"\nğŸ‰ çˆ¬å–å®Œæˆé€šçŸ¥:")
            self.logger.info("=" * 60)
            self.logger.info(f"ğŸ“Š å‘ç° {len(self.new_files)} ä¸ªæ–°æ–‡ä»¶:")
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = {}
            for file_info in self.new_files:
                file_type = file_info['type']
                type_counts[file_type] = type_counts.get(file_type, 0) + 1
            
            for file_type, count in type_counts.items():
                self.logger.info(f"  ğŸ“ {file_type}: {count} ä¸ª")
            
            self.logger.info(f"\nğŸ“‚ æœ€æ–°æ–‡ä»¶é¢„è§ˆ:")
            for file_info in self.new_files[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                size_str = f" ({file_info['size']/1024:.1f}KB)" if 'size' in file_info else ""
                self.logger.info(f"  ğŸ“„ {file_info['title']}{size_str}")
            
            if len(self.new_files) > 5:
                self.logger.info(f"  ... è¿˜æœ‰ {len(self.new_files) - 5} ä¸ªæ–‡ä»¶")
                
            self.logger.info(f"\nğŸ’¾ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³: {self.base_dir}")
        
            # é’‰é’‰é€šçŸ¥
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            total_files = len(self.new_files)
            success_rate = 100.0 if self.stats['failed_files'] == 0 else (self.stats['downloaded_files'] / (self.stats['downloaded_files'] + self.stats['failed_files'])) * 100
            
            message = f"""âœ… è²ä»•ç§‘æŠ€ äº§å“èµ„æ–™ä¸‹è½½çˆ¬å–æˆåŠŸï¼Œè¯·åŠæ—¶å®¡æ ¸

ğŸ“Š ä¸‹è½½ç»Ÿè®¡:
  æˆåŠŸä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶
  å¤„ç†äº§å“: {self.stats['processed_products']} ä¸ª
  æ€»äº§å“æ•°: {self.stats['total_products']} ä¸ª
  æˆåŠŸç‡: {success_rate:.1f}%

ğŸ“ æ–‡ä»¶å­˜æ”¾è·¯å¾„: {self.base_dir}
â° å®Œæˆæ—¶é—´: {current_time}
ğŸ”§ åŒ…å«å†…å®¹: è§„æ ¼å‚æ•°PDFã€å‹å½•æ‰‹å†Œã€3Då›¾çº¸ç­‰"""
            
            # å‘é€é’‰é’‰é€šçŸ¥
            self.send_dingtalk_notification(message)
            
        except Exception as e:
            self.logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.logger.info("å¼€å§‹çˆ¬å–è²ä»•ç§‘æŠ€äº§å“èµ„æ–™")
        self.logger.info(f"ä¿å­˜ç›®å½•: {self.base_dir}")
        
        start_time = time.time()
        
        try:
            # è·å–æ‰€æœ‰äº§å“
            self.logger.info("è·å–äº§å“åˆ—è¡¨...")
            products = self.get_all_products()
            self.stats['total_products'] = len(products)
            
            if not products:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•äº§å“")
                return
            
            self.logger.info(f"æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
            
            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            category_counts = {}
            for product in products:
                category = product['category']
                category_counts[category] = category_counts.get(category, 0) + 1
            
            for category, count in category_counts.items():
                self.logger.info(f"  {category}: {count} ä¸ªäº§å“")
            
            # å¦‚æœæŒ‡å®šäº†ç±»åˆ«è¿‡æ»¤å™¨ï¼Œè¿‡æ»¤äº§å“
            if self.category_filter:
                filtered_products = [p for p in products if p['category'] == self.category_filter]
                self.logger.info(f"ç±»åˆ«è¿‡æ»¤å™¨: {self.category_filter}")
                self.logger.info(f"è¿‡æ»¤åäº§å“æ•°é‡: {len(filtered_products)}")
                products = filtered_products
                self.stats['total_products'] = len(products)
            
            # å¤„ç†æ¯ä¸ªäº§å“
            for i, product in enumerate(products):
                self.logger.info(f"=" * 50)
                self.logger.info(f"è¿›åº¦: {i+1}/{len(products)} - {product['category']}")
                
                self.process_product(product)
                time.sleep(2)  # äº§å“é—´æš‚åœ
                
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            # ä¿å­˜è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
            self.save_progress()
            
            # ä¿å­˜å·²å¤„ç†çš„URLs
            self.save_processed_urls()
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            duration = end_time - start_time
            
            self.logger.info("=" * 50)
            self.logger.info("çˆ¬å–å®Œæˆ!")
            self.logger.info(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
            self.logger.info(f"æ€»äº§å“æ•°: {self.stats['total_products']}")
            self.logger.info(f"å¤„ç†äº§å“: {self.stats['processed_products']}")
            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {self.stats['downloaded_files']}")
            self.logger.info(f"ä¸‹è½½å¤±è´¥: {self.stats['failed_files']}")
            self.logger.info(f"ä¿å­˜ç›®å½•: {self.base_dir}")
            
            # å‘é€é€šçŸ¥
            if self.new_files:
                self.send_notifications()

    def parse_download_center_page(self, url: str) -> List[Dict]:
        """
        è§£æä¸‹è½½ä¸­å¿ƒé¡µé¢ï¼Œæå–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            url: ä¸‹è½½ä¸­å¿ƒé¡µé¢URL
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        files = []
        
        try:
            response = self.safe_request(url)
            if not response:
                return files
                
            soup = BeautifulSoup(response.content, 'html.parser')
            content = response.text
            
            self.logger.info(f"æ­£åœ¨è§£æé¡µé¢: {url}")
            self.logger.info(f"é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æ£€æŸ¥é¡µé¢ç±»å‹
            if 'ProductCenter' in content and 'DownloadCenter' not in content:
                self.logger.warning(f"æ”¶åˆ°çš„æ˜¯äº§å“ä¸­å¿ƒé¡µé¢è€Œä¸æ˜¯ä¸‹è½½ä¸­å¿ƒé¡µé¢: {url}")
                return files
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç©ºé¡µé¢æˆ–æ— è®¿é—®æƒé™
            if 'æš‚æ— æ•°æ®' in content or 'æ— æƒé™' in content or len(content) < 1000:
                self.logger.warning(f"é¡µé¢å¯èƒ½ä¸ºç©ºæˆ–æ— è®¿é—®æƒé™: {url}")
                return files
            
            # æ–¹æ³•1: æŸ¥æ‰¾ TextList002208 å®¹å™¨
            text_list = soup.find('div', class_='TextList002208')
            if text_list:
                # æŸ¥æ‰¾æ‰€æœ‰çš„æ–‡ä»¶æ¡ç›® (dl å…ƒç´ )
                file_items = text_list.find_all('dl', class_='dl')
                
                for item in file_items:
                    try:
                        # åœ¨æ¯ä¸ª dl ä¸­æŸ¥æ‰¾ dt å…ƒç´ 
                        dt = item.find('dt', class_='dt')
                        if not dt:
                            continue
                            
                        # æå–å„ä¸ªå­—æ®µçš„ span å…ƒç´ 
                        spans = dt.find_all('span')
                        if len(spans) < 6:  # åç§°ã€ç‰ˆæœ¬ã€æ ¼å¼ã€å¤§å°ã€æ—¥æœŸã€ä¸‹è½½
                            continue
                            
                        name = spans[0].get_text(strip=True)
                        version = spans[1].get_text(strip=True)
                        file_format = spans[2].get_text(strip=True)
                        size = spans[3].get_text(strip=True)
                        date_str = spans[4].get_text(strip=True)
                        
                        # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
                        download_link = None
                        download_span = spans[5]
                        link_tag = download_span.find('a', href=True)
                        if link_tag:
                            href = link_tag.get('href')
                            if href:
                                download_link = urljoin(self.base_url, href)
                        
                        # è§£ææ—¥æœŸ
                        file_date = None
                        if date_str:
                            try:
                                # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
                                for date_format in ['%Y-%m-%d', '%Y.%m.%d', '%Y/%m/%d']:
                                    try:
                                        file_date = datetime.strptime(date_str, date_format)
                                        break
                                    except ValueError:
                                        continue
                                
                                # å¦‚æœè¿˜ä¸è¡Œï¼Œå°è¯•è§£ææ›´å¤æ‚çš„æ ¼å¼
                                if not file_date and len(date_str) >= 8:
                                    # å°è¯•åªæå–æ—¥æœŸéƒ¨åˆ†
                                    import re
                                    date_match = re.search(r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})', date_str)
                                    if date_match:
                                        clean_date = date_match.group(1).replace('.', '-').replace('/', '-')
                                        file_date = datetime.strptime(clean_date, '%Y-%m-%d')
                                    
                            except Exception as e:
                                self.logger.warning(f"æ—¥æœŸè§£æå¤±è´¥ {date_str}: {e}")
                        
                        # åªæ”¶é›†2024å¹´11æœˆåçš„æ–‡ä»¶
                        if file_date and file_date >= self.filter_date and download_link and name:
                            files.append({
                                'name': name,
                                'version': version,
                                'format': file_format,
                                'size': size,
                                'date': file_date,
                                'date_str': date_str,
                                'download_url': download_link,
                                'source_url': url
                            })
                            self.logger.info(f"æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶: {name} ({date_str})")
                            
                    except Exception as e:
                        self.logger.warning(f"è§£ææ–‡ä»¶æ¡ç›®å¤±è´¥: {e}")
                        continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° TextList002208ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not files:
                # æ–¹æ³•1: æŸ¥æ‰¾è¡¨æ ¼
                tables = soup.find_all('table')
                for table in tables:
                    rows = table.find_all('tr')
                    
                    for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                        cells = row.find_all(['td', 'th'])
                        
                        if len(cells) >= 4:  # è‡³å°‘éœ€è¦åç§°ã€æ ¼å¼ã€å¤§å°ã€æ—¥æœŸ
                            try:
                                name = cells[0].get_text(strip=True)
                                
                                # å°è¯•ä¸åŒçš„åˆ—é¡ºåº
                                if len(cells) >= 6:  # åç§° ç‰ˆæœ¬ æ ¼å¼ å¤§å° æ—¥æœŸ ä¸‹è½½
                                    version = cells[1].get_text(strip=True)
                                    file_format = cells[2].get_text(strip=True) 
                                    size = cells[3].get_text(strip=True)
                                    date_str = cells[4].get_text(strip=True)
                                    download_cell = cells[5]
                                elif len(cells) >= 5:  # åç§° æ ¼å¼ å¤§å° æ—¥æœŸ ä¸‹è½½
                                    version = ""
                                    file_format = cells[1].get_text(strip=True)
                                    size = cells[2].get_text(strip=True)
                                    date_str = cells[3].get_text(strip=True)
                                    download_cell = cells[4]
                                else:
                                    continue
                                
                                # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
                                download_link = None
                                link_tag = download_cell.find('a')
                                if link_tag and link_tag.get('href'):
                                    download_link = urljoin(self.base_url, link_tag.get('href'))
                                
                                # è§£ææ—¥æœŸ
                                file_date = None
                                if date_str:
                                    try:
                                        # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
                                        for date_format in ['%Y-%m-%d', '%Y.%m.%d', '%Y/%m/%d', '%Y-%m-%d %H:%M:%S']:
                                            try:
                                                file_date = datetime.strptime(date_str, date_format)
                                                break
                                            except ValueError:
                                                continue
                                        
                                        # å¦‚æœè¿˜ä¸è¡Œï¼Œå°è¯•è§£ææ›´å¤æ‚çš„æ ¼å¼
                                        if not file_date and len(date_str) >= 8:
                                            # å°è¯•åªæå–æ—¥æœŸéƒ¨åˆ†
                                            import re
                                            date_match = re.search(r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})', date_str)
                                            if date_match:
                                                clean_date = date_match.group(1).replace('.', '-').replace('/', '-')
                                                file_date = datetime.strptime(clean_date, '%Y-%m-%d')
                                            
                                    except Exception as e:
                                        self.logger.warning(f"æ—¥æœŸè§£æå¤±è´¥ {date_str}: {e}")
                                
                                # åªæ”¶é›†2024å¹´11æœˆåçš„æ–‡ä»¶
                                if file_date and file_date >= self.filter_date and download_link and name:
                                    files.append({
                                        'name': name,
                                        'version': version,
                                        'format': file_format,
                                        'size': size,
                                        'date': file_date,
                                        'date_str': date_str,
                                        'download_url': download_link,
                                        'source_url': url
                                    })
                                    self.logger.info(f"æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶: {name} ({date_str})")
                                    
                            except Exception as e:
                                self.logger.warning(f"è§£ææ–‡ä»¶è¡Œå¤±è´¥: {e}")
                                continue
            
            self.logger.info(f"ä»é¡µé¢ {url} è§£æåˆ° {len(files)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return files
            
        except Exception as e:
            self.logger.error(f"è§£æä¸‹è½½ä¸­å¿ƒé¡µé¢å¤±è´¥ {url}: {e}")
            return files

    def get_download_center_files(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰ä¸‹è½½ä¸­å¿ƒçš„æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        all_files = []
        
        for download_url in self.download_center_urls:
            self.logger.info(f"æ­£åœ¨çˆ¬å–ä¸‹è½½ä¸­å¿ƒ: {download_url}")
            
            # è·å–åˆ†ç±»åç§°
            category_map = {
                "lcid=8": "å‹å½•æ‰‹å†Œ",
                "lcid=18": "3Då›¾çº¸", 
                "lcid=7": "æ“ä½œè¯´æ˜ä¹¦",
                "lcid=6": "é©±åŠ¨å™¨é…ç½®è½¯ä»¶"
            }
            
            category = "æœªçŸ¥åˆ†ç±»"
            for key, value in category_map.items():
                if key in download_url:
                    category = value
                    break
            
            # çˆ¬å–æ‰€æœ‰é¡µé¢
            page = 1
            while True:
                if page == 1:
                    page_url = self.base_url + download_url
                else:
                    page_url = self.base_url + download_url + f"&page={page}"
                
                self.logger.info(f"æ­£åœ¨çˆ¬å– {category} ç¬¬{page}é¡µ: {page_url}")
                
                files = self.parse_download_center_page(page_url)
                if not files:
                    break
                
                # æ·»åŠ åˆ†ç±»ä¿¡æ¯
                for file_info in files:
                    file_info['category'] = category
                
                all_files.extend(files)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                # è¿™é‡Œéœ€è¦æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰åˆ†é¡µé“¾æ¥
                response = self.safe_request(page_url)
                if response:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    # æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"é“¾æ¥æˆ–é¡µç é“¾æ¥
                    next_page_found = False
                    
                    # æŸ¥æ‰¾åˆ†é¡µé“¾æ¥
                    pagination_links = soup.find_all('a', href=True)
                    for link in pagination_links:
                        href = link.get('href', '')
                        if f'page={page + 1}' in href or 'ä¸‹ä¸€é¡µ' in link.get_text():
                            next_page_found = True
                            break
                    
                    if not next_page_found:
                        break
                else:
                    break
                
                page += 1
                time.sleep(1)  # åˆ†é¡µé—´æš‚åœ
            
            self.logger.info(f"{category} çˆ¬å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len([f for f in all_files if f.get('category') == category])} ä¸ªæ–‡ä»¶")
            time.sleep(2)  # åˆ†ç±»é—´æš‚åœ
        
        return all_files

    def download_download_center_file(self, file_info: Dict, download_dir: Path) -> bool:
        """
        ä¸‹è½½ä¸‹è½½ä¸­å¿ƒçš„æ–‡ä»¶
        
        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯
            download_dir: ä¸‹è½½ç›®å½•
            
        Returns:
            ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            category = file_info.get('category', 'æœªçŸ¥åˆ†ç±»')
            name = file_info.get('name', 'æœªçŸ¥æ–‡ä»¶')
            file_format = file_info.get('format', '')
            download_url = file_info.get('download_url')
            
            if not download_url:
                self.logger.warning(f"æ–‡ä»¶æ²¡æœ‰ä¸‹è½½é“¾æ¥: {name}")
                return False
            
            # åˆ›å»ºåˆ†ç±»ç›®å½•
            category_dir = download_dir / category
            category_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            safe_name = self.clean_filename(name)
            if file_format and not safe_name.lower().endswith(f'.{file_format.lower()}'):
                filename = f"{safe_name}.{file_format.lower()}"
            else:
                filename = safe_name
            
            file_path = category_dir / filename
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if file_path.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                return True
            
            # ä¸‹è½½æ–‡ä»¶
            self.logger.info(f"æ­£åœ¨ä¸‹è½½: {filename}")
            
            response = self.session.get(download_url, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            file_size = file_path.stat().st_size
            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {filename} ({file_size} bytes)")
            
            # è®°å½•åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
            self.new_files.append({
                'type': file_format.upper() if file_format else 'FILE',
                'title': name,
                'path': str(file_path),
                'url': download_url,
                'size': file_size,
                'category': category,
                'date': file_info.get('date_str', '')
            })
            
            self.stats['downloaded_files'] += 1
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {file_info.get('name', 'æœªçŸ¥')}: {e}")
            self.stats['failed_files'] += 1
            return False

    def check_download_center_updates(self):
        """
        æ£€æŸ¥ä¸‹è½½ä¸­å¿ƒæ›´æ–°å¹¶ä¸‹è½½æ–°æ–‡ä»¶
        """
        self.logger.info("=" * 50)
        self.logger.info("å¼€å§‹æ£€æŸ¥ä¸‹è½½ä¸­å¿ƒæ›´æ–°...")
        
        # åˆ›å»ºä¸‹è½½ä¸­å¿ƒç›®å½•
        download_center_dir = self.base_dir / "ä¸‹è½½ä¸­å¿ƒ"
        download_center_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰æ–‡ä»¶
        all_files = self.get_download_center_files()
        
        if not all_files:
            self.logger.info("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°æ–‡ä»¶")
            return
        
        self.logger.info(f"æ‰¾åˆ° {len(all_files)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ï¼ˆ2024å¹´11æœˆåï¼‰")
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_counts = {}
        for file_info in all_files:
            category = file_info.get('category', 'æœªçŸ¥')
            category_counts[category] = category_counts.get(category, 0) + 1
        
        for category, count in category_counts.items():
            self.logger.info(f"  {category}: {count} ä¸ªæ–‡ä»¶")
        
        # ä¸‹è½½æ‰€æœ‰æ–‡ä»¶
        for i, file_info in enumerate(all_files):
            self.logger.info(f"è¿›åº¦: {i+1}/{len(all_files)} - {file_info.get('category', 'æœªçŸ¥')}")
            self.logger.info(f"æ–‡ä»¶: {file_info.get('name', 'æœªçŸ¥')} ({file_info.get('date_str', '')})")
            
            success = self.download_download_center_file(file_info, download_center_dir)
            if success:
                self.logger.info("ä¸‹è½½æˆåŠŸ")
            else:
                self.logger.warning("ä¸‹è½½å¤±è´¥")
            
            time.sleep(1)  # æ–‡ä»¶é—´æš‚åœ
        
        self.logger.info("ä¸‹è½½ä¸­å¿ƒæ£€æŸ¥å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è²ä»•ç§‘æŠ€äº§å“èµ„æ–™ä¸‹è½½çˆ¬è™«')
    parser.add_argument('--base-dir', type=str, help='ä¸‹è½½æ–‡ä»¶ä¿å­˜çš„åŸºç¡€ç›®å½•')
    parser.add_argument('--monitor', action='store_true', help='è¿è¡Œç›‘æ§æ¨¡å¼ï¼Œåªæ£€æµ‹æ–°æ–‡ä»¶ä¸ä¸‹è½½')
    parser.add_argument('--download-center', action='store_true', help='åªçˆ¬å–ä¸‹è½½ä¸­å¿ƒï¼ˆ2024å¹´11æœˆåçš„æ–‡ä»¶ï¼‰')
    parser.add_argument('--products-only', action='store_true', help='åªçˆ¬å–äº§å“ä¸­å¿ƒï¼Œä¸åŒ…æ‹¬ä¸‹è½½ä¸­å¿ƒ')
    parser.add_argument('--osai-only', action='store_true', help='åªçˆ¬å–OSAIæ§åˆ¶ç³»ç»Ÿç±»åˆ«çš„äº§å“')
    parser.add_argument('--category', type=str, help='æŒ‡å®šè¦çˆ¬å–çš„äº§å“ç±»åˆ«')
    
    args = parser.parse_args()
    
    # ç¡®å®šç±»åˆ«è¿‡æ»¤å™¨
    category_filter = None
    if args.osai_only:
        category_filter = 'OSAIæ§åˆ¶ç³»ç»Ÿ'
    elif args.category:
        category_filter = args.category
    
    spider = PhysisSpider(base_dir=args.base_dir, monitor_mode=args.monitor, category_filter=category_filter)
    
    if args.download_center:
        # åªçˆ¬å–ä¸‹è½½ä¸­å¿ƒ
        spider.check_download_center_updates()
    elif args.products_only:
        # åªçˆ¬å–äº§å“ä¸­å¿ƒ  
        spider.run()
    else:
        # åŒæ—¶çˆ¬å–äº§å“ä¸­å¿ƒå’Œä¸‹è½½ä¸­å¿ƒ
        spider.run()
        spider.check_download_center_updates()


if __name__ == "__main__":
    main()
