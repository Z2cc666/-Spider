#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å®‰ç¿æ§ä¸‹è½½ä¸­å¿ƒçˆ¬è™«
çˆ¬å–æ‰€æœ‰åˆ†ç±»ä¸‹çš„æŠ€æœ¯èµ„æ–™å’Œäº§å“æ–‡æ¡£
æ”¯æŒé’‰é’‰é€šçŸ¥å’Œè‡ªåŠ¨æ£€æµ‹æ–°æ–‡ä»¶
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
import pickle
import hmac
import hashlib
import base64
from datetime import datetime
from urllib.parse import urljoin, urlparse, quote_plus
import re
import urllib.request

class ChuangAnSpider:
    def __init__(self):
        self.base_url = "https://www.cschueun.com"
        self.main_url = "https://www.cschueun.com/download.html"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/åˆ›å®‰ç¿æ§"
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "ä¸‹è½½æ–‡ä»¶")
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "åˆ›å®‰ç¿æ§ä¸‹è½½")
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "ä¸‹è½½æ–‡ä»¶")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.download_dir, exist_ok=True)
        
        # é’‰é’‰é…ç½®ï¼ˆå†…ç½®ï¼‰
        self.dingtalk_config = {
            "access_token": "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24",
            "secret": "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb",
            "webhook_url": "https://oapi.dingtalk.com/robot/send?access_token=1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        }
        
        # åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•
        self.processed_files = self.load_processed_files()
        self.new_files = []
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_files.pkl'))
    
    def load_processed_files(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_files(self):
        """ä¿å­˜å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_files, f)
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥ï¼ˆæ”¯æŒåŠ å¯†ç­¾åï¼‰"""
        if not self.dingtalk_config or not self.dingtalk_config.get('webhook_url'):
            self.log("âš ï¸ é’‰é’‰é…ç½®æœªè®¾ç½®ï¼Œè·³è¿‡é€šçŸ¥")
            return
        
        try:
            # è·å–é…ç½®
            access_token = self.dingtalk_config.get('access_token', '')
            secret = self.dingtalk_config.get('secret', '')
            webhook_url = self.dingtalk_config.get('webhook_url', '')
            
            # å¦‚æœæœ‰secretï¼Œç”Ÿæˆç­¾å
            if secret:
                timestamp = str(round(time.time() * 1000))
                string_to_sign = f'{timestamp}\n{secret}'
                hmac_code = hmac.new(
                    secret.encode('utf-8'),
                    string_to_sign.encode('utf-8'),
                    digestmod=hashlib.sha256
                ).digest()
                sign = quote_plus(base64.b64encode(hmac_code))
                webhook_url = f"{webhook_url}&timestamp={timestamp}&sign={sign}"
            
            # æ„å»ºæ¶ˆæ¯
            data = {
                "msgtype": "text",
                "text": {
                    "content": f"ğŸ¤– åˆ›å®‰ç¿æ§çˆ¬è™«é€šçŸ¥\n{message}"
                }
            }
            
            # å‘é€é€šçŸ¥
            response = requests.post(
                webhook_url,
                json=data,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    self.log("âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
                else:
                    self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {result.get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
            else:
                self.log(f"âŒ é’‰é’‰é€šçŸ¥HTTPé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¼‚å¸¸: {str(e)}")
    
    def get_page(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 502 and attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(3 * (attempt + 1))
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
        return None
    
    def parse_main_page(self):
        """è§£æä¸»é¡µé¢ï¼Œè·å–æ‰€æœ‰å¤§æ¨¡å—"""
        print("æ­£åœ¨è§£æä¸»é¡µé¢...")
        html = self.get_page(self.main_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        modules = []
        
        # æŸ¥æ‰¾å¤§æ¨¡å—
        tree_div = soup.find('div', class_='p_c_tree')
        if tree_div:
            # æŸ¥æ‰¾æ‰€æœ‰ä¸€çº§æ¨¡å—ï¼ˆdeep-1ï¼‰
            level1_items = tree_div.find_all('li', class_='p_c_item')
            
            for item in level1_items:
                # æŸ¥æ‰¾ä¸€çº§æ¨¡å—é“¾æ¥
                level1_link = item.find('a', class_='p_c_title1')
                if level1_link:
                    module_name = level1_link.get_text(strip=True)
                    module_url = level1_link.get('href', '').strip()
                    
                    if module_name and module_url:
                        full_url = urljoin(self.base_url, module_url)
                        
                        # æŸ¥æ‰¾å­æ¨¡å—
                        sub_modules = []
                        level2_ul = item.find('ul', class_='deep-2')
                        if level2_ul:
                            level2_items = level2_ul.find_all('li', class_='p_c_item')
                            for sub_item in level2_items:
                                sub_link = sub_item.find('a', class_='p_c_title2')
                                if sub_link:
                                    sub_name = sub_link.get_text(strip=True)
                                    sub_url = sub_link.get('href', '').strip()
                                    if sub_name and sub_url:
                                        sub_full_url = urljoin(self.base_url, sub_url)
                                        sub_modules.append({
                                            'name': sub_name,
                                            'url': sub_full_url
                                        })
                        
                        module_info = {
                            'name': module_name,
                            'url': full_url,
                            'sub_modules': sub_modules,
                            'type': 'ä¸€çº§æ¨¡å—'
                        }
                        modules.append(module_info)
                        print(f"æ‰¾åˆ°æ¨¡å—: {module_name} - {full_url}")
                        if sub_modules:
                            for sub in sub_modules:
                                print(f"  å­æ¨¡å—: {sub['name']} - {sub['url']}")
        
        return modules
    
    def parse_module_page(self, module_info):
        """è§£ææ¨¡å—é¡µé¢ï¼Œè·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒç¿»é¡µï¼‰"""
        print(f"\næ­£åœ¨è§£ææ¨¡å—: {module_info['name']}")
        all_files = []
        page = 1
        
        while True:
            # æ„å»ºåˆ†é¡µURL
            if page == 1:
                url = module_info['url']
            else:
                # å¦‚æœä¸Šä¸€æ¬¡å¾ªç¯å·²ç»æ‰¾åˆ°äº†ä¸‹ä¸€é¡µURLï¼Œç›´æ¥ä½¿ç”¨
                if hasattr(self, '_next_page_url') and self._next_page_url:
                    url = self._next_page_url
                    self._next_page_url = None  # æ¸…é™¤ï¼Œé¿å…é‡å¤ä½¿ç”¨
                else:
                    # ä½¿ç”¨ä¿å­˜çš„æ¨¡å—IDæ„å»ºURL
                    if hasattr(self, '_module_ids') and module_info['name'] in self._module_ids:
                        module_id = self._module_ids[module_info['name']]
                        start = (page - 1) * 6
                        
                        base_url = module_info['url']
                        if '/download_1/' in base_url:
                            url = f"{self.base_url}/download_1/{module_id}-{start}-6.html"
                        else:
                            url = f"{self.base_url}/download/{module_id}-{start}-6.html"
                    else:
                        # å¤‡ç”¨æ–¹æ³•
                        base_url = module_info['url']
                        parts = base_url.split('/')
                        module_id = parts[-1].replace('.html', '')
                        start = (page - 1) * 6
                        
                        if '/download_1/' in base_url:
                            url = f"{self.base_url}/download_1/{module_id}-{start}-6.html"
                        else:
                            url = f"{self.base_url}/download/{module_id}-{start}-6.html"
            
            print(f"  è§£æç¬¬ {page} é¡µ: {url}")
            html = self.get_page(url)
            if not html:
                break
            
            soup = BeautifulSoup(html, 'html.parser')
            files_found = False
            
            # æŸ¥æ‰¾æ–‡ä»¶åˆ—è¡¨ - ä½¿ç”¨æ­£ç¡®çš„cbox-33ç»“æ„
            p_list = soup.find('div', class_='p_list')
            if p_list:
                file_items = p_list.find_all('div', class_='cbox-33')
                
                for item in file_items:
                    files_found = True
                    
                    # è·å–æ–‡ä»¶æ ‡é¢˜ (åœ¨cbox-35-0ä¸­çš„h1)
                    title_container = item.find('div', class_='cbox-35-0')
                    title = "æœªçŸ¥æ–‡ä»¶"
                    if title_container:
                        title_elem = title_container.find('h1', class_='e_h1-41')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                    
                    # è·å–æ–‡ä»¶å¤§å° (åœ¨cbox-35-1ä¸­çš„p)
                    size_container = item.find('div', class_='cbox-35-1')
                    size = "æœªçŸ¥å¤§å°"
                    if size_container:
                        size_elem = size_container.find('p', class_='e_text-40')
                        if size_elem:
                            size = size_elem.get_text(strip=True)
                    
                    # è·å–å‘å¸ƒæ—¥æœŸ (åœ¨cbox-35-2ä¸­çš„p)
                    date_container = item.find('div', class_='cbox-35-2')
                    date = "æœªçŸ¥æ—¥æœŸ"
                    if date_container:
                        date_elem = date_container.find('p', class_='e_timeFormat-36')
                        if date_elem:
                            date = date_elem.get_text(strip=True)
                    
                    # è·å–ä¸‹è½½é“¾æ¥ (åœ¨cbox-35-3ä¸­çš„a)
                    download_container = item.find('div', class_='cbox-35-3')
                    download_url = ""
                    if download_container:
                        download_link = download_container.find('a', href=True)
                        if download_link:
                            href = download_link.get('href', '')
                            if href.startswith('http'):
                                download_url = href
                            else:
                                download_url = urljoin(self.base_url, href)
                    
                    if download_url and title != "æœªçŸ¥æ–‡ä»¶":
                        file_info = {
                            'title': title,
                            'size': size,
                            'date': date,
                            'url': download_url,
                            'module': module_info['name'],
                            'page': page
                        }
                        all_files.append(file_info)
                        print(f"    æ‰¾åˆ°æ–‡ä»¶: {title} ({size}) - {date}")
            
            # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°cbox-2ï¼ŒæŸ¥æ‰¾Downloadé“¾æ¥åŠå…¶ç›¸å…³ä¿¡æ¯
            if not files_found:
                download_links = soup.find_all('a', string=lambda x: x and 'download' in x.lower())
                
                for download_link in download_links:
                    files_found = True
                    
                    # è·å–ä¸‹è½½URL
                    href = download_link.get('href', '')
                    if href.startswith('http'):
                        download_url = href
                    else:
                        download_url = urljoin(self.base_url, href)
                    
                    # å‘ä¸ŠæŸ¥æ‰¾åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å®¹å™¨
                    container = download_link
                    for _ in range(10):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾10çº§
                        container = container.parent
                        if not container:
                            break
                        
                        # æŸ¥æ‰¾æ–‡ä»¶æ ‡é¢˜ - åœ¨e_container-3æˆ–ç±»ä¼¼å®¹å™¨ä¸­
                        if 'e_container' in str(container.get('class', [])):
                            break
                    
                    title = "æœªçŸ¥æ–‡ä»¶"
                    size = "æœªçŸ¥å¤§å°"
                    date = "æœªçŸ¥æ—¥æœŸ"
                    
                    if container:
                        # æŸ¥æ‰¾æ ‡é¢˜
                        title_elem = container.find(['h1', 'h2', 'h3', 'p'], class_=lambda x: x and ('title' in str(x) or 'subtitle' in str(x)))
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                        
                        # æŸ¥æ‰¾æ–‡ä»¶å¤§å°
                        size_elem = container.find('p', string=lambda x: x and ('MB' in x or 'KB' in x or 'GB' in x))
                        if size_elem:
                            size = size_elem.get_text(strip=True)
                        
                        # æŸ¥æ‰¾æ—¥æœŸ
                        date_elem = container.find('p', string=lambda x: x and ('/' in x or '-' in x) and len(x.strip()) <= 12)
                        if date_elem:
                            date = date_elem.get_text(strip=True)
                    
                    if download_url and title != "æœªçŸ¥æ–‡ä»¶":
                        file_info = {
                            'title': title,
                            'size': size,
                            'date': date,
                            'url': download_url,
                            'module': module_info['name'],
                            'page': page
                        }
                        all_files.append(file_info)
                        print(f"    æ‰¾åˆ°æ–‡ä»¶: {title} ({size}) - {date}")
            
            # æ–¹æ³•3: å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾ç›´æ¥çš„æ–‡ä»¶é“¾æ¥
            if not files_found:
                # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶é“¾æ¥
                file_links = soup.find_all('a', href=lambda x: x and any(ext in x.lower() for ext in ['.pdf', '.doc', '.docx', '.zip', '.rar']))
                
                for link in file_links:
                    files_found = True
                    
                    href = link.get('href', '')
                    if href.startswith('http'):
                        download_url = href
                    else:
                        download_url = urljoin(self.base_url, href)
                    
                    # ä»URLæˆ–é“¾æ¥æ–‡æœ¬è·å–æ ‡é¢˜
                    title = link.get_text(strip=True) or href.split('/')[-1]
                    
                    file_info = {
                        'title': title,
                        'size': "æœªçŸ¥å¤§å°",
                        'date': "æœªçŸ¥æ—¥æœŸ",
                        'url': download_url,
                        'module': module_info['name'],
                        'page': page
                    }
                    all_files.append(file_info)
                    print(f"    æ‰¾åˆ°æ–‡ä»¶é“¾æ¥: {title}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ - ä½¿ç”¨æ¨¡æ‹Ÿæµè§ˆå™¨çš„æ–¹æ³•
            has_next = False
            next_page_url = None
            
            # æ–¹æ³•1: æ£€æŸ¥æ ‡å‡†åˆ†é¡µç»“æ„ï¼ˆä¼˜å…ˆæ–¹æ³•ï¼‰
            page_div = soup.find('div', class_='p_page')
            if page_div:
                # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                next_link = page_div.find('a', class_='page_next')
                if next_link:
                    next_href = next_link.get('href', '')
                    disabled = 'disabled' in next_link.get('class', [])
                    
                    if not disabled and next_href and next_href != 'javascript:;':
                        # æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹ä¸€é¡µé“¾æ¥
                        if next_href.startswith('/'):
                            next_page_url = urljoin(self.base_url, next_href)
                        else:
                            next_page_url = next_href
                        has_next = True
                        print(f"    æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®: {next_page_url}")
                
                # å¦‚æœæ²¡æœ‰ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œæ£€æŸ¥é¡µç é“¾æ¥
                if not has_next:
                    page_links = page_div.find_all('a', class_='page_num')
                    current_page_found = False
                    for i, link in enumerate(page_links):
                        link_classes = link.get('class', [])
                        link_text = link.get_text().strip()
                        
                        # æ‰¾åˆ°å½“å‰é¡µ
                        if 'current' in link_classes or link_text == str(page):
                            current_page_found = True
                            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€ä¸ªé¡µç 
                            if i + 1 < len(page_links):
                                next_page_link = page_links[i + 1]
                                next_href = next_page_link.get('href', '')
                                if next_href and next_href != 'javascript:;':
                                    if next_href.startswith('/'):
                                        next_page_url = urljoin(self.base_url, next_href)
                                    else:
                                        next_page_url = next_href
                                    has_next = True
                                    next_page_num = next_page_link.get_text().strip()
                                    print(f"    æ‰¾åˆ°ç¬¬{next_page_num}é¡µé“¾æ¥: {next_page_url}")
                            break
            
            # æ–¹æ³•2: å¦‚æœç¬¬ä¸€é¡µæ²¡æœ‰åˆ†é¡µç»“æ„ï¼Œä½†æ–‡ä»¶æ•°é‡è¾ƒå¤šï¼Œå°è¯•æ¢æµ‹æ€§æ£€æŸ¥
            if not has_next and page == 1 and files_found:
                current_files = len([f for f in all_files if f['page'] == page])
                
                # ä½¿ç”¨æ¨¡å—ç‰¹å®šçš„è§„åˆ™æ¥åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†é¡µ
                should_paginate = False
                module_name = module_info.get('name', '')
                
                # æ ¹æ®å·²çŸ¥ä¿¡æ¯è¿›è¡Œç‰¹å®šåˆ¤æ–­
                if 'å®£ä¼ å½©é¡µ' in module_name:
                    # å®£ä¼ å½©é¡µç¡®å®æœ‰3é¡µ
                    should_paginate = True
                    print(f"    å®£ä¼ å½©é¡µæ¨¡å—ï¼šå·²çŸ¥æœ‰3é¡µï¼Œéœ€è¦åˆ†é¡µ")
                elif 'è¡Œä¸šä¸“æœºé©±åŠ¨å™¨' in module_name:
                    # è¡Œä¸šä¸“æœºé©±åŠ¨å™¨åªæœ‰1é¡µ
                    should_paginate = False
                    print(f"    è¡Œä¸šä¸“æœºé©±åŠ¨å™¨æ¨¡å—ï¼šå·²çŸ¥åªæœ‰1é¡µï¼Œæ— éœ€åˆ†é¡µ")
                elif 'è¯´æ˜ä¹¦-å˜é¢‘å™¨' in module_name:
                    # è¯´æ˜ä¹¦-å˜é¢‘å™¨ç¡®å®æœ‰3é¡µ
                    should_paginate = True
                    print(f"    è¯´æ˜ä¹¦-å˜é¢‘å™¨æ¨¡å—ï¼šå·²çŸ¥æœ‰3é¡µï¼Œéœ€è¦åˆ†é¡µ")
                else:
                    # å…¶ä»–æ¨¡å—ä½¿ç”¨ pageParamsJson å’Œæ–‡ä»¶æ•°é‡ç»¼åˆåˆ¤æ–­
                    page_params_input = soup.find('input', attrs={'name': 'pageParamsJson'})
                    if page_params_input:
                        try:
                            import json
                            page_params = json.loads(page_params_input.get('value', '{}'))
                            total_count = page_params.get('totalCount', 0)
                            page_size = page_params.get('size', 6)
                            
                            # å¦‚æœ pageParamsJson æ˜ç¡®æ˜¾ç¤ºæ€»æ•°å°‘äºå½“å‰æ–‡ä»¶æ•°ï¼Œè¯´æ˜æ— éœ€åˆ†é¡µ
                            if total_count > 0 and total_count < current_files:
                                should_paginate = False
                                print(f"    pageParamsJsonæ˜¾ç¤ºæ€»å…±{total_count}ä¸ªæ–‡ä»¶ï¼Œå½“å‰å·²æœ‰{current_files}ä¸ªï¼Œæ— éœ€åˆ†é¡µ")
                            elif current_files >= 6:
                                should_paginate = True
                                print(f"    å½“å‰{current_files}ä¸ªæ–‡ä»¶è¾ƒå¤šï¼Œå°è¯•åˆ†é¡µ")
                            else:
                                should_paginate = False
                                print(f"    å½“å‰{current_files}ä¸ªæ–‡ä»¶ï¼Œæ— éœ€åˆ†é¡µ")
                        except:
                            should_paginate = current_files >= 6
                    else:
                        should_paginate = current_files >= 6
                
                # åªæœ‰å½“ç¡®å®éœ€è¦åˆ†é¡µæ—¶æ‰æ¢æµ‹ç¬¬2é¡µ
                if should_paginate:
                    print(f"    ç¬¬ä¸€é¡µæœ‰{current_files}ä¸ªæ–‡ä»¶ï¼Œå°è¯•æ¢æµ‹ç¬¬2é¡µ")
                    
                    # æ„é€ å¯èƒ½çš„ç¬¬äºŒé¡µURL
                    test_urls = []
                    base_url = module_info['url']
                    
                    if '/download/' in base_url:
                        parts = base_url.split('/')
                        module_id = parts[-1].replace('.html', '')
                        test_urls.append(f"{self.base_url}/download/{module_id}-6-6.html")
                        # å°è¯•å¸¸è§æ¨¡å¼
                        test_urls.append(f"{self.base_url}/download/16957180-6-6.html")
                    elif '/download_1/' in base_url:
                        parts = base_url.split('/')
                        module_id = parts[-1].replace('.html', '')
                        # å¯¹äºå®£ä¼ å½©é¡µï¼Œä½¿ç”¨å·²çŸ¥çš„æ­£ç¡®æ¨¡å—ID
                        if module_id == '1' and 'å®£ä¼ å½©é¡µ' in module_info['name']:
                            test_urls.append(f"{self.base_url}/download_1/16728513-6-6.html")
                        else:
                            test_urls.append(f"{self.base_url}/download_1/{module_id}-6-6.html")
                    
                    # æµ‹è¯•URLæ˜¯å¦æœ‰æ•ˆ
                    for test_url in test_urls:
                        test_html = self.get_page(test_url)
                        if test_html:
                            test_soup = BeautifulSoup(test_html, 'html.parser')
                            test_downloads = test_soup.find_all('a', string=lambda x: x and 'download' in x.lower())
                            if test_downloads:
                                next_page_url = test_url
                                has_next = True
                                # ä¿å­˜æ¨¡å—IDä»¥ä¾¿åç»­ä½¿ç”¨
                                if not hasattr(self, '_module_ids'):
                                    self._module_ids = {}
                                url_parts = test_url.split('/')[-1].split('-')
                                if len(url_parts) >= 3:
                                    self._module_ids[module_info['name']] = url_parts[0]
                                print(f"    æ¢æµ‹åˆ°ç¬¬2é¡µå­˜åœ¨: {test_url}")
                                break
            
            # æ–¹æ³•3: å¯¹äºå·²ç¡®è®¤æœ‰åˆ†é¡µçš„æ¨¡å—ï¼Œç»§ç»­æ„é€ åç»­é¡µé¢
            elif not has_next and page > 1 and hasattr(self, '_module_ids'):
                if module_info['name'] in self._module_ids:
                    module_id = self._module_ids[module_info['name']]
                    start = page * 6
                    
                    base_url = module_info['url']
                    if '/download_1/' in base_url:
                        test_url = f"{self.base_url}/download_1/{module_id}-{start}-6.html"
                    else:
                        test_url = f"{self.base_url}/download/{module_id}-{start}-6.html"
                    
                    # æµ‹è¯•æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                    test_html = self.get_page(test_url)
                    if test_html:
                        test_soup = BeautifulSoup(test_html, 'html.parser')
                        test_downloads = test_soup.find_all('a', string=lambda x: x and 'download' in x.lower())
                        if test_downloads:
                            next_page_url = test_url
                            has_next = True
                            print(f"    ç»§ç»­æ‰¾åˆ°ç¬¬{page + 1}é¡µ: {test_url}")
            
            # å¦‚æœæ‰¾åˆ°äº†ä¸‹ä¸€é¡µURLï¼Œä¿å­˜å®ƒä»¥ä¾¿ä¸‹æ¬¡å¾ªç¯ä½¿ç”¨
            if has_next and next_page_url:
                self._next_page_url = next_page_url
                # ä»ä¸‹ä¸€é¡µURLä¸­æå–æ¨¡å—IDï¼Œç”¨äºåç»­é¡µé¢æ„é€ 
                if not hasattr(self, '_module_ids'):
                    self._module_ids = {}
                if module_info['name'] not in self._module_ids:
                    url_parts = next_page_url.split('/')[-1].split('-')
                    if len(url_parts) >= 3:
                        self._module_ids[module_info['name']] = url_parts[0]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶æˆ–æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œé€€å‡ºå¾ªç¯
            if not files_found or not has_next:
                break
                
            page += 1
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        print(f"  å…±æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        return all_files
    
    def download_file(self, file_info):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            url = file_info['url']
            title = file_info['title']
            
            # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
            filename = re.sub(r'[<>:"/\\|?*]', '_', title)
            
            # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼ï¼Œè®¾ç½®é»˜è®¤åç§°
            if not filename or not filename.strip():
                filename = "ä¸‹è½½æ–‡ä»¶" 
            
            # ä»URLè·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            path = parsed_url.path
            if '.' in path:
                ext = path.split('.')[-1].lower()
                if ext in ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'zip', 'rar']:
                    if not filename.lower().endswith(f'.{ext}'):
                        filename += f'.{ext}'
            else:
                # å¦‚æœURLä¸­æ²¡æœ‰æ‰©å±•åï¼Œæ ¹æ®å†…å®¹ç±»å‹åˆ¤æ–­
                filename += '.pdf'  # é»˜è®¤PDF
            
            # è§£ææ¨¡å—åï¼Œåˆ†ç¦»å¤§æ¨¡å—å’Œå­æ¨¡å—
            module_full_name = file_info['module']
            if '-' in module_full_name:
                # ä¾‹å¦‚ï¼šè¯´æ˜ä¹¦-å˜é¢‘å™¨ -> å¤§æ¨¡å—ï¼šè¯´æ˜ä¹¦ï¼Œå­æ¨¡å—ï¼šå˜é¢‘å™¨
                main_module, sub_module = module_full_name.split('-', 1)
                main_module = re.sub(r'[<>:"/\\|?*]', '_', main_module.strip())
                sub_module = re.sub(r'[<>:"/\\|?*]', '_', sub_module.strip())
            else:
                # åªæœ‰ä¸€çº§æ¨¡å—
                main_module = re.sub(r'[<>:"/\\|?*]', '_', module_full_name)
                sub_module = None
            
            # åˆ›å»ºç›®å½•ç»“æ„ï¼šå¤§æ¨¡å—/å­æ¨¡å—/
            if sub_module:
                module_dir = os.path.join(self.download_dir, main_module, sub_module)
            else:
                module_dir = os.path.join(self.download_dir, main_module)
            
            if not os.path.exists(module_dir):
                os.makedirs(module_dir)
            
            # å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            filepath = os.path.join(module_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°åˆç†ï¼Œè·³è¿‡ä¸‹è½½
            if os.path.exists(filepath):
                file_size = os.path.getsize(filepath)
                if file_size > 1024:  # å¤§äº1KB
                    print(f"        æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                    return True
            
            print(f"        æ­£åœ¨ä¸‹è½½: {filename}")
            
            # ä¸ºä¸‹è½½è®¾ç½®æ­£ç¡®çš„è¯·æ±‚å¤´ï¼ŒåŒ…æ‹¬Referer
            download_headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.cschueun.com/',  # é‡è¦ï¼šè®¾ç½®Referer
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
                'Cache-Control': 'max-age=0'
            }
            
            # ä½¿ç”¨requestsä¸‹è½½æ–‡ä»¶
            response = self.session.get(url, headers=download_headers, stream=True, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' in content_type:
                print(f"        è­¦å‘Š: ä¸‹è½½çš„å¯èƒ½ä¸æ˜¯æ–‡ä»¶ï¼Œè€Œæ˜¯HTMLé¡µé¢")
                return False
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filepath)
            if file_size < 1024:  # å°äº1KBå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                print(f"        è­¦å‘Š: æ–‡ä»¶å¤§å°å¼‚å¸¸ ({file_size} bytes)")
                return False
            
            print(f"        ä¸‹è½½å®Œæˆ: {filename} ({file_size} bytes)")
            
            # è®°å½•æ–°æ–‡ä»¶
            file_key = f"{file_info['module']}_{filename}"
            if file_key not in self.processed_files:
                # æ„å»ºæ˜¾ç¤ºç”¨çš„æ¨¡å—è·¯å¾„
                if sub_module:
                    display_module = f"{main_module}/{sub_module}"
                else:
                    display_module = main_module
                    
                self.new_files.append({
                    'filename': filename,
                    'path': filepath,
                    'url': file_info['url'],
                    'size': file_size,
                    'module': display_module,
                    'title': file_info['title']
                })
                self.processed_files.add(file_key)
            
            return True
            
        except Exception as e:
            print(f"        ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def download_all_files(self, all_files):
        """ä¸‹è½½æ‰€æœ‰æ–‡ä»¶"""
        if not all_files:
            print("æ²¡æœ‰æ–‡ä»¶éœ€è¦ä¸‹è½½")
            return
        
        print(f"\nå¼€å§‹ä¸‹è½½æ–‡ä»¶...")
        print(f"å…±æœ‰ {len(all_files)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")
        
        total_downloads = 0
        successful_downloads = 0
        
        for i, file_info in enumerate(all_files, 1):
            print(f"\nè¿›åº¦: {i}/{len(all_files)} - {file_info['module']} - {file_info['title']}")
            
            total_downloads += 1
            if self.download_file(file_info):
                successful_downloads += 1
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        print(f"\næ–‡ä»¶ä¸‹è½½å®Œæˆï¼")
        print(f"æ€»å°è¯•ä¸‹è½½: {total_downloads} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸä¸‹è½½: {successful_downloads} ä¸ªæ–‡ä»¶")
        print(f"å¤±è´¥: {total_downloads - successful_downloads} ä¸ªæ–‡ä»¶")
    
    def save_data(self, data, filename):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        filepath = os.path.join(self.output_dir, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.log("ğŸš€ å¼€å§‹çˆ¬å–åˆ›å®‰ç¿æ§ä¸‹è½½ä¸­å¿ƒ...")
        
        # 1. è·å–æ‰€æœ‰å¤§æ¨¡å—
        modules = self.parse_main_page()
        if not modules:
            self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å—")
            return
        
        self.log(f"ğŸ“‹ å…±æ‰¾åˆ° {len(modules)} ä¸ªå¤§æ¨¡å—")
        
        # 2. ä¿å­˜æ¨¡å—ä¿¡æ¯
        self.save_data(modules, 'modules.json')
        
        # 3. çˆ¬å–æ¯ä¸ªæ¨¡å—çš„æ–‡ä»¶
        all_files = []
        
        for i, module in enumerate(modules, 1):
            self.log(f"ğŸ”„ è¿›åº¦: {i}/{len(modules)} - {module['name']}")
            
            # å¦‚æœæ¨¡å—æœ‰å­æ¨¡å—ï¼Œçˆ¬å–å­æ¨¡å—
            if module['sub_modules']:
                for sub_module in module['sub_modules']:
                    sub_module_info = {
                        'name': f"{module['name']}-{sub_module['name']}",
                        'url': sub_module['url']
                    }
                    files = self.parse_module_page(sub_module_info)
                    all_files.extend(files)
                    time.sleep(1)
            else:
                # ç›´æ¥çˆ¬å–æ¨¡å—
                files = self.parse_module_page(module)
                all_files.extend(files)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        # 4. ä¿å­˜æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
        if all_files:
            self.save_data(all_files, 'files.json')
            self.log(f"âœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_files)} ä¸ªæ–‡ä»¶")
        else:
            self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ä¿¡æ¯")
            return
        
        # 5. ä¸‹è½½æ–‡ä»¶
        self.download_all_files(all_files)
        
        # 6. ä¿å­˜å¤„ç†è®°å½•
        self.save_processed_files()
        
        # 7. å‘é€é’‰é’‰é€šçŸ¥
        self.send_completion_notification()
        
        # 8. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(modules, all_files)
    
    def send_completion_notification(self):
        """å‘é€å®Œæˆé€šçŸ¥"""
        if not self.new_files:
            if not self.is_first_run:
                self.log("ğŸ“¢ æ— æ–°æ–‡ä»¶ï¼Œä¸å‘é€é€šçŸ¥")
            return
        
        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
        message_parts = []
        message_parts.append(f"ğŸ“Š åˆ›å®‰ç¿æ§çˆ¬è™«å®Œæˆ")
        message_parts.append(f"ğŸ•’ æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        message_parts.append(f"ğŸ“ æ–°ä¸‹è½½æ–‡ä»¶: {len(self.new_files)} ä¸ª")
        
        if self.is_first_run:
            message_parts.append("ğŸ†• é¦–æ¬¡è¿è¡Œï¼Œå·²å»ºç«‹åŸºçº¿")
        
        # æŒ‰æ¨¡å—åˆ†ç»„æ˜¾ç¤ºæ–°æ–‡ä»¶
        module_files = {}
        for file_info in self.new_files:
            module = file_info['module']
            if module not in module_files:
                module_files[module] = []
            module_files[module].append(file_info)
        
        message_parts.append("\nğŸ“‹ æ–°æ–‡ä»¶è¯¦æƒ…:")
        for module, files in module_files.items():
            message_parts.append(f"  ğŸ“‚ {module}: {len(files)} ä¸ªæ–‡ä»¶")
            for file_info in files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                size_mb = file_info['size'] / 1024 / 1024
                message_parts.append(f"    ğŸ“„ {file_info['filename']} ({size_mb:.1f}MB)")
            if len(files) > 3:
                message_parts.append(f"    ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")
        
        message = "\n".join(message_parts)
        self.send_dingtalk_notification(message)
    
    def generate_report(self, modules, all_files):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            'çˆ¬å–æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S'),
            'æ€»æ¨¡å—æ•°': len(modules),
            'æ€»æ–‡ä»¶æ•°': len(all_files),
            'æ¨¡å—åˆ—è¡¨': [m['name'] for m in modules],
            'å„æ¨¡å—æ–‡ä»¶æ•°é‡': {}
        }
        
        # ç»Ÿè®¡å„æ¨¡å—æ–‡ä»¶æ•°é‡
        for module in modules:
            module_files = [f for f in all_files if f['module'].startswith(module['name'])]
            report['å„æ¨¡å—æ–‡ä»¶æ•°é‡'][module['name']] = len(module_files)
        
        # ä¿å­˜æŠ¥å‘Š
        self.save_data(report, 'çˆ¬å–æŠ¥å‘Š.json')
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "="*50)
        print("çˆ¬å–æŠ¥å‘Š")
        print("="*50)
        print(f"çˆ¬å–æ—¶é—´: {report['çˆ¬å–æ—¶é—´']}")
        print(f"æ€»æ¨¡å—æ•°: {report['æ€»æ¨¡å—æ•°']}")
        print(f"æ€»æ–‡ä»¶æ•°: {report['æ€»æ–‡ä»¶æ•°']}")
        print("\nå„æ¨¡å—æ–‡ä»¶æ•°é‡:")
        for module_name, count in report['å„æ¨¡å—æ–‡ä»¶æ•°é‡'].items():
            print(f"  {module_name}: {count} ä¸ª")
        print("="*50)

if __name__ == "__main__":
    spider = ChuangAnSpider()
    try:
        spider.run()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
