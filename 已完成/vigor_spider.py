#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vigor (ä¸°ç‚œç§‘æŠ€) æ¡£æ¡ˆä¸‹è½½çˆ¬è™«
ç½‘ç«™ï¼šhttps://www.vigorplc.com/zh-cn/download.html
æ—¶é—´ç­›é€‰ï¼šåªä¸‹è½½2024å¹´11æœˆä¹‹åçš„æ–‡ä»¶
"""

import os
import sys
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import logging
from pathlib import Path
import re
from typing import Dict, List, Tuple, Optional
import json
import pickle
import hashlib
import hmac
import base64
import urllib.parse
from datetime import datetime, date
import dateutil.parser

class VigorSpider:
    def __init__(self, base_dir: str = None, monitor_mode: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            base_dir: ä¸‹è½½æ–‡ä»¶ä¿å­˜çš„åŸºç¡€ç›®å½•
            monitor_mode: ç›‘æ§æ¨¡å¼ï¼Œåªæ£€æµ‹æ–°æ–‡ä»¶è€Œä¸ä¸‹è½½
        """
        if base_dir is None:
            # é»˜è®¤ä½¿ç”¨æœåŠ¡å™¨ç›®å½•
            base_dir = "/srv/downloads/approved/ä¸°ç‚œ"
        
        self.base_url = "https://www.vigorplc.com"
        self.base_dir = Path(base_dir)
        self.session = requests.Session()
        self.monitor_mode = monitor_mode
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
        })
        
        # åˆ›å»ºåŸºç¡€ç›®å½•
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.ACCESS_TOKEN = "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        self.SECRET = "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb"
        
        # å·²å¤„ç†URLè®°å½•
        self.processed_urls = self.load_processed_urls()
        self.new_files = []  # æ–°å¢æ–‡ä»¶åˆ—è¡¨
        
        # æ–‡ä»¶å†å²çŠ¶æ€è·Ÿè¸ª
        self.file_history = self.load_file_history()
        self.detected_new_files = []  # æ–°æ£€æµ‹åˆ°çš„æ–‡ä»¶
        self.detected_updated_files = []  # æ£€æµ‹åˆ°æ›´æ–°çš„æ–‡ä»¶
        
        # æ–‡ä»¶åˆ†ç±»è§„åˆ™
        self.file_classification_rules = {
            'ç«¯å­å°ºå¯¸': 'ç«¯å­å°ºå¯¸å›¾',
            'é€šè®¯åè®®': 'é€šè®¯åè®®é©±åŠ¨ç¨‹åº',
            'é©±åŠ¨ç¨‹åº': 'é€šè®¯åè®®é©±åŠ¨ç¨‹åº',
            'CEè¯ä¹¦': 'CEè¯ä¹¦',
            'å‹å½•': 'å‹å½•',
            'Cata': 'å‹å½•',
            'ç¼–ç¨‹è½¯ä»¶': 'è½¯ä»¶',
            'LadderMaster': 'è½¯ä»¶',
            'simple': 'è½¯ä»¶'
        }
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        self.is_first_run = not (self.base_dir / 'processed_urls.pkl').exists()
        
        # æ—¶é—´è¿‡æ»¤ï¼šåªä¸‹è½½2024å¹´11æœˆä¹‹åçš„æ–‡ä»¶
        self.cutoff_date = date(2024, 11, 1)
        
        # ä¸‹è½½ç»Ÿè®¡
        self.stats = {
            'total_files': 0,
            'downloaded_files': 0,
            'failed_files': 0,
            'skipped_files': 0,
            'filtered_by_date': 0
        }
        
        # æ¡£æ¡ˆä¸‹è½½æ¨¡å—é…ç½® - åŸºäºå®é™…å‘ç°çš„URLç»“æ„
        self.modules = {
            'è½¯ä»¶': {
                'has_subcategories': True,
                'subcategories': {
                    'VS/VBSç³»åˆ—ç¨‹åºç¼–ç¨‹è½¯ä»¶': '/zh-cn/download-c5023/VS-VBSç³»åˆ—ç¨‹åºç¼–ç¨‹è½¯ä»¶.html',
                    'VB/VHç³»åˆ—ç¨‹åºç¼–ç¨‹è½¯ä»¶': '/zh-cn/download-c5024/VB-VHç³»åˆ—ç¨‹åºç¼–ç¨‹è½¯ä»¶.html'
                },
                'folder_name': 'è½¯ä»¶',
                'main_url': '/zh-cn/download-c5013/è½¯ä»¶.html'
            },
            'å‹å½•': {
                'has_subcategories': False,
                'url': '/zh-cn/download-c5012/å‹å½•.html',
                'folder_name': 'å‹å½•'
            },
            'æ‰‹å†Œ/è¯´æ˜ä¹¦': {
                'has_subcategories': True,
                'subcategories': {
                    'VSç³»åˆ—': '/zh-cn/download-c5036/VSç³»åˆ—.html',
                    'VBSç³»åˆ—': '/zh-cn/download-c18142/VBSç³»åˆ—.html',
                    'VB/VHç³»åˆ—': '/zh-cn/download-c5037/VB-VHç³»åˆ—.html'
                },
                'folder_name': 'æ‰‹å†Œè¯´æ˜ä¹¦',
                'main_url': '/zh-cn/download-c5017/æ‰‹å†Œ-è¯´æ˜ä¹¦.html'
            },
            'é€šè®¯åè®® / é©±åŠ¨ç¨‹åº': {
                'has_subcategories': True,
                'subcategories': {
                    'VSç³»åˆ—é€šè®¯åè®®': '/zh-cn/download-c5028/VSç³»åˆ—é€šè®¯åè®®.html',
                    'VB/VHç³»åˆ—é€šè®¯åè®®': '/zh-cn/download-c5029/VB-VHç³»åˆ—é€šè®¯åè®®.html',
                    'VSé©±åŠ¨ç¨‹åºæ‰‹åŠ¨å®‰è£…è¯´æ˜': '/zh-cn/download-c16442/VSé©±åŠ¨ç¨‹åºæ‰‹åŠ¨å®‰è£…è¯´æ˜.html',
                    'VB/VHé©±åŠ¨ç¨‹åº': '/zh-cn/download-c16438/VB-VHé©±åŠ¨ç¨‹åº.html'
                },
                'folder_name': 'é€šè®¯åè®®é©±åŠ¨ç¨‹åº',
                'main_url': '/zh-cn/download-c5014/é€šè®¯åè®®-é©±åŠ¨ç¨‹åº.html'
            },
            'ç«¯å­å°ºå¯¸å›¾': {
                'has_subcategories': False,
                'url': '/zh-cn/download-c5015/ç«¯å­å°ºå¯¸å›¾.html',
                'folder_name': 'ç«¯å­å°ºå¯¸å›¾'
            },
            'CEè¯ä¹¦': {
                'has_subcategories': True,
                'subcategories': {
                    'VSç³»åˆ—CEè¯ä¹¦': '/zh-cn/download-c5032/VSç³»åˆ—CEè¯ä¹¦.html',
                    'VBSç³»åˆ—CEè¯ä¹¦': '/zh-cn/download-c18230/VBSç³»åˆ—CEè¯ä¹¦.html',
                    'VHç³»åˆ—CEè¯ä¹¦': '/zh-cn/download-c5034/VHç³»åˆ—CEè¯ä¹¦.html',
                    'VBç³»åˆ—CEè¯ä¹¦': '/zh-cn/download-c5033/VBç³»åˆ—CEè¯ä¹¦.html'
                },
                'folder_name': 'CEè¯ä¹¦',
                'main_url': '/zh-cn/download-c5016/CEè¯ä¹¦.html'
            }
        }

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_file = self.base_dir / 'spider.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URL"""
        urls_file = self.base_dir / 'processed_urls.pkl'
        if urls_file.exists():
            try:
                with open(urls_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
        
    def save_processed_urls(self):
        """ä¿å­˜å·²å¤„ç†çš„URL"""
        urls_file = self.base_dir / 'processed_urls.pkl'
        with open(urls_file, 'wb') as f:
            pickle.dump(self.processed_urls, f)

    def load_file_history(self):
        """åŠ è½½æ–‡ä»¶å†å²è®°å½•"""
        history_file = self.base_dir / 'file_history.json'
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"åŠ è½½æ–‡ä»¶å†å²è®°å½•å¤±è´¥: {e}")
        return {}

    def save_file_history(self):
        """ä¿å­˜æ–‡ä»¶å†å²è®°å½•"""
        history_file = self.base_dir / 'file_history.json'
        history_file.parent.mkdir(parents=True, exist_ok=True)
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å†å²è®°å½•å¤±è´¥: {e}")

    def check_file_changes(self, file_info):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–"""
        file_key = f"{file_info['module']}_{file_info['title']}"
        
        if file_key not in self.file_history:
            # æ–°æ–‡ä»¶
            self.detected_new_files.append(file_info)
            self.file_history[file_key] = {
                'title': file_info['title'],
                'module': file_info['module'],
                'url': file_info['url'],
                'size': file_info.get('size', 0),
                'date': file_info.get('date', ''),
                'first_detected': datetime.now().isoformat(),
                'last_checked': datetime.now().isoformat(),
                'status': 'new'
            }
            return 'new'
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°
            existing = self.file_history[file_key]
            has_changes = False
            
            if existing.get('size', 0) != file_info.get('size', 0):
                has_changes = True
            if existing.get('date', '') != file_info.get('date', ''):
                has_changes = True
                
            # æ›´æ–°æ£€æŸ¥æ—¶é—´
            existing['last_checked'] = datetime.now().isoformat()
            
            if has_changes:
                existing['size'] = file_info.get('size', 0)
                existing['date'] = file_info.get('date', '')
                existing['status'] = 'updated'
                existing['last_updated'] = datetime.now().isoformat()
                self.detected_updated_files.append(file_info)
                return 'updated'
            else:
                existing['status'] = 'unchanged'
                return 'unchanged'
    
    def classify_file_by_name(self, filename: str, title: str = "") -> str:
        """
        æ ¹æ®æ–‡ä»¶åå’Œæ ‡é¢˜æ™ºèƒ½åˆ†ç±»æ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶å
            title: æ–‡ä»¶æ ‡é¢˜
            
        Returns:
            åˆ†ç±»ç›®å½•å
        """
        text_to_check = f"{filename} {title}".lower()
        
        for keyword, category in self.file_classification_rules.items():
            if keyword.lower() in text_to_check:
                return category
        
        # é»˜è®¤è¿”å›Noneï¼Œè®©è°ƒç”¨è€…å†³å®š
        return None

    def safe_request(self, url: str, timeout: int = 30) -> Optional[requests.Response]:
        """
        å®‰å…¨çš„HTTPè¯·æ±‚
        
        Args:
            url: è¯·æ±‚çš„URL
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            Responseå¯¹è±¡æˆ–None
        """
        try:
            self.logger.info(f"è¯·æ±‚URL: {url}")
            response = self.session.get(url, timeout=timeout, verify=False)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None

    def parse_date_from_text(self, text: str) -> Optional[date]:
        """
        ä»æ–‡æœ¬ä¸­è§£ææ—¥æœŸï¼ˆæ”¹è¿›ç‰ˆï¼Œé¿å…è¯¯åˆ¤ï¼‰
        
        Args:
            text: åŒ…å«æ—¥æœŸçš„æ–‡æœ¬
            
        Returns:
            è§£æåˆ°çš„æ—¥æœŸæˆ–None
        """
        if not text:
            return None
        
        # å¸¸è§çš„æ—¥æœŸæ ¼å¼ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ›´å‡†ç¡®ï¼‰
        date_patterns = [
            r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2024/11/23
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-11-23
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})', # 2024.11.23
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', # 2024å¹´11æœˆ23æ—¥
            r'(\d{4})å¹´(\d{1,2})æœˆ',          # 2024å¹´11æœˆ
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    year = int(match.group(1))
                    month = int(match.group(2))
                    day = int(match.group(3)) if len(match.groups()) >= 3 else 1
                    # éªŒè¯æ—¥æœŸçš„åˆç†æ€§
                    if 1900 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                        return date(year, month, day)
                except (ValueError, IndexError):
                    continue
        
        # åªæœ‰åœ¨æ–‡æœ¬æ˜ç¡®åŒ…å«æ•°å­—æ—¥æœŸæ ¼å¼æ—¶æ‰ä½¿ç”¨dateutil
        # é¿å…ä»"VSç³»åˆ—6é¡µç®€æ˜“å‹å½•"è¿™æ ·çš„æ–‡æœ¬ä¸­è¯¯è§£ææ—¥æœŸ
        if re.search(r'\d{4}[\-/\.å¹´]\d{1,2}[\-/\.æœˆ]', text):
            try:
                parsed_date = dateutil.parser.parse(text, fuzzy=True)
                # éªŒè¯è§£æç»“æœçš„åˆç†æ€§
                result_date = parsed_date.date()
                if 1900 <= result_date.year <= 2030:
                    return result_date
            except:
                pass
        
        return None

    def is_file_recent(self, file_info: Dict) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸º2024å¹´11æœˆä¹‹åçš„æ–‡ä»¶
        
        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸
            
        Returns:
            æ˜¯å¦ä¸ºæœ€è¿‘çš„æ–‡ä»¶
        """
        # ä¼˜å…ˆä»æ˜ç¡®çš„æ—¥æœŸå­—æ®µè·å–æ—¥æœŸï¼Œé¿å…ä»æ ‡é¢˜è¯¯è§£æ
        date_sources = [
            file_info.get('date_text', ''),    # é¦–å…ˆä½¿ç”¨æ˜ç¡®çš„æ—¥æœŸæ–‡æœ¬
            file_info.get('description', ''),  # ç„¶åæ˜¯æè¿°
            file_info.get('filename', ''),     # æœ€åæ˜¯æ–‡ä»¶å
            # æ³¨æ„ï¼šä¸å†ä»æ ‡é¢˜è§£ææ—¥æœŸï¼Œé¿å…"VSç³»åˆ—6é¡µç®€æ˜“å‹å½•"è¢«è¯¯åˆ¤
        ]
        
        for source in date_sources:
            if source:
                file_date = self.parse_date_from_text(source)
                if file_date and file_date >= self.cutoff_date:
                    self.logger.info(f"æ–‡ä»¶æ—¥æœŸç¬¦åˆè¦æ±‚: {file_date} >= {self.cutoff_date}")
                    return True
                elif file_date:
                    self.logger.info(f"æ–‡ä»¶æ—¥æœŸè¿‡æ—©: {file_date} < {self.cutoff_date}")
                    return False
        
        # å¦‚æœä»æ˜ç¡®å­—æ®µæ— æ³•ç¡®å®šæ—¥æœŸï¼Œå†å°è¯•ä»æ ‡é¢˜è§£æï¼ˆä½†è¦æ›´ä¸¥æ ¼ï¼‰
        title = file_info.get('title', '')
        if title:
            # åªæœ‰å½“æ ‡é¢˜æ˜ç¡®åŒ…å«å¹´ä»½æ—¶æ‰å°è¯•è§£æ
            if re.search(r'\d{4}', title):
                file_date = self.parse_date_from_text(title)
                if file_date and file_date >= self.cutoff_date:
                    self.logger.info(f"ä»æ ‡é¢˜è§£ææ—¥æœŸç¬¦åˆè¦æ±‚: {file_date} >= {self.cutoff_date}")
                    return True
                elif file_date:
                    self.logger.info(f"ä»æ ‡é¢˜è§£ææ—¥æœŸè¿‡æ—©: {file_date} < {self.cutoff_date}")
                    return False
        
        # å¦‚æœæ— æ³•ç¡®å®šæ—¥æœŸï¼Œé»˜è®¤ä¸‹è½½ï¼ˆè®©ç”¨æˆ·æ‰‹åŠ¨ç­›é€‰ï¼‰
        self.logger.warning(f"æ— æ³•ç¡®å®šæ–‡ä»¶æ—¥æœŸï¼Œé»˜è®¤ä¸‹è½½: {file_info.get('title', 'æœªçŸ¥æ–‡ä»¶')}")
        return True

    def get_download_page_url(self) -> str:
        """è·å–æ¡£æ¡ˆä¸‹è½½é¡µé¢URL"""
        return f"{self.base_url}/zh-cn/download.html"

    def parse_navigation_categories(self, soup: BeautifulSoup) -> List[Dict]:
        """
        è§£æä¸»å¯¼èˆªåˆ†ç±»
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            åˆ†ç±»åˆ—è¡¨
        """
        categories = []
        
        # æŸ¥æ‰¾å·¦ä¾§å¯¼èˆªèœå•
        # æ ¹æ®ç½‘ç«™ç»“æ„ï¼ŒæŸ¥æ‰¾æ¡£æ¡ˆä¸‹è½½éƒ¨åˆ†çš„å¯¼èˆª
        nav_sections = soup.find_all(['div', 'ul', 'li'], class_=lambda x: x and any(
            keyword in str(x).lower() for keyword in ['nav', 'menu', 'sidebar', 'category']))
        
        for section in nav_sections:
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            links = section.find_all('a', href=True)
            
            for link in links:
                text = link.get_text(strip=True)
                href = link.get('href', '')
                
                # åŒ¹é…æˆ‘ä»¬å…³å¿ƒçš„åˆ†ç±»
                if text in self.modules:
                    full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    categories.append({
                        'name': text,
                        'url': full_url,
                        'has_subcategories': self.modules[text]['has_subcategories']
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¼èˆªé“¾æ¥ï¼Œæ„å»ºé»˜è®¤URL
        if not categories:
            for module_name in self.modules:
                # æ ¹æ®è§‚å¯Ÿåˆ°çš„URLæ¨¡å¼æ„å»ºé“¾æ¥
                category_url = f"{self.base_url}/zh-cn/download.html#{module_name}"
                categories.append({
                    'name': module_name,
                    'url': category_url,
                    'has_subcategories': self.modules[module_name]['has_subcategories']
                })
        
        return categories

    def parse_subcategories(self, category_name: str, soup: BeautifulSoup) -> List[Dict]:
        """
        è§£æå­åˆ†ç±»
        
        Args:
            category_name: ä¸»åˆ†ç±»åç§°
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            å­åˆ†ç±»åˆ—è¡¨
        """
        subcategories = []
        
        if not self.modules[category_name]['has_subcategories']:
            return subcategories
        
        expected_subcats = self.modules[category_name]['subcategories']
        
        # æŸ¥æ‰¾å­åˆ†ç±»é“¾æ¥
        for subcat_name in expected_subcats:
            # åœ¨é¡µé¢ä¸­æŸ¥æ‰¾åŒ¹é…çš„é“¾æ¥
            links = soup.find_all('a', href=True)
            
            for link in links:
                link_text = link.get_text(strip=True)
                if subcat_name in link_text or link_text in subcat_name:
                    href = link.get('href', '')
                    full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    
                    subcategories.append({
                        'name': subcat_name,
                        'url': full_url,
                        'folder': self.sanitize_folder_name(subcat_name)
                    })
                    break
        
        return subcategories

    def parse_download_links(self, url: str) -> List[Dict]:
        """
        è§£æä¸‹è½½é“¾æ¥
        
        Args:
            url: é¡µé¢URL
            
        Returns:
            ä¸‹è½½ä¿¡æ¯åˆ—è¡¨
        """
        response = self.safe_request(url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        downloads = []
        
        # æŸ¥æ‰¾å®é™…çš„ä¸‹è½½é“¾æ¥
        # æŸ¥æ‰¾æ‰€æœ‰å¸¦hrefçš„é“¾æ¥
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '').strip()
            text = link.get_text(strip=True)
            
            # åªå¤„ç†çœŸæ­£çš„ä¸‹è½½é“¾æ¥
            if self.is_download_link(href, text):
                # æ„å»ºå®Œæ•´URL
                if href.startswith('/'):
                    full_url = self.base_url + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    full_url = urljoin(url, href)
                
                # æå–æ–‡ä»¶ä¿¡æ¯
                file_info = self.extract_file_info(link, soup)
                file_info['url'] = full_url
                file_info['original_href'] = href
                
                # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„é“¾æ¥
                if not any(d['url'] == full_url for d in downloads):
                    downloads.append(file_info)
                    self.logger.info(f"æ‰¾åˆ°ä¸‹è½½é“¾æ¥: {full_url} - {text}")
        
        self.logger.info(f"åœ¨ {url} æ‰¾åˆ° {len(downloads)} ä¸ªæœ‰æ•ˆä¸‹è½½é“¾æ¥")
        return downloads

    def is_download_link(self, href: str, text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºä¸‹è½½é“¾æ¥
        
        Args:
            href: é“¾æ¥åœ°å€
            text: é“¾æ¥æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºä¸‹è½½é“¾æ¥
        """
        if not href:
            return False
        
        # é¦–å…ˆæ’é™¤æ˜æ˜¾çš„HTMLé¡µé¢é“¾æ¥
        exclude_patterns = [
            'download.html', 'download.asp?customer_id=', 'product-c', 
            '.html', 'Directory_ID=', 'name_id=', '#'
        ]
        
        for pattern in exclude_patterns:
            if pattern in href:
                return False
        
        # æ£€æŸ¥çœŸæ­£çš„ä¸‹è½½é“¾æ¥æ ¼å¼
        # ä¸°ç‚œç§‘æŠ€çš„çœŸå®ä¸‹è½½é“¾æ¥æ ¼å¼: /v_comm/inc/download_file.asp?re_id=xxx&fid=xxx
        if 'download_file.asp' in href and 'fid=' in href and 're_id=' in href:
            return True
        
        # æ£€æŸ¥ç›´æ¥æ–‡ä»¶é“¾æ¥
        download_extensions = ['.pdf', '.doc', '.docx', '.zip', '.rar', '.exe', '.msi']
        href_lower = href.lower()
        for ext in download_extensions:
            if href_lower.endswith(ext):
                return True
        
        return False

    def extract_file_info(self, link, soup: BeautifulSoup) -> Dict:
        """
        æå–æ–‡ä»¶ä¿¡æ¯ï¼ˆé’ˆå¯¹ä¸°ç‚œç§‘æŠ€é¡µé¢ç»“æ„ä¼˜åŒ–ï¼‰
        
        Args:
            link: ä¸‹è½½é“¾æ¥å…ƒç´ 
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯å­—å…¸
        """
        file_info = {
            'title': '',
            'filename': '',
            'date_text': '',
            'description': '',
            'size': ''
        }
        
        # æå–æ ‡é¢˜
        title = link.get_text(strip=True)
        
        # ç‰¹æ®Šå¤„ç†ä¸°ç‚œç§‘æŠ€çš„é¡µé¢ç»“æ„
        if 'download_file.asp' in link.get('href', ''):
            # å¦‚æœæ˜¯"æ¡£æ¡ˆä¸‹è½½"æŒ‰é’®ï¼Œå¯»æ‰¾å®é™…çš„æ–‡ä»¶æ ‡é¢˜
            if not title or 'æ¡£æ¡ˆä¸‹è½½' in title:
                # å¯»æ‰¾åŒä¸€ä¸ªdlå…ƒç´ ä¸­çš„æ ‡é¢˜
                parent_dl = link.find_parent('dl')
                if parent_dl:
                    title_elem = parent_dl.find('dt', class_='list_title')
                    if title_elem:
                        title_link = title_elem.find('a')
                        if title_link:
                            title = title_link.get_text(strip=True)
            
            # æå–æ–‡ä»¶å¤§å°
            parent_span = link.find_parent('span', class_='list_download_icon')
            if parent_span:
                size_text = parent_span.get_text(strip=True)
                size_match = re.search(r'\(([\d.]+\s*[KMGT]?B)\)', size_text)
                if size_match:
                    file_info['size'] = size_match.group(1)
            
            # æå–å‘å¸ƒæ—¥æœŸ
            parent_dl = link.find_parent('dl')
            if parent_dl:
                # æŸ¥æ‰¾vigor-timeç±»çš„div
                date_elem = parent_dl.find('div', class_='vigor-time')
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    date_match = re.search(r'(\d{4}/\d{1,2}/\d{1,2})', date_text)
                    if date_match:
                        file_info['date_text'] = date_match.group(1)
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ—¥æœŸæ ¼å¼
                if not file_info['date_text']:
                    dd_elem = parent_dl.find('dd', class_='list_txt')
                    if dd_elem:
                        dd_text = dd_elem.get_text()
                        date_match = re.search(r'(\d{4}/\d{1,2}/\d{1,2})', dd_text)
                        if date_match:
                            file_info['date_text'] = date_match.group(1)
        else:
            # åŸæœ‰çš„é€šç”¨é€»è¾‘
            if title:
                file_info['title'] = title
                file_info['filename'] = self.extract_filename_from_title(title)
            
            # ä»çˆ¶å…ƒç´ æˆ–å…„å¼Ÿå…ƒç´ æå–æ›´å¤šä¿¡æ¯
            parent = link.parent
            if parent:
                # æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
                date_text = self.find_date_in_element(parent)
                if date_text:
                    file_info['date_text'] = date_text
                
                # æŸ¥æ‰¾æè¿°ä¿¡æ¯
                description = parent.get_text(strip=True)
                if description and len(description) > len(title):
                    file_info['description'] = description
            
            # æŸ¥æ‰¾å…„å¼Ÿå…ƒç´ ä¸­çš„ä¿¡æ¯
            siblings = link.find_next_siblings()
            for sibling in siblings[:3]:  # åªæ£€æŸ¥å‰3ä¸ªå…„å¼Ÿå…ƒç´ 
                sibling_text = sibling.get_text(strip=True)
                if sibling_text:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸ
                    if self.parse_date_from_text(sibling_text):
                        file_info['date_text'] = sibling_text
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶å¤§å°
                    if re.search(r'\d+\s*(KB|MB|GB)', sibling_text, re.I):
                        file_info['size'] = sibling_text
        
        # è®¾ç½®æœ€ç»ˆçš„æ ‡é¢˜å’Œæ–‡ä»¶å
        if title:
            file_info['title'] = title
            file_info['filename'] = self.extract_filename_from_title(title)
        
        return file_info

    def extract_file_info_from_table_row(self, row) -> Dict:
        """
        ä»è¡¨æ ¼è¡Œä¸­æå–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            row: è¡¨æ ¼è¡Œå…ƒç´ 
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯å­—å…¸
        """
        file_info = {
            'title': '',
            'filename': '',
            'date_text': '',
            'description': '',
            'size': ''
        }
        
        cells = row.find_all(['td', 'th'])
        
        # å°è¯•è¯†åˆ«è¡¨æ ¼åˆ—çš„å«ä¹‰
        for i, cell in enumerate(cells):
            cell_text = cell.get_text(strip=True)
            
            if i == 0:  # é€šå¸¸ç¬¬ä¸€åˆ—æ˜¯æ–‡ä»¶åæˆ–æ ‡é¢˜
                file_info['title'] = cell_text
                file_info['filename'] = self.extract_filename_from_title(cell_text)
            elif self.parse_date_from_text(cell_text):  # åŒ…å«æ—¥æœŸçš„åˆ—
                file_info['date_text'] = cell_text
            elif re.search(r'\d+\s*(KB|MB|GB)', cell_text, re.I):  # åŒ…å«æ–‡ä»¶å¤§å°çš„åˆ—
                file_info['size'] = cell_text
            elif len(cell_text) > 10:  # å¯èƒ½æ˜¯æè¿°åˆ—
                file_info['description'] = cell_text
        
        return file_info

    def find_date_in_element(self, element) -> str:
        """
        åœ¨å…ƒç´ ä¸­æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
        
        Args:
            element: HTMLå…ƒç´ 
            
        Returns:
            æ‰¾åˆ°çš„æ—¥æœŸæ–‡æœ¬
        """
        text = element.get_text(strip=True)
        
        # æŸ¥æ‰¾å¸¸è§çš„æ—¥æœŸæ¨¡å¼
        date_patterns = [
            r'\d{4}[/-]\d{1,2}[/-]\d{1,2}',
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{4}å¹´\d{1,2}æœˆ',
            r'\d{4}/\d{1,2}/\d{1,2}',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group()
        
        return ''

    def extract_filename_from_title(self, title: str) -> str:
        """
        ä»æ ‡é¢˜ä¸­æå–æ–‡ä»¶å
        
        Args:
            title: æ ‡é¢˜æ–‡æœ¬
            
        Returns:
            æ–‡ä»¶å
        """
        # æ¸…ç†æ–‡ä»¶å
        filename = re.sub(r'[<>:"/\\|?*\[\]{}]', '', title.strip())
        
        # å¤„ç†ç‰¹æ®Šå­—ç¬¦
        filename = filename.replace('/', '_').replace(' ', '_')
        
        # ç¡®ä¿æ–‡ä»¶åæœ‰æ‰©å±•å
        if not re.search(r'\.[a-zA-Z0-9]+$', filename):
            filename += '.pdf'  # é»˜è®¤ä¸ºPDF
        
        return filename

    def test_asp_download_link(self, url: str) -> bool:
        """
        æµ‹è¯•ASPä¸‹è½½é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            url: ASPä¸‹è½½é“¾æ¥
            
        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥
        """
        try:
            # å‘é€HEADè¯·æ±‚æ£€æŸ¥é“¾æ¥
            response = self.session.head(url, timeout=10, verify=False)
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                content_length = int(response.headers.get('content-length', 0))
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ä¸‹è½½
                if any(ct in content_type for ct in ['application/', 'binary/', 'octet-stream']):
                    self.logger.info(f"ASPé“¾æ¥æœ‰æ•ˆ: content-type={content_type}, size={content_length}")
                    return True
                elif content_length > 100000:  # å¤§äº100KBå¯èƒ½æ˜¯æ–‡ä»¶
                    self.logger.info(f"ASPé“¾æ¥å¯èƒ½æœ‰æ•ˆï¼ˆå¤§æ–‡ä»¶ï¼‰: size={content_length}")
                    return True
                else:
                    self.logger.warning(f"ASPé“¾æ¥å¯èƒ½æ— æ•ˆ: content-type={content_type}, size={content_length}")
                    return False
            else:
                self.logger.error(f"ASPé“¾æ¥è®¿é—®å¤±è´¥: status={response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"æµ‹è¯•ASPé“¾æ¥å¤±è´¥: {e}")
            return False

    def sanitize_folder_name(self, name: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶å¤¹åç§°
        
        Args:
            name: åŸå§‹åç§°
            
        Returns:
            æ¸…ç†åçš„åç§°
        """
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'[<>:"/\\|?*]', '_', name.strip())
        name = re.sub(r'\s+', '_', name)
        return name

    def get_unique_filename(self, filepath: Path) -> Path:
        """
        è·å–å”¯ä¸€çš„æ–‡ä»¶åï¼Œå¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™æ·»åŠ æ•°å­—åç¼€
        
        Args:
            filepath: åŸå§‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            å”¯ä¸€çš„æ–‡ä»¶è·¯å¾„
        """
        if not filepath.exists():
            return filepath
        
        # åˆ†ç¦»æ–‡ä»¶åå’Œæ‰©å±•å
        stem = filepath.stem
        suffix = filepath.suffix
        parent = filepath.parent
        
        # æ·»åŠ æ•°å­—åç¼€
        counter = 1
        while True:
            new_name = f"{stem}_{counter:02d}{suffix}"
            new_filepath = parent / new_name
            if not new_filepath.exists():
                return new_filepath
            counter += 1
            
            # é˜²æ­¢æ— é™å¾ªç¯
            if counter > 999:
                return filepath

    def download_file(self, url: str, filepath: Path, file_info: Dict) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒæ–‡ä»¶éªŒè¯ï¼‰
        
        Args:
            url: æ–‡ä»¶URL
            filepath: ä¿å­˜è·¯å¾„
            file_info: æ–‡ä»¶ä¿¡æ¯
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            # æ—¶é—´è¿‡æ»¤
            if not self.is_file_recent(file_info):
                self.logger.info(f"æ–‡ä»¶æ—¶é—´è¿‡æ—©ï¼Œè·³è¿‡: {file_info.get('title', filepath.name)}")
                self.stats['filtered_by_date'] += 1
                return True
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡
            if url in self.processed_urls:
                self.logger.info(f"URLå·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {file_info.get('title', filepath.name)}")
                self.stats['skipped_files'] += 1
                return True
            
            # ç”Ÿæˆæ›´å¥½çš„æ–‡ä»¶å
            if file_info.get('title'):
                better_filename = self.extract_filename_from_title(file_info['title'])
                new_filepath = filepath.parent / better_filename
                filepath = self.get_unique_filename(new_filepath)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if filepath.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filepath.name}")
                self.stats['skipped_files'] += 1
                return True
            
            # åˆ›å»ºç›®å½•
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½æ–‡ä»¶
            self.logger.info(f"å¼€å§‹ä¸‹è½½: {file_info.get('title', filepath.name)} -> {filepath.name}")
            self.logger.info(f"è¯·æ±‚URL: {url}")
            
            response = self.safe_request(url)
            if not response:
                return False
            
            # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            content_length = int(response.headers.get('content-length', 0))
            content_disposition = response.headers.get('content-disposition', '')
            
            # ä»Content-Dispositionå¤´ä¸­æå–çœŸå®æ–‡ä»¶å
            real_filename = None
            if content_disposition:
                import urllib.parse
                # è§£æContent-Dispositionå¤´
                if 'filename=' in content_disposition:
                    filename_part = content_disposition.split('filename=')[1]
                    if filename_part.startswith('"'):
                        # å¸¦å¼•å·çš„æ–‡ä»¶å
                        real_filename = filename_part.split('"')[1]
                    else:
                        # ä¸å¸¦å¼•å·çš„æ–‡ä»¶åï¼Œå¯èƒ½æœ‰URLç¼–ç 
                        real_filename = filename_part.split(';')[0].strip()
                        real_filename = urllib.parse.unquote(real_filename)
                
                if real_filename:
                    self.logger.info(f"ä»å“åº”å¤´è·å–çœŸå®æ–‡ä»¶å: {real_filename}")
                    # æ›´æ–°æ–‡ä»¶è·¯å¾„
                    new_filepath = filepath.parent / real_filename
                    filepath = self.get_unique_filename(new_filepath)
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ–‡ä»¶ä¸‹è½½
            if 'text/html' in content_type and content_length < 50000:  # å°äº50KBçš„HTMLå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                self.logger.warning(f"å“åº”å¯èƒ½æ˜¯é”™è¯¯é¡µé¢è€Œéæ–‡ä»¶: content-type={content_type}, length={content_length}")
                # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                content_preview = response.content[:1000].decode('utf-8', errors='ignore')
                if any(keyword in content_preview.lower() for keyword in ['error', 'é”™è¯¯', '404', '403', 'not found']):
                    self.logger.error(f"å“åº”åŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œè·³è¿‡ä¸‹è½½: {file_info.get('title', filepath.name)}")
                    self.stats['failed_files'] += 1
                    return False
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            file_size = filepath.stat().st_size
            
            # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
            if file_size < 512:  # å°äº512å­—èŠ‚çš„æ–‡ä»¶å¯èƒ½æœ‰é—®é¢˜
                self.logger.warning(f"ä¸‹è½½çš„æ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½æ— æ•ˆ: {filepath.name} ({file_size} bytes)")
                # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                try:
                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read(500)
                        if any(keyword in content.lower() for keyword in ['error', 'é”™è¯¯', '404', '403', 'not found', '<html']):
                            self.logger.error(f"æ–‡ä»¶å†…å®¹åŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œåˆ é™¤æ–‡ä»¶: {filepath.name}")
                            filepath.unlink()  # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                            self.stats['failed_files'] += 1
                            return False
                except:
                    pass  # å¦‚æœä¸æ˜¯æ–‡æœ¬æ–‡ä»¶ï¼Œå¿½ç•¥å†…å®¹æ£€æŸ¥
            
            # å¦‚æœæ˜¯å¤§æ–‡ä»¶ï¼Œæ˜¾ç¤ºæ›´å‹å¥½çš„å¤§å°ä¿¡æ¯
            if file_size > 1024 * 1024:  # å¤§äº1MB
                size_str = f"{file_size/1024/1024:.2f} MB"
            elif file_size > 1024:  # å¤§äº1KB
                size_str = f"{file_size/1024:.1f} KB"
            else:
                size_str = f"{file_size} bytes"
            
            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {filepath.name} ({size_str})")
            
            # è®°å½•å¤„ç†çš„URL
            self.processed_urls.add(url)
            
            # æ·»åŠ åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
            self.new_files.append({
                'type': 'PDF' if filepath.suffix.lower() == '.pdf' else 'æ–‡æ¡£',
                'title': file_info.get('title', filepath.stem),
                'path': str(filepath),
                'url': url,
                'size': file_size,
                'date': file_info.get('date_text', '')
            })
            
            self.stats['downloaded_files'] += 1
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            self.stats['failed_files'] += 1
            return False

    def crawl_module(self, module_name: str, module_config: Dict):
        """
        çˆ¬å–æŒ‡å®šæ¨¡å—
        
        Args:
            module_name: æ¨¡å—åç§°
            module_config: æ¨¡å—é…ç½®
        """
        self.logger.info(f"å¼€å§‹çˆ¬å–æ¨¡å—: {module_name}")
        
        folder_name = module_config['folder_name']
        has_subcategories = module_config['has_subcategories']
        
        module_dir = self.base_dir / folder_name
        module_dir.mkdir(parents=True, exist_ok=True)
        
        if has_subcategories:
            # å¤„ç†æœ‰å­åˆ†ç±»çš„æ¨¡å—
            subcategories = module_config['subcategories']
            
            # é¦–å…ˆå°è¯•ä»ä¸»åˆ†ç±»é¡µé¢è·å–ä¸‹è½½
            if 'main_url' in module_config:
                main_url = self.base_url + module_config['main_url']
                self.logger.info(f"æ£€æŸ¥ä¸»åˆ†ç±»é¡µé¢: {main_url}")
                
                main_downloads = self.parse_download_links(main_url)
                if main_downloads:
                    self.logger.info(f"ä¸»åˆ†ç±»é¡µé¢æ‰¾åˆ° {len(main_downloads)} ä¸ªä¸‹è½½")
                    for download in main_downloads:
                        self.stats['total_files'] += 1
                        
                        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ—¶é—´è¿‡æ»¤æ¡ä»¶
                        if not self.is_file_recent(download):
                            self.stats['filtered_by_date'] += 1
                            self.logger.info(f"æ–‡ä»¶å› æ—¥æœŸè¿‡æ»¤è€Œè·³è¿‡: {download.get('title', 'æœªçŸ¥æ–‡ä»¶')}")
                            continue
                        
                        # æ™ºèƒ½åˆ†ç±»æ–‡ä»¶
                        smart_category = self.classify_file_by_name(
                            download.get('filename', ''), 
                            download.get('title', '')
                        )
                        
                        if smart_category and smart_category != folder_name:
                            # æ–‡ä»¶åº”è¯¥å½’ç±»åˆ°å…¶ä»–ç›®å½•
                            target_dir = self.base_dir / smart_category
                            target_dir.mkdir(parents=True, exist_ok=True)
                            self.logger.info(f"æ ¹æ®æ–‡ä»¶åæ™ºèƒ½åˆ†ç±»: {download.get('title', '')} -> {smart_category}")
                        else:
                            target_dir = module_dir
                        
                        # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºä¸‹è½½
                        temp_filename = f"temp_{int(time.time())}_{download.get('url', '').split('fid=')[-1]}.tmp"
                        temp_filepath = target_dir / temp_filename
                        
                        self.download_file(download['url'], temp_filepath, download)
                        time.sleep(1)
            
            # ç„¶åå¤„ç†å­åˆ†ç±»
            for subcat_name, subcat_url in subcategories.items():
                self.logger.info(f"å¤„ç†å­åˆ†ç±»: {subcat_name}")
                
                # åˆ›å»ºå­åˆ†ç±»ç›®å½•
                subcat_dir = module_dir / self.sanitize_folder_name(subcat_name)
                subcat_dir.mkdir(parents=True, exist_ok=True)
                
                # æ„å»ºå®Œæ•´URL
                full_url = self.base_url + subcat_url
                
                # çˆ¬å–å­åˆ†ç±»é¡µé¢
                downloads = self.parse_download_links(full_url)
                
                for download in downloads:
                    self.stats['total_files'] += 1
                    
                    # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ—¶é—´è¿‡æ»¤æ¡ä»¶
                    if not self.is_file_recent(download):
                        self.stats['filtered_by_date'] += 1
                        self.logger.info(f"æ–‡ä»¶å› æ—¥æœŸè¿‡æ»¤è€Œè·³è¿‡: {download.get('title', 'æœªçŸ¥æ–‡ä»¶')}")
                        continue
                    
                    # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºä¸‹è½½
                    temp_filename = f"temp_{int(time.time())}_{download.get('url', '').split('fid=')[-1]}.tmp"
                    temp_filepath = subcat_dir / temp_filename
                    
                    self.download_file(download['url'], temp_filepath, download)
                    time.sleep(1)
        else:
            # å¤„ç†æ²¡æœ‰å­åˆ†ç±»çš„æ¨¡å—
            module_url = self.base_url + module_config['url']
            downloads = self.parse_download_links(module_url)
            
            for download in downloads:
                self.stats['total_files'] += 1
                
                # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ—¶é—´è¿‡æ»¤æ¡ä»¶
                if not self.is_file_recent(download):
                    self.stats['filtered_by_date'] += 1
                    self.logger.info(f"æ–‡ä»¶å› æ—¥æœŸè¿‡æ»¤è€Œè·³è¿‡: {download.get('title', 'æœªçŸ¥æ–‡ä»¶')}")
                    continue
                
                # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºä¸‹è½½
                temp_filename = f"temp_{int(time.time())}_{download.get('url', '').split('fid=')[-1]}.tmp"
                temp_filepath = module_dir / temp_filename
                
                self.download_file(download['url'], temp_filepath, download)
                time.sleep(1)

    def save_progress(self):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        progress_file = self.base_dir / 'crawl_progress.json'
        progress_data = {
            'timestamp': time.time(),
            'stats': self.stats,
            'cutoff_date': self.cutoff_date.isoformat(),
            'completed_modules': []
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        if self.monitor_mode:
            return self.run_monitor_mode()
            
        self.logger.info("å¼€å§‹çˆ¬å–ä¸°ç‚œç§‘æŠ€æ¡£æ¡ˆä¸‹è½½ä¸­å¿ƒ")
        self.logger.info(f"ä¿å­˜ç›®å½•: {self.base_dir}")
        self.logger.info(f"æ—¶é—´è¿‡æ»¤: åªä¸‹è½½ {self.cutoff_date} ä¹‹åçš„æ–‡ä»¶")
        
        start_time = time.time()
        
        try:
            # çˆ¬å–æ‰€æœ‰æ¨¡å—
            for module_name, module_config in self.modules.items():
                self.logger.info(f"=" * 50)
                self.crawl_module(module_name, module_config)
                time.sleep(2)  # æ¨¡å—é—´æš‚åœ
                
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            # ä¿å­˜è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
            self.save_progress()
            
            # ä¿å­˜å·²å¤„ç†çš„URLs
            self.save_processed_urls()
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            duration = end_time - start_time
            
            self.logger.info("=" * 50)
            self.logger.info("çˆ¬å–å®Œæˆ!")
            self.logger.info(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
            self.logger.info(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {self.stats['downloaded_files']}")
            self.logger.info(f"è·³è¿‡æ–‡ä»¶: {self.stats['skipped_files']}")
            self.logger.info(f"æ—¶é—´è¿‡æ»¤: {self.stats['filtered_by_date']}")
            self.logger.info(f"ä¸‹è½½å¤±è´¥: {self.stats['failed_files']}")
            self.logger.info(f"ä¿å­˜ç›®å½•: {self.base_dir}")
            
            # å‘é€é€šçŸ¥
            if self.new_files:
                self.send_notifications()

    def run_monitor_mode(self):
        """
        è¿è¡Œç›‘æ§æ¨¡å¼ - åªæ£€æµ‹æ–°æ–‡ä»¶ï¼Œä¸ä¸‹è½½
        """
        self.logger.info("ğŸ” å¼€å§‹è¿è¡Œç›‘æ§æ¨¡å¼ - æ£€æµ‹æ–°æ–‡ä»¶")
        self.logger.info(f"ç›‘æ§ç›®å½•: {self.base_dir}")
        self.logger.info(f"æ—¶é—´è¿‡æ»¤: åªå…³æ³¨ {self.cutoff_date} ä¹‹åçš„æ–‡ä»¶")
        
        start_time = time.time()
        
        try:
            # ç›‘æ§æ‰€æœ‰æ¨¡å—
            for module_name, module_config in self.modules.items():
                self.logger.info(f"=" * 50)
                self.logger.info(f"ğŸ” ç›‘æ§æ¨¡å—: {module_name}")
                
                # è·å–æ–‡ä»¶åˆ—è¡¨ä½†ä¸ä¸‹è½½
                files = self.get_module_files(module_name, module_config)
                
                # æ£€æŸ¥æ–‡ä»¶å˜åŒ–
                for file_info in files:
                    if self.is_file_recent(file_info):
                        change_status = self.check_file_changes(file_info)
                        if change_status == 'new':
                            self.logger.info(f"  ğŸ†• å‘ç°æ–°æ–‡ä»¶: {file_info['title']}")
                        elif change_status == 'updated':
                            self.logger.info(f"  ğŸ”„ æ–‡ä»¶å·²æ›´æ–°: {file_info['title']}")
                
                time.sleep(1)  # å‡å°‘å»¶æ—¶
                
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ç›‘æ§")
        except Exception as e:
            self.logger.error(f"ç›‘æ§è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            # ä¿å­˜æ–‡ä»¶å†å²
            self.save_file_history()
            
            # å‘é€ç›‘æ§é€šçŸ¥
            if self.detected_new_files or self.detected_updated_files:
                self.send_monitor_notifications()
            else:
                self.logger.info("âœ… ç›‘æ§å®Œæˆï¼Œæœªå‘ç°æ–°æ–‡ä»¶æˆ–æ›´æ–°")
                
            end_time = time.time()
            duration = end_time - start_time
            self.logger.info("=" * 50)
            self.logger.info(f"ç›‘æ§å®Œæˆ! è€—æ—¶: {duration:.2f} ç§’")
            self.logger.info(f"æ–°æ–‡ä»¶: {len(self.detected_new_files)} ä¸ª")
            self.logger.info(f"æ›´æ–°æ–‡ä»¶: {len(self.detected_updated_files)} ä¸ª")

    def get_module_files(self, module_name, module_config):
        """è·å–æ¨¡å—çš„æ–‡ä»¶åˆ—è¡¨ï¼Œä¸ä¸‹è½½"""
        files = []
        try:
            has_subcategories = module_config['has_subcategories']
            
            if has_subcategories:
                # å¤„ç†æœ‰å­åˆ†ç±»çš„æ¨¡å— - å…ˆæ£€æŸ¥ä¸»é¡µé¢
                if 'main_url' in module_config:
                    main_url = self.base_url + module_config['main_url']
                    self.logger.info(f"æ£€æŸ¥ä¸»åˆ†ç±»é¡µé¢: {main_url}")
                    main_files = self.parse_download_links(main_url)
                    for file_info in main_files:
                        file_info['module'] = module_name
                        files.append(file_info)
                
                # å¤„ç†å­åˆ†ç±»
                subcategories = module_config.get('subcategories', {})
                for subcategory, sub_config in subcategories.items():
                    self.logger.info(f"å¤„ç†å­åˆ†ç±»: {subcategory}")
                    if isinstance(sub_config, dict) and 'url' in sub_config:
                        sub_url = self.base_url + sub_config['url']
                    else:
                        # å¦‚æœé…ç½®æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½œä¸ºURL
                        sub_url = self.base_url + str(sub_config)
                    sub_files = self.parse_download_links(sub_url)
                    for file_info in sub_files:
                        file_info['module'] = module_name
                        file_info['subcategory'] = subcategory
                        files.append(file_info)
            else:
                # å¤„ç†æ²¡æœ‰å­åˆ†ç±»çš„æ¨¡å—
                main_url = self.base_url + module_config['main_url']
                files = self.parse_download_links(main_url)
                for file_info in files:
                    file_info['module'] = module_name
                    
        except Exception as e:
            self.logger.error(f"è·å– {module_name} æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        
        return files

    def send_monitor_notifications(self):
        """å‘é€ç›‘æ§æ¨¡å¼çš„é€šçŸ¥"""
        try:
            new_count = len(self.detected_new_files)
            updated_count = len(self.detected_updated_files)
            
            if new_count == 0 and updated_count == 0:
                return
                
            # æ§åˆ¶å°é€šçŸ¥
            self.logger.info(f"\nğŸ”” æ–‡ä»¶å˜åŒ–æ£€æµ‹é€šçŸ¥:")
            self.logger.info("=" * 60)
            
            if new_count > 0:
                self.logger.info(f"ğŸ†• å‘ç° {new_count} ä¸ªæ–°æ–‡ä»¶:")
                for file_info in self.detected_new_files[:5]:
                    self.logger.info(f"  ğŸ“„ {file_info['title']} ({file_info['module']})")
                if new_count > 5:
                    self.logger.info(f"  ... è¿˜æœ‰ {new_count - 5} ä¸ªæ–°æ–‡ä»¶")
            
            if updated_count > 0:
                self.logger.info(f"ğŸ”„ å‘ç° {updated_count} ä¸ªæ–‡ä»¶æ›´æ–°:")
                for file_info in self.detected_updated_files[:5]:
                    self.logger.info(f"  ğŸ“„ {file_info['title']} ({file_info['module']})")
                if updated_count > 5:
                    self.logger.info(f"  ... è¿˜æœ‰ {updated_count - 5} ä¸ªæ›´æ–°æ–‡ä»¶")
            
            # é’‰é’‰é€šçŸ¥
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            message = f"""ğŸ”” ä¸°ç‚œç§‘æŠ€ æ–‡ä»¶ç›‘æ§æŠ¥å‘Š

ğŸ“Š æ£€æµ‹ç»Ÿè®¡:
  æ–°å¢æ–‡ä»¶: {new_count} ä¸ª
  æ›´æ–°æ–‡ä»¶: {updated_count} ä¸ª
  æ€»å˜åŒ–: {new_count + updated_count} ä¸ª

ğŸ“‚ ç›‘æ§ç›®å½•: {self.base_dir}
â° æ£€æµ‹æ—¶é—´: {current_time}
ğŸ•’ æ—¶é—´ç­›é€‰: åªå…³æ³¨2024å¹´11æœˆä¹‹åçš„æ–‡ä»¶

ğŸ’¡ æç¤º: è¿è¡Œæ­£å¸¸ä¸‹è½½æ¨¡å¼å¯è·å–è¿™äº›æ–°æ–‡ä»¶"""
            
            # å‘é€é’‰é’‰é€šçŸ¥
            self.send_dingtalk_notification(message)
            
        except Exception as e:
            self.logger.error(f"å‘é€ç›‘æ§é€šçŸ¥å¤±è´¥: {e}")

    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥"""
        try:
            timestamp = str(round(time.time() * 1000))
            string_to_sign = f'{timestamp}\n{self.SECRET}'
            hmac_code = hmac.new(self.SECRET.encode(), string_to_sign.encode(), hashlib.sha256).digest()
            sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))

            url = f'https://oapi.dingtalk.com/robot/send?access_token={self.ACCESS_TOKEN}&timestamp={timestamp}&sign={sign}'
            headers = {'Content-Type': 'application/json'}
            data = {
                "msgtype": "text",
                "text": {"content": message},
                "at": {"isAtAll": False}
            }

            response = requests.post(url, json=data, headers=headers)
            self.logger.info(f"é’‰é’‰é€šçŸ¥å“åº”ï¼š{response.status_code} {response.text}")
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {e}")
            return False

    def send_notifications(self):
        """å‘é€æ–°å¢æ–‡ä»¶é€šçŸ¥"""
        try:
            if not self.new_files:
                return
            
            # æ§åˆ¶å°é€šçŸ¥
            self.logger.info(f"\nğŸ‰ çˆ¬å–å®Œæˆé€šçŸ¥:")
            self.logger.info("=" * 60)
            self.logger.info(f"ğŸ“Š å‘ç° {len(self.new_files)} ä¸ªæ–°æ–‡ä»¶ (2024.11ä¹‹å):")
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = {}
            for file_info in self.new_files:
                file_type = file_info['type']
                type_counts[file_type] = type_counts.get(file_type, 0) + 1
            
            for file_type, count in type_counts.items():
                self.logger.info(f"  ğŸ“ {file_type}: {count} ä¸ª")
            
            self.logger.info(f"\nğŸ“‚ æœ€æ–°æ–‡ä»¶é¢„è§ˆ:")
            for file_info in self.new_files[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                size_str = f" ({file_info['size']/1024:.1f}KB)" if 'size' in file_info else ""
                date_str = f" [{file_info['date']}]" if file_info.get('date') else ""
                self.logger.info(f"  ğŸ“„ {file_info['title']}{size_str}{date_str}")
            
            if len(self.new_files) > 5:
                self.logger.info(f"  ... è¿˜æœ‰ {len(self.new_files) - 5} ä¸ªæ–‡ä»¶")
                
            self.logger.info(f"\nğŸ’¾ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³: {self.base_dir}")
        
            # é’‰é’‰é€šçŸ¥
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            total_files = len(self.new_files)
            success_rate = 100.0 if self.stats['failed_files'] == 0 else (self.stats['downloaded_files'] / (self.stats['downloaded_files'] + self.stats['failed_files'])) * 100
            
            message = f"""âœ… ä¸°ç‚œç§‘æŠ€ æ¡£æ¡ˆä¸‹è½½çˆ¬å–æˆåŠŸï¼Œè¯·åŠæ—¶å®¡æ ¸

ğŸ“Š ä¸‹è½½ç»Ÿè®¡:
  æˆåŠŸä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶ (2024.11+)
  æ€»è¯†åˆ«æ–‡ä»¶: {self.stats['total_files']} ä¸ª
  æ—¶é—´è¿‡æ»¤: {self.stats['filtered_by_date']} ä¸ª
  æˆåŠŸç‡: {success_rate:.1f}%

ğŸ“ æ–‡ä»¶å­˜æ”¾è·¯å¾„: {self.base_dir}
â° å®Œæˆæ—¶é—´: {current_time}
ğŸ•’ æ—¶é—´ç­›é€‰: åªä¸‹è½½2024å¹´11æœˆä¹‹åçš„æ–‡ä»¶"""
            
            # å‘é€é’‰é’‰é€šçŸ¥
            self.send_dingtalk_notification(message)
            
        except Exception as e:
            self.logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¸°ç‚œç§‘æŠ€æ¡£æ¡ˆä¸‹è½½çˆ¬è™«')
    parser.add_argument('--base-dir', type=str, help='ä¸‹è½½æ–‡ä»¶ä¿å­˜çš„åŸºç¡€ç›®å½•')
    parser.add_argument('--monitor', action='store_true', help='è¿è¡Œç›‘æ§æ¨¡å¼ï¼Œåªæ£€æµ‹æ–°æ–‡ä»¶ä¸ä¸‹è½½')
    parser.add_argument('--modules', nargs='+', help='æŒ‡å®šè¦çˆ¬å–çš„æ¨¡å—ï¼Œå¦‚ï¼šè½¯ä»¶ å‹å½• CEè¯ä¹¦')
    parser.add_argument('--no-date-filter', action='store_true', help='ç¦ç”¨æ—¶é—´è¿‡æ»¤ï¼Œä¸‹è½½æ‰€æœ‰æ–‡ä»¶')
    
    args = parser.parse_args()
    
    spider = VigorSpider(base_dir=args.base_dir, monitor_mode=args.monitor)
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å—ï¼Œåªå¤„ç†è¿™äº›æ¨¡å—
    if args.modules:
        # ä¸´æ—¶ä¿®æ”¹æ¨¡å—é…ç½®
        selected_modules = {}
        for module_name in args.modules:
            if module_name in spider.modules:
                selected_modules[module_name] = spider.modules[module_name]
            else:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ¨¡å— '{module_name}'")
                print(f"å¯ç”¨æ¨¡å—: {list(spider.modules.keys())}")
        
        if selected_modules:
            spider.modules = selected_modules
        else:
            print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ¨¡å—")
            return
    
    # å¦‚æœç¦ç”¨æ—¶é—´è¿‡æ»¤
    if args.no_date_filter:
        spider.cutoff_date = date(2000, 1, 1)  # è®¾ç½®ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸ
    
    spider.run()


if __name__ == "__main__":
    main()
