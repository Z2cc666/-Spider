#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹“æ–¯è¾¾äº§å“ä¸­å¿ƒçˆ¬è™«
çˆ¬å–äº§å“å’ŒæŠ€æœ¯ä¸‹é¢çš„æ‰€æœ‰æ¨¡å—
æ”¯æŒé’‰é’‰é€šçŸ¥å’Œè‡ªåŠ¨æ£€æµ‹æ–°æ–‡ä»¶
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
import pickle
import hmac
import hashlib
import base64
from datetime import datetime
from urllib.parse import urljoin, urlparse, quote_plus
import re
import urllib.request

class TopstarSpider:
    def __init__(self):
        self.base_url = "https://www.topstarltd.com"
        self.main_url = "https://www.topstarltd.com/lang-cn/product.html"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/æ‹“æ–¯è¾¾"
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "èµ„æ–™ä¸‹è½½")
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "æ‹“æ–¯è¾¾")
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "èµ„æ–™ä¸‹è½½")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.download_dir, exist_ok=True)
        
        # é’‰é’‰é…ç½®ï¼ˆå†…ç½®ï¼‰
        self.dingtalk_config = {
            "access_token": "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24",
            "secret": "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb",
            "webhook_url": "https://oapi.dingtalk.com/robot/send?access_token=1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        }
        
        # åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•
        self.processed_files = self.load_processed_files()
        self.new_files = []
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_files.pkl'))
    
    def load_processed_files(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_files(self):
        """ä¿å­˜å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_files, f)
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥ï¼ˆæ”¯æŒåŠ å¯†ç­¾åï¼‰"""
        if not self.dingtalk_config or not self.dingtalk_config.get('webhook_url'):
            self.log("âš ï¸ é’‰é’‰é…ç½®æœªè®¾ç½®ï¼Œè·³è¿‡é€šçŸ¥")
            return
        
        try:
            # è·å–é…ç½®
            access_token = self.dingtalk_config.get('access_token', '')
            secret = self.dingtalk_config.get('secret', '')
            webhook_url = self.dingtalk_config.get('webhook_url', '')
            
            # å¦‚æœæœ‰secretï¼Œç”Ÿæˆç­¾å
            if secret:
                timestamp = str(round(time.time() * 1000))
                string_to_sign = f'{timestamp}\n{secret}'
                hmac_code = hmac.new(
                    secret.encode('utf-8'),
                    string_to_sign.encode('utf-8'),
                    digestmod=hashlib.sha256
                ).digest()
                sign = quote_plus(base64.b64encode(hmac_code))
                webhook_url = f"{webhook_url}&timestamp={timestamp}&sign={sign}"
            
            # æ„å»ºæ¶ˆæ¯
            data = {
                "msgtype": "text",
                "text": {
                    "content": f"ğŸ¤– æ‹“æ–¯è¾¾çˆ¬è™«é€šçŸ¥\n{message}"
                }
            }
            
            # å‘é€é€šçŸ¥
            response = requests.post(
                webhook_url,
                json=data,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    self.log("âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
                else:
                    self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {result.get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
            else:
                self.log(f"âŒ é’‰é’‰é€šçŸ¥HTTPé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¼‚å¸¸: {str(e)}")
    
    def get_page(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 502 and attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(3 * (attempt + 1))
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
        return None
    
    def parse_main_page(self):
        """è§£æä¸»é¡µé¢ï¼Œè·å–æ‰€æœ‰äº§å“å’ŒæŠ€æœ¯æ¨¡å—é“¾æ¥"""
        print("æ­£åœ¨è§£æä¸»é¡µé¢...")
        html = self.get_page(self.main_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        modules = []
        
        # æŸ¥æ‰¾äº§å“å’ŒæŠ€æœ¯æ¨¡å—
        # æ ¹æ®æä¾›çš„HTMLç»“æ„ï¼ŒæŸ¥æ‰¾ class="menuli active" ä¸‹çš„äº§å“å’ŒæŠ€æœ¯æ¨¡å—
        menu_li = soup.find('li', class_='menuli active')
        if menu_li:
            # åœ¨ toggle ä¸‹çš„ ul ä¸­æŸ¥æ‰¾æ‰€æœ‰æ¨¡å—
            toggle_div = menu_li.find('div', class_='toggle')
            if toggle_div:
                # è·å–æ‰€æœ‰ä¸»è¦æ¨¡å—ï¼ˆbox1çº§åˆ«ï¼‰
                main_modules = toggle_div.find_all('li')
                
                for main_module in main_modules:
                    box1 = main_module.find('div', class_='box1')
                    if box1:
                        # è·å–ä¸»æ¨¡å—ä¿¡æ¯
                        main_link = box1.find('a')
                        if main_link:
                            main_name = main_link.get_text(strip=True)
                            main_url = main_link.get('href', '')
                            
                            if main_url and not main_url.startswith('http'):
                                main_url = urljoin(self.base_url, main_url)
                            
                            print(f"æ‰¾åˆ°ä¸»æ¨¡å—: {main_name} - {main_url}")
                            
                            # æŸ¥æ‰¾å­æ¨¡å—
                            moretoggle = main_module.find('div', class_='moretoggle')
                            if moretoggle:
                                # æŸ¥æ‰¾æ‰€æœ‰ä¸‰çº§æ¨¡å—ï¼ˆbox3çº§åˆ«ï¼‰
                                box3_divs = moretoggle.find_all('div', class_='box3')
                                
                                for box3 in box3_divs:
                                    sub_link = box3.find('a')
                                    if sub_link:
                                        sub_name = sub_link.get_text(strip=True)
                                        sub_url = sub_link.get('href', '')
                                        
                                        # è¿‡æ»¤æ‰ javascript:void(0) é“¾æ¥
                                        if sub_url and sub_url != 'javascript:void(0)':
                                            if not sub_url.startswith('http'):
                                                sub_url = urljoin(self.base_url, sub_url)
                                            
                                            modules.append({
                                                'name': sub_name,
                                                'url': sub_url,
                                                'type': 'äº§å“æ¨¡å—',
                                                'parent': main_name
                                            })
                                            print(f"  æ‰¾åˆ°å­æ¨¡å—: {sub_name} - {sub_url}")
                            
                            # å¦‚æœæ²¡æœ‰å­æ¨¡å—ï¼Œå°†ä¸»æ¨¡å—ä¹Ÿæ·»åŠ ï¼ˆå¦‚ç§‘ç ”åŠ›é‡ï¼‰
                            if not moretoggle.find_all('div', class_='box3'):
                                # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥çš„box3å­æ¨¡å—
                                direct_box3 = moretoggle.find_all('div', class_='box3')
                                if direct_box3:
                                    for box3 in direct_box3:
                                        sub_link = box3.find('a')
                                        if sub_link:
                                            sub_name = sub_link.get_text(strip=True)
                                            sub_url = sub_link.get('href', '')
                                            
                                            if sub_url and sub_url != 'javascript:void(0)':
                                                if not sub_url.startswith('http'):
                                                    sub_url = urljoin(self.base_url, sub_url)
                                                
                                                modules.append({
                                                    'name': sub_name,
                                                    'url': sub_url,
                                                    'type': 'ç§‘ç ”æ¨¡å—',
                                                    'parent': main_name
                                                })
                                                print(f"  æ‰¾åˆ°ç§‘ç ”æ¨¡å—: {sub_name} - {sub_url}")
                                else:
                                    # å¦‚æœç¡®å®æ²¡æœ‰å­æ¨¡å—ï¼Œæ·»åŠ ä¸»æ¨¡å—æœ¬èº«
                                    if main_url and main_url != 'javascript:void(0)':
                                        modules.append({
                                            'name': main_name,
                                            'url': main_url,
                                            'type': 'ä¸»æ¨¡å—',
                                            'parent': 'äº§å“å’ŒæŠ€æœ¯'
                                        })
        
        return modules
    
    def parse_module_page(self, module_info):
        """è§£æå…·ä½“æ¨¡å—é¡µé¢ï¼Œæå–äº§å“åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
        print(f"\næ­£åœ¨è§£ææ¨¡å—: {module_info['name']}")
        
        all_products = []
        page = 1
        
        while True:
            # æ„å»ºåˆ†é¡µURL
            if page == 1:
                page_url = module_info['url']
            else:
                # æ·»åŠ åˆ†é¡µå‚æ•°
                if '?' in module_info['url']:
                    page_url = f"{module_info['url']}&page={page}"
                else:
                    page_url = f"{module_info['url']}?page={page}"
            
            print(f"  æ­£åœ¨è§£æç¬¬ {page} é¡µ: {page_url}")
            
            html = self.get_page(page_url)
            if not html:
                break
            
            soup = BeautifulSoup(html, 'html.parser')
            page_products = []
            
            # æ ¹æ®æä¾›çš„HTMLç»“æ„ï¼ŒæŸ¥æ‰¾äº§å“åˆ—è¡¨
            # æŸ¥æ‰¾ <div class="all list"> ä¸‹çš„äº§å“
            all_list_div = soup.find('div', class_='all list')
            if all_list_div:
                # åœ¨ ul ä¸­æŸ¥æ‰¾æ‰€æœ‰ li äº§å“é¡¹
                product_ul = all_list_div.find('ul')
                if product_ul:
                    product_lis = product_ul.find_all('li', class_='time05')
                    
                    for li in product_lis:
                        # è·å–äº§å“åç§°å’Œé“¾æ¥
                        name_div = li.find('div', class_='name line-one')
                        if name_div:
                            product_link = name_div.find('a')
                            if product_link:
                                product_name = product_link.get_text(strip=True)
                                product_url = product_link.get('href', '')
                                
                                if product_url and not product_url.startswith('http'):
                                    product_url = urljoin(self.base_url, product_url)
                                
                                # è·å–äº§å“æè¿°ä¿¡æ¯
                                desc_info = {}
                                desc_div = li.find('div', class_='desc')
                                if desc_div:
                                    # æå–è´Ÿè½½å’Œè‡‚é•¿ç­‰ä¿¡æ¯
                                    text_divs = desc_div.find_all('div', class_=['text1', 'text2'])
                                    for text_div in text_divs:
                                        font_elem = text_div.find('font')
                                        if font_elem:
                                            text = font_elem.get_text(strip=True)
                                            if 'è´Ÿè½½ï¼š' in text:
                                                desc_info['è´Ÿè½½'] = text
                                            elif 'è‡‚é•¿ï¼š' in text:
                                                desc_info['è‡‚é•¿'] = text
                                
                                # è·å–äº§å“å›¾ç‰‡
                                img_url = ""
                                img_div = li.find('div', class_='img')
                                if img_div:
                                    img_elem = img_div.find('img')
                                    if img_elem:
                                        img_src = img_elem.get('src', '')
                                        if img_src and not img_src.startswith('http'):
                                            img_url = urljoin(self.base_url, img_src)
                                        else:
                                            img_url = img_src
                                
                                product_info = {
                                    'name': product_name,
                                    'url': product_url,
                                    'description': desc_info,
                                    'image_url': img_url,
                                    'module': module_info['name'],
                                    'parent_module': module_info.get('parent', ''),
                                    'page': page  # è®°å½•é¡µç 
                                }
                                page_products.append(product_info)
                                print(f"    æ‰¾åˆ°äº§å“: {product_name}")
            
            # å¦‚æœå½“å‰é¡µæ²¡æœ‰äº§å“ï¼Œè¯´æ˜å·²ç»åˆ°äº†æœ€åä¸€é¡µ
            if not page_products:
                print(f"  ç¬¬ {page} é¡µæ²¡æœ‰äº§å“ï¼Œç»“æŸåˆ†é¡µçˆ¬å–")
                break
            

            
            all_products.extend(page_products)
            print(f"  ç¬¬ {page} é¡µæ‰¾åˆ° {len(page_products)} ä¸ªäº§å“")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯ï¼Œå¦‚æœæœ‰"ä¸‹ä¸€é¡µ"æˆ–è€…æ•°å­—é“¾æ¥ï¼Œç»§ç»­çˆ¬å–
            has_next_page = False
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾åˆ†é¡µå¯¼èˆª
            pagination_divs = soup.find_all('div', class_='fen-page')
            for pagination_div in pagination_divs:
                # æŸ¥æ‰¾åˆ†é¡µé“¾æ¥
                page_links = pagination_div.find_all('a')
                for link in page_links:
                    link_text = link.get_text(strip=True)
                    href = link.get('href', '')
                    # å¦‚æœæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥æˆ–è€…é¡µç å¤§äºå½“å‰é¡µ
                    if ('ä¸‹ä¸€é¡µ' in link_text or '>' in link_text or 
                        (link_text.isdigit() and int(link_text) > page)):
                        has_next_page = True
                        break
                if has_next_page:
                    break
            
            # ç®€åŒ–åˆ†é¡µé€»è¾‘ï¼šç›´æ¥æ ¹æ®æ¨¡å—åç§°å†³å®šé¡µæ•°
            if module_info['name'] == 'æœºæ¢°æ‰‹':
                max_pages = 2
            elif module_info['name'] == 'è¾…æœº':
                max_pages = 3
            else:
                max_pages = 1
            
            # å¦‚æœå½“å‰é¡µè¾¾åˆ°æœ€å¤§é¡µæ•°ï¼Œåœæ­¢çˆ¬å–
            if page >= max_pages:
                has_next_page = False
                print(f"    æ¨¡å— {module_info['name']} å·²è¾¾åˆ°æœ€å¤§é¡µæ•° {max_pages}ï¼Œåœæ­¢çˆ¬å–")
                break
            
            # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœå½“å‰é¡µå°äºæœ€å¤§é¡µæ•°ï¼Œç»§ç»­çˆ¬å–ä¸‹ä¸€é¡µ
            if page < max_pages:
                has_next_page = True
                print(f"    æ¨¡å— {module_info['name']} ç¬¬{page}é¡µå®Œæˆï¼Œç»§ç»­çˆ¬å–ç¬¬{page+1}é¡µ")
            else:
                has_next_page = False
                print(f"    æ¨¡å— {module_info['name']} å·²è¾¾åˆ°æœ€å¤§é¡µæ•° {max_pages}ï¼Œåœæ­¢çˆ¬å–")
            
            if not has_next_page:
                print(f"  æ²¡æœ‰æ›´å¤šé¡µé¢ï¼Œç»“æŸçˆ¬å–")
                break
            
            page += 1
            
            # æ·»åŠ é¡µé¢é—´å»¶è¿Ÿ
            time.sleep(1)
        
        print(f"  æ¨¡å— {module_info['name']} å…±æ‰¾åˆ° {len(all_products)} ä¸ªäº§å“ï¼ˆè·¨ {page} é¡µï¼‰")
        return all_products
    
    def parse_product_page(self, product_info):
        """è§£æäº§å“è¯¦æƒ…é¡µï¼ŒæŸ¥æ‰¾èµ„æ–™ä¸‹è½½é“¾æ¥"""
        print(f"    æ­£åœ¨è§£æäº§å“: {product_info['name']}")
        html = self.get_page(product_info['url'])
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        download_links = []
        
        # æ ¹æ®æä¾›çš„HTMLç»“æ„ï¼ŒæŸ¥æ‰¾èµ„æ–™ä¸‹è½½åŒºåŸŸ
        # æŸ¥æ‰¾ <div class="all provdown"> ä¸‹çš„èµ„æ–™ä¸‹è½½
        provdown_div = soup.find('div', class_='all provdown')
        if provdown_div:
            # æŸ¥æ‰¾èµ„æ–™ä¸‹è½½æ ‡é¢˜ç¡®è®¤
            title_div = provdown_div.find('div', class_='nytitle')
            if title_div and 'èµ„æ–™ä¸‹è½½' in title_div.get_text():
                print(f"      æ‰¾åˆ°èµ„æ–™ä¸‹è½½åŒºåŸŸ")
                
                # åœ¨ desc çš„ ul ä¸­æŸ¥æ‰¾æ‰€æœ‰ä¸‹è½½é¡¹
                desc_div = provdown_div.find('div', class_='desc')
                if desc_div:
                    download_ul = desc_div.find('ul')
                    if download_ul:
                        download_lis = download_ul.find_all('li', class_='time05')
                        
                        for li in download_lis:
                            download_link = li.find('a')
                            if download_link:
                                # è·å–ä¸‹è½½URL
                                download_url = download_link.get('href', '')
                                if download_url and not download_url.startswith('http'):
                                    download_url = urljoin(self.base_url, download_url)
                                
                                # è·å–èµ„æ–™åç§°
                                data_download = download_link.get('data-download', '')
                                if not data_download:
                                    # å¦‚æœæ²¡æœ‰data-downloadå±æ€§ï¼Œä»pæ ‡ç­¾è·å–
                                    p_elem = download_link.find('p')
                                    if p_elem:
                                        data_download = p_elem.get_text(strip=True)
                                
                                if download_url and data_download:
                                    # ç”Ÿæˆæ–‡ä»¶å
                                    filename = data_download
                                    if not filename.lower().endswith('.pdf'):
                                        filename += '.pdf'
                                    
                                    download_links.append({
                                        'url': download_url,
                                        'title': filename,
                                        'original_name': data_download,
                                        'product_name': product_info['name'],
                                        'module': product_info['module'],
                                        'parent_module': product_info.get('parent_module', ''),
                                        'is_material': True  # æ ‡è®°ä¸ºäº§å“èµ„æ–™æ–‡ä»¶
                                    })
                                    print(f"      æ‰¾åˆ°èµ„æ–™: {data_download} - {download_url}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸“é—¨çš„èµ„æ–™ä¸‹è½½åŒºåŸŸï¼ŒæŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ä¸‹è½½é“¾æ¥
        if not download_links:
            print(f"      æœªæ‰¾åˆ°ä¸“é—¨çš„èµ„æ–™ä¸‹è½½åŒºåŸŸï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–ä¸‹è½½é“¾æ¥...")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ä¸‹è½½é“¾æ¥
            all_links = soup.find_all('a', href=True)
            download_keywords = ['ä¸‹è½½', 'èµ„æ–™', 'æ‰‹å†Œ', 'æ–‡æ¡£', 'pdf', 'doc', 'download', 'manual']
            
            for link in all_links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True).lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡æ¡£ä¸‹è½½é“¾æ¥
                is_download_link = any(keyword in link_text or keyword in href.lower() 
                                     for keyword in download_keywords)
                
                if is_download_link and href:
                    if not href.startswith('http'):
                        href = urljoin(product_info['url'], href)
                    
                    # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸æ˜¯æ–‡ä»¶çš„é“¾æ¥
                    if not any(skip in href.lower() for skip in ['javascript:', 'mailto:', '#']):
                        title = link.get_text(strip=True) or "äº§å“èµ„æ–™"
                        
                        download_links.append({
                            'url': href,
                            'title': title,
                            'original_name': title,
                            'product_name': product_info['name'],
                            'module': product_info['module'],
                            'parent_module': product_info.get('parent_module', ''),
                            'is_material': False
                        })
                        print(f"      æ‰¾åˆ°å¤‡ç”¨ä¸‹è½½é“¾æ¥: {title} - {href}")
        
        return download_links
    
    def download_file(self, download_info):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            url = download_info['url']
            filename = download_info['title']
            
            # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
            filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
            
            # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼ï¼Œè®¾ç½®é»˜è®¤åç§°
            if not filename or not filename.strip():
                filename = "äº§å“èµ„æ–™"
            
            # ç¡®ä¿æ–‡ä»¶æœ‰æ‰©å±•å
            if '.' not in filename.split('/')[-1]:
                # å°è¯•ä»URLè·å–æ‰©å±•å
                parsed_url = urlparse(url)
                path = parsed_url.path
                if '.' in path:
                    ext = path.split('.')[-1].lower()
                    if ext in ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'zip', 'rar']:
                        filename += f'.{ext}'
                else:
                    # é»˜è®¤ä¸ºPDF
                    filename += '.pdf'
            
            # æ¸…ç†ç›®å½•åä¸­çš„éæ³•å­—ç¬¦
            parent_module = re.sub(r'[<>:"/\\|?*]', '_', download_info.get('parent_module', 'å…¶ä»–'))
            module_name = re.sub(r'[<>:"/\\|?*]', '_', download_info['module'])
            product_name = re.sub(r'[<>:"/\\|?*]', '_', download_info['product_name'])
            
            # åˆ›å»ºç›®å½•ç»“æ„: çˆ¶æ¨¡å—/å­æ¨¡å—/äº§å“å/èµ„æ–™
            parent_dir = os.path.join(self.download_dir, parent_module)
            if not os.path.exists(parent_dir):
                os.makedirs(parent_dir)
            
            module_dir = os.path.join(parent_dir, module_name)
            if not os.path.exists(module_dir):
                os.makedirs(module_dir)
            
            product_dir = os.path.join(module_dir, product_name)
            if not os.path.exists(product_dir):
                os.makedirs(product_dir)
            
            # æ‰€æœ‰æ–‡ä»¶éƒ½æ”¾åœ¨äº§å“ç›®å½•ä¸‹çš„èµ„æ–™æ–‡ä»¶å¤¹
            material_dir = os.path.join(product_dir, "äº§å“èµ„æ–™")
            if not os.path.exists(material_dir):
                os.makedirs(material_dir)
            
            # å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            filepath = os.path.join(material_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            if os.path.exists(filepath):
                print(f"        æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return True
            
            print(f"        æ­£åœ¨ä¸‹è½½: {filename}")
            
            # ä½¿ç”¨requestsä¸‹è½½æ–‡ä»¶
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' in content_type:
                print(f"        è­¦å‘Š: ä¸‹è½½çš„å¯èƒ½ä¸æ˜¯æ–‡ä»¶ï¼Œè€Œæ˜¯HTMLé¡µé¢")
                return False
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filepath)
            if file_size < 1024:  # å°äº1KBå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                print(f"        è­¦å‘Š: æ–‡ä»¶å¤§å°å¼‚å¸¸ ({file_size} bytes)")
            
            print(f"        ä¸‹è½½å®Œæˆ: {filename} ({file_size} bytes)")
            
            # è®°å½•æ–°æ–‡ä»¶
            file_key = f"{download_info['module']}_{download_info['product_name']}_{filename}"
            if file_key not in self.processed_files:
                self.new_files.append({
                    'filename': filename,
                    'path': filepath,
                    'url': download_info['url'],
                    'size': file_size,
                    'module': download_info['module'],
                    'product': download_info['product_name'],
                    'parent_module': download_info.get('parent_module', '')
                })
                self.processed_files.add(file_key)
            
            return True
            
        except Exception as e:
            print(f"        ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def download_materials(self, products):
        """ä¸‹è½½æ‰€æœ‰äº§å“çš„èµ„æ–™"""
        if not products:
            print("æ²¡æœ‰äº§å“éœ€è¦ä¸‹è½½èµ„æ–™")
            return
        
        print(f"\nå¼€å§‹ä¸‹è½½äº§å“èµ„æ–™...")
        print(f"å…±æœ‰ {len(products)} ä¸ªäº§å“éœ€è¦å¤„ç†")
        
        total_downloads = 0
        successful_downloads = 0
        
        for i, product in enumerate(products, 1):
            print(f"\nè¿›åº¦: {i}/{len(products)} - {product['module']} - {product['name']}")
            
            # è§£æäº§å“é¡µé¢ï¼ŒæŸ¥æ‰¾ä¸‹è½½é“¾æ¥
            download_links = self.parse_product_page(product)
            
            if download_links:
                print(f"    æ‰¾åˆ° {len(download_links)} ä¸ªä¸‹è½½é“¾æ¥")
                
                # ä¸‹è½½æ¯ä¸ªæ–‡ä»¶
                for download_info in download_links:
                    total_downloads += 1
                    if self.download_file(download_info):
                        successful_downloads += 1
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)
            else:
                print(f"    æœªæ‰¾åˆ°ä¸‹è½½é“¾æ¥")
            
            # äº§å“é—´å»¶è¿Ÿ
            time.sleep(2)
        
        print(f"\näº§å“èµ„æ–™ä¸‹è½½å®Œæˆï¼")
        print(f"æ€»å°è¯•ä¸‹è½½: {total_downloads} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸä¸‹è½½: {successful_downloads} ä¸ªæ–‡ä»¶")
        print(f"å¤±è´¥: {total_downloads - successful_downloads} ä¸ªæ–‡ä»¶")
    
    def save_data(self, data, filename):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        filepath = os.path.join(self.output_dir, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.log("ğŸš€ å¼€å§‹çˆ¬å–æ‹“æ–¯è¾¾äº§å“ä¸­å¿ƒ...")
        
        # 1. è·å–æ‰€æœ‰æ¨¡å—
        modules = self.parse_main_page()
        if not modules:
            self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å—")
            return
        
        self.log(f"ğŸ“‹ å…±æ‰¾åˆ° {len(modules)} ä¸ªæ¨¡å—")
        
        # 2. ä¿å­˜æ¨¡å—ä¿¡æ¯
        self.save_data(modules, 'modules.json')
        
        # 3. çˆ¬å–æ¯ä¸ªæ¨¡å—çš„äº§å“
        all_products = []
        for i, module in enumerate(modules, 1):
            self.log(f"ğŸ”„ è¿›åº¦: {i}/{len(modules)} - {module['name']}")
            products = self.parse_module_page(module)
            all_products.extend(products)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        # 4. ä¿å­˜æ‰€æœ‰äº§å“ä¿¡æ¯
        if all_products:
            self.save_data(all_products, 'products.json')
            self.log(f"âœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_products)} ä¸ªäº§å“")
        else:
            self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•äº§å“ä¿¡æ¯")
            return
        
        # 5. ä¸‹è½½äº§å“èµ„æ–™
        self.download_materials(all_products)
        
        # 6. ä¿å­˜å¤„ç†è®°å½•
        self.save_processed_files()
        
        # 7. å‘é€é’‰é’‰é€šçŸ¥
        self.send_completion_notification()
        
        # 8. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(modules, all_products)
    
    def send_completion_notification(self):
        """å‘é€å®Œæˆé€šçŸ¥"""
        if not self.new_files:
            if not self.is_first_run:
                self.log("ğŸ“¢ æ— æ–°æ–‡ä»¶ï¼Œä¸å‘é€é€šçŸ¥")
            return
        
        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
        message_parts = []
        message_parts.append(f"ğŸ“Š æ‹“æ–¯è¾¾çˆ¬è™«å®Œæˆ")
        message_parts.append(f"ğŸ•’ æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        message_parts.append(f"ğŸ“ æ–°ä¸‹è½½æ–‡ä»¶: {len(self.new_files)} ä¸ª")
        
        if self.is_first_run:
            message_parts.append("ğŸ†• é¦–æ¬¡è¿è¡Œï¼Œå·²å»ºç«‹åŸºçº¿")
        
        # æŒ‰æ¨¡å—åˆ†ç»„æ˜¾ç¤ºæ–°æ–‡ä»¶
        module_files = {}
        for file_info in self.new_files:
            module = file_info['module']
            if module not in module_files:
                module_files[module] = []
            module_files[module].append(file_info)
        
        message_parts.append("\nğŸ“‹ æ–°æ–‡ä»¶è¯¦æƒ…:")
        for module, files in module_files.items():
            message_parts.append(f"  ğŸ“‚ {module}: {len(files)} ä¸ªæ–‡ä»¶")
            for file_info in files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                size_mb = file_info['size'] / 1024 / 1024
                message_parts.append(f"    ğŸ“„ {file_info['filename']} ({size_mb:.1f}MB)")
            if len(files) > 3:
                message_parts.append(f"    ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")
        
        message = "\n".join(message_parts)
        self.send_dingtalk_notification(message)
    
    def generate_report(self, modules, products):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            'çˆ¬å–æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S'),
            'æ€»æ¨¡å—æ•°': len(modules),
            'æ€»äº§å“æ•°': len(products),
            'æ¨¡å—åˆ—è¡¨': [m['name'] for m in modules],
            'å„æ¨¡å—äº§å“æ•°é‡': {},
            'åˆ†é¡µç»Ÿè®¡': {}
        }
        
        # ç»Ÿè®¡å„æ¨¡å—äº§å“æ•°é‡å’Œåˆ†é¡µä¿¡æ¯
        for module in modules:
            module_products = [p for p in products if p['module'] == module['name']]
            report['å„æ¨¡å—äº§å“æ•°é‡'][module['name']] = len(module_products)
            
            # ç»Ÿè®¡åˆ†é¡µä¿¡æ¯
            if module_products:
                max_page = max([p.get('page', 1) for p in module_products])
                page_counts = {}
                for page_num in range(1, max_page + 1):
                    page_products = [p for p in module_products if p.get('page', 1) == page_num]
                    if page_products:
                        page_counts[f'ç¬¬{page_num}é¡µ'] = len(page_products)
                
                if max_page > 1:
                    report['åˆ†é¡µç»Ÿè®¡'][module['name']] = {
                        'æ€»é¡µæ•°': max_page,
                        'å„é¡µäº§å“æ•°': page_counts
                    }
        
        # ä¿å­˜æŠ¥å‘Š
        self.save_data(report, 'çˆ¬å–æŠ¥å‘Š.json')
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "="*50)
        print("çˆ¬å–æŠ¥å‘Š")
        print("="*50)
        print(f"çˆ¬å–æ—¶é—´: {report['çˆ¬å–æ—¶é—´']}")
        print(f"æ€»æ¨¡å—æ•°: {report['æ€»æ¨¡å—æ•°']}")
        print(f"æ€»äº§å“æ•°: {report['æ€»äº§å“æ•°']}")
        print("\nå„æ¨¡å—äº§å“æ•°é‡:")
        for module_name, count in report['å„æ¨¡å—äº§å“æ•°é‡'].items():
            print(f"  {module_name}: {count} ä¸ª")
            
            # æ˜¾ç¤ºåˆ†é¡µä¿¡æ¯
            if module_name in report['åˆ†é¡µç»Ÿè®¡']:
                page_info = report['åˆ†é¡µç»Ÿè®¡'][module_name]
                print(f"    â””â”€ åˆ†é¡µæƒ…å†µ: å…±{page_info['æ€»é¡µæ•°']}é¡µ")
                for page, count in page_info['å„é¡µäº§å“æ•°'].items():
                    print(f"       {page}: {count}ä¸ªäº§å“")
        
        print("="*50)
    
    def download_materials_only(self):
        """åªä¸‹è½½äº§å“èµ„æ–™ï¼Œä¸é‡æ–°çˆ¬å–äº§å“ä¿¡æ¯"""
        print("å¼€å§‹ä¸‹è½½äº§å“èµ„æ–™ï¼ˆä½¿ç”¨ç°æœ‰äº§å“æ•°æ®ï¼‰...")
        
        # åŠ è½½ç°æœ‰äº§å“æ•°æ®
        products_file = os.path.join(self.output_dir, 'products.json')
        if not os.path.exists(products_file):
            print("æœªæ‰¾åˆ°äº§å“æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´çˆ¬å–")
            return
        
        try:
            with open(products_file, 'r', encoding='utf-8') as f:
                products = json.load(f)
            
            print(f"åŠ è½½äº† {len(products)} ä¸ªäº§å“ä¿¡æ¯")
            self.download_materials(products)
            
        except Exception as e:
            print(f"åŠ è½½äº§å“æ•°æ®å¤±è´¥: {e}")

if __name__ == "__main__":
    spider = TopstarSpider()
    try:
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        import sys
        if len(sys.argv) > 1 and sys.argv[1] == '--download-only':
            spider.download_materials_only()
        else:
            spider.run()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
