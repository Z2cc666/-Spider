#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›ä¼˜äº§å“ä¸­å¿ƒçˆ¬è™«
çˆ¬å–äº§å“ä¸­å¿ƒä¸‹é¢çš„æ‰€æœ‰äº§å“ä¿¡æ¯
æ”¯æŒé’‰é’‰é€šçŸ¥å’Œè‡ªåŠ¨æ£€æµ‹æ–°æ–‡ä»¶
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
import pickle
import hmac
import hashlib
import base64
import argparse
from datetime import datetime
from urllib.parse import urljoin, urlparse, quote_plus
import re
import urllib.request
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class ChuangyouSpider:
    def __init__(self, limit=None, categories=None, no_webdriver=False, skip_download=False):
        self.base_url = "https://www.cuhnj.com"
        self.main_url = "https://www.cuhnj.com/href/html/prodXl"
        self.limit = limit
        self.categories = categories
        self.no_webdriver = no_webdriver
        self.skip_download = skip_download
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/åˆ›ä¼˜"
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "äº§å“èµ„æ–™ä¸‹è½½")
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "åˆ›ä¼˜")
            self.output_dir = os.path.join(self.base_dir, "äº§å“æ•°æ®")
            self.download_dir = os.path.join(self.base_dir, "äº§å“èµ„æ–™ä¸‹è½½")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.download_dir, exist_ok=True)
        
        # é’‰é’‰é…ç½®ï¼ˆå†…ç½®ï¼‰
        self.dingtalk_config = {
            "access_token": "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24",
            "secret": "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb",
            "webhook_url": "https://oapi.dingtalk.com/robot/send?access_token=1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        }
        
        # åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•
        self.processed_files = self.load_processed_files()
        self.new_files = []
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_files.pkl'))
        
        # åˆå§‹åŒ–Chrome WebDriver
        self.driver = None
        self.init_webdriver()
    
    def init_webdriver(self):
        """åˆå§‹åŒ–Chrome WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            # æ£€æŸ¥å¯ç”¨çš„ChromeDriverè·¯å¾„
            chromedriver_paths = [
                '/Users/z2cc/ä¼¯æœ—ç‰¹/chromedriver_downloads/chromedriver_mac-arm64/chromedriver-mac-arm64/chromedriver',
                '/Users/z2cc/ä¼¯æœ—ç‰¹/chromedriver_downloads/chromedriver_mac-x64/chromedriver-mac-x64/chromedriver',
                'chromedriver'  # ç³»ç»ŸPATHä¸­çš„chromedriver
            ]
            
            driver_path = None
            for path in chromedriver_paths:
                if os.path.exists(path):
                    driver_path = path
                    break
            
            if driver_path:
                service = Service(driver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                self.log("âœ… Chrome WebDriver åˆå§‹åŒ–æˆåŠŸ")
            else:
                # å°è¯•ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„chromedriver
                try:
                    self.driver = webdriver.Chrome(options=chrome_options)
                    self.log("âœ… Chrome WebDriver åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨ç³»ç»ŸPATHï¼‰")
                except Exception:
                    self.log("âš ï¸ æœªæ‰¾åˆ°ChromeDriverï¼Œå°†ä½¿ç”¨requestsæ¨¡å¼")
                
        except Exception as e:
            self.log(f"âš ï¸ Chrome WebDriver åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨requestsæ¨¡å¼")
            self.driver = None
    
    def close_webdriver(self):
        """å…³é—­WebDriver"""
        if self.driver:
            try:
                self.driver.quit()
                self.log("âœ… WebDriver å·²å…³é—­")
            except Exception as e:
                self.log(f"âš ï¸ å…³é—­WebDriveræ—¶å‡ºé”™: {e}")
    
    def load_processed_files(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_files(self):
        """ä¿å­˜å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_file = os.path.join(self.base_dir, 'processed_files.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_files, f)
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥ï¼ˆæ”¯æŒåŠ å¯†ç­¾åï¼‰"""
        if not self.dingtalk_config or not self.dingtalk_config.get('webhook_url'):
            self.log("âš ï¸ é’‰é’‰é…ç½®æœªè®¾ç½®ï¼Œè·³è¿‡é€šçŸ¥")
            return
        
        try:
            # è·å–é…ç½®
            access_token = self.dingtalk_config.get('access_token', '')
            secret = self.dingtalk_config.get('secret', '')
            webhook_url = self.dingtalk_config.get('webhook_url', '')
            
            # å¦‚æœæœ‰secretï¼Œç”Ÿæˆç­¾å
            if secret:
                timestamp = str(round(time.time() * 1000))
                string_to_sign = f'{timestamp}\n{secret}'
                hmac_code = hmac.new(
                    secret.encode('utf-8'),
                    string_to_sign.encode('utf-8'),
                    digestmod=hashlib.sha256
                ).digest()
                sign = quote_plus(base64.b64encode(hmac_code))
                webhook_url = f"{webhook_url}&timestamp={timestamp}&sign={sign}"
            
            # æ„å»ºæ¶ˆæ¯
            data = {
                "msgtype": "text",
                "text": {
                    "content": f"ğŸ¤– åˆ›ä¼˜çˆ¬è™«é€šçŸ¥\n{message}"
                }
            }
            
            # å‘é€é€šçŸ¥
            response = requests.post(
                webhook_url,
                json=data,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    self.log("âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
                else:
                    self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {result.get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
            else:
                self.log(f"âŒ é’‰é’‰é€šçŸ¥HTTPé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¼‚å¸¸: {str(e)}")
    
    def get_page(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 502 and attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e} (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(3 * (attempt + 1))
                    continue
                else:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None
        return None
    
    def parse_main_page(self):
        """è§£æä¸»é¡µé¢ï¼Œè·å–æ‰€æœ‰äº§å“æ¨¡å—é“¾æ¥"""
        print("æ­£åœ¨è§£æä¸»é¡µé¢...")
        html = self.get_page(self.main_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        modules = []
        
        # æŸ¥æ‰¾äº§å“ä¸­å¿ƒæ¨¡å—
        # æ ¹æ®æä¾›çš„HTMLç»“æ„ï¼ŒæŸ¥æ‰¾äº§å“ä¸­å¿ƒå¯¼èˆªé¡¹
        nav_items = soup.find_all('li', class_='nav-item clickLi')
        product_center_nav = None
        
        for nav_item in nav_items:
            nav_link = nav_item.find('a')
            if nav_link and 'äº§å“ä¸­å¿ƒ' in nav_link.get_text(strip=True):
                product_center_nav = nav_item
                break
        
        if product_center_nav:
            print("æ‰¾åˆ°äº§å“ä¸­å¿ƒå¯¼èˆªé¡¹")
            # åœ¨ submenu ä¸‹æŸ¥æ‰¾æ‰€æœ‰äº§å“åˆ†ç±»
            submenu = product_center_nav.find('div', class_='submenu submenu2')
            if submenu:
                # è·å–æ‰€æœ‰äº§å“åˆ†ç±»ï¼ˆgzy-herd-liï¼‰
                category_lis = submenu.find_all('li', class_='gzy-herd-li')
                print(f"æ‰¾åˆ° {len(category_lis)} ä¸ªäº§å“åˆ†ç±»")
                
                for i, category_li in enumerate(category_lis, 1):
                    print(f"å¤„ç†ç¬¬ {i} ä¸ªäº§å“åˆ†ç±»...")
                    
                    # è·å–åˆ†ç±»å›¾ç‰‡å’Œé“¾æ¥
                    prod_slt = category_li.find('div', class_='prod-slt')
                    category_url = ""
                    category_img_url = ""
                    category_name = f"åˆ†ç±»{i}"
                    
                    if prod_slt:
                        category_link = prod_slt.find('a')
                        if category_link:
                            category_url = category_link.get('href', '')
                            if category_url and not category_url.startswith('http'):
                                category_url = urljoin(self.base_url, category_url)
                            
                            # è·å–åˆ†ç±»å›¾ç‰‡
                            category_img = category_link.find('img')
                            if category_img:
                                category_img_url = category_img.get('src', '')
                                if category_img_url and not category_img_url.startswith('http'):
                                    category_img_url = urljoin(self.base_url, category_img_url)
                    
                    # è·å–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰äº§å“
                    xh_ul = category_li.find('div', class_='xh-ul left-div')
                    if xh_ul:
                        product_links = xh_ul.find_all('a', class_='gzy_product_top')
                        print(f"  åˆ†ç±»{i}ä¸‹æ‰¾åˆ° {len(product_links)} ä¸ªäº§å“")
                        
                        for product_link in product_links:
                            product_name = product_link.get_text(strip=True)
                            product_url = product_link.get('href', '')
                            
                            if product_url and not product_url.startswith('http'):
                                product_url = urljoin(self.base_url, product_url)
                            
                            if product_name and product_url:
                                modules.append({
                                    'name': product_name,
                                    'url': product_url,
                                    'type': 'äº§å“',
                                    'category': category_name,
                                    'category_url': category_url,
                                    'category_image': category_img_url
                                })
                                print(f"  æ‰¾åˆ°äº§å“: {product_name}")
            else:
                print("æœªæ‰¾åˆ°äº§å“ä¸­å¿ƒçš„submenuç»“æ„")
        else:
            print("æœªæ‰¾åˆ°äº§å“ä¸­å¿ƒå¯¼èˆªé¡¹")
        
        return modules
    
    def parse_product_page(self, product_info):
        """è§£æäº§å“è¯¦æƒ…é¡µï¼Œæå–æ‰€æœ‰å†…å®¹ï¼Œæ¯è§£æä¸€ä¸ªæ¨¡å—å°±ç«‹å³ä¸‹è½½"""
        print(f"    æ­£åœ¨è§£æäº§å“: {product_info['name']}")
        
        product_data = {
            'basic_info': {},
            'content_sections': {},
            'download_links': [],
            'downloaded_files': []
        }
        
        # å¦‚æœæœ‰WebDriverï¼Œä½¿ç”¨Seleniumè·å–åŠ¨æ€å†…å®¹
        if self.driver:
            try:
                self.driver.get(product_info['url'])
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "shop-went"))
                )
                
                # ç­‰å¾…Vueæ¸²æŸ“å®Œæˆ
                time.sleep(3)
                
                # è·å–æ¸²æŸ“åçš„HTML
                html = self.driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
            except Exception as e:
                print(f"    WebDriverè·å–é¡µé¢å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨requests")
                html = self.get_page(product_info['url'])
                if not html:
                    return {}
                soup = BeautifulSoup(html, 'html.parser')
        else:
            # ä½¿ç”¨requestsè·å–é™æ€HTML
            html = self.get_page(product_info['url'])
            if not html:
                return {}
            soup = BeautifulSoup(html, 'html.parser')
        
        # 1. æå–åŸºæœ¬ä¿¡æ¯
        shop_went = soup.find('div', class_='shop-went')
        if shop_went:
            # äº§å“æ ‡é¢˜
            h2_elem = shop_went.find('h2')
            if h2_elem:
                spans = h2_elem.find_all('span')
                if len(spans) >= 2:
                    product_data['basic_info']['model'] = spans[0].get_text(strip=True)
                    product_data['basic_info']['title'] = spans[1].get_text(strip=True)
                elif len(spans) == 1:
                    # å¦‚æœåªæœ‰ä¸€ä¸ªspanï¼Œå¯èƒ½æ˜¯å®Œæ•´çš„æ ‡é¢˜
                    product_data['basic_info']['title'] = spans[0].get_text(strip=True)
            
            # æ‰€å±åˆ†ç±» - æŸ¥æ‰¾åŒ…å«"æ‰€å±åˆ†ç±»ï¼š"çš„å…ƒç´ 
            category_b = shop_went.find('b', string=lambda text: text and 'æ‰€å±åˆ†ç±»ï¼š' in text if text else False)
            if not category_b:
                # å°è¯•æŸ¥æ‰¾æ–‡æœ¬èŠ‚ç‚¹
                category_elements = shop_went.find_all(string=lambda text: text and 'æ‰€å±åˆ†ç±»ï¼š' in text)
                if category_elements:
                    # æ‰¾åˆ°åŒ…å«"æ‰€å±åˆ†ç±»ï¼š"çš„æ–‡æœ¬åï¼ŒæŸ¥æ‰¾å…¶åçš„spanå…ƒç´ 
                    for elem in category_elements:
                        parent = elem.parent
                        if parent:
                            category_span = parent.find('span')
                            if category_span:
                                product_data['basic_info']['category'] = category_span.get_text(strip=True)
                                break
            else:
                # ç›´æ¥ä»bæ ‡ç­¾åæ‰¾span
                category_span = category_b.find('span')
                if category_span:
                    product_data['basic_info']['category'] = category_span.get_text(strip=True)
            
            # æ¦‚è¦ä¿¡æ¯ - æŸ¥æ‰¾åŒ…å«"æ¦‚è¦ä¿¡æ¯ï¼š"çš„å…ƒç´ 
            summary_p = shop_went.find('p', class_='p-gy')
            if summary_p:
                # æŸ¥æ‰¾pæ ‡ç­¾å†…çš„spanå…ƒç´ 
                summary_span = summary_p.find('span')
                if summary_span:
                    product_data['basic_info']['summary'] = summary_span.get_text(strip=True)
            else:
                # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æ–‡æœ¬èŠ‚ç‚¹
                summary_elements = shop_went.find_all(string=lambda text: text and 'æ¦‚è¦ä¿¡æ¯ï¼š' in text)
                if summary_elements:
                    summary_text = summary_elements[0]
                    product_data['basic_info']['summary'] = summary_text.replace('æ¦‚è¦ä¿¡æ¯ï¼š', '').strip()
        
        # 2. é€ä¸ªè§£ææ ‡ç­¾é¡µå†…å®¹ï¼Œæ¯è§£æä¸€ä¸ªå°±ä¸‹è½½
        tabs = {
            'tab1': 'äº§å“æè¿°',
            'tab2': 'æŠ€æœ¯è§„æ ¼', 
            'tab3': 'äº§å“èµ„æ–™',
            'tab4': 'è§†é¢‘',
            'tab5': 'åº”ç”¨',
            'tab6': 'æŠ¥é”™ä¿¡æ¯'
        }
        
        for tab_id, tab_name in tabs.items():
            print(f"      è§£ææ¨¡å—: {tab_name}")
            
            # åº”ç”¨æ¨¡å—ç‰¹æ®Šå¤„ç†
            if tab_id == 'tab5' and tab_name == 'åº”ç”¨':
                content = self.extract_application_content_simple(soup, product_info, product_data)
                if content:
                    product_data['content_sections'][tab_name] = content
                    print(f"      æ¨¡å— {tab_name} è§£æå®Œæˆ")
                else:
                    print(f"      æ¨¡å— {tab_name} æ— å†…å®¹")
            else:
                tab_div = soup.find('div', id=tab_id)
                if tab_div:
                    # æå–å†…å®¹å¹¶ç«‹å³ä¸‹è½½
                    content = self.extract_tab_content_with_download(tab_div, tab_name, product_info, product_data)
                    if content:
                        product_data['content_sections'][tab_name] = content
                        print(f"      æ¨¡å— {tab_name} è§£æå®Œæˆ")
                    else:
                        print(f"      æ¨¡å— {tab_name} æ— å†…å®¹")
                else:
                    print(f"      æ¨¡å— {tab_name} æœªæ‰¾åˆ°")
        
        return product_data
    
    def extract_application_content_simple(self, soup, product_info, product_data):
        """ç®€å•æ–¹å¼æå–åº”ç”¨æ¨¡å—å†…å®¹ï¼Œä¸ä½¿ç”¨Selenium"""
        try:
            # æŸ¥æ‰¾åº”ç”¨æ¨¡å—çš„div
            tab5_div = soup.find('div', id='tab5')
            if not tab5_div:
                print("        æœªæ‰¾åˆ°åº”ç”¨æ¨¡å—div")
                return None
            
            content = {
                'text_content': '',
                'images': [],
                'videos': [],
                'download_links': []
            }
            
            # æŸ¥æ‰¾shop-nt shop-ulç»“æ„
            shop_div = tab5_div.find('div', class_=['shop-nt', 'shop-ul'])
            if shop_div:
                ul_tag = shop_div.find('ul')
                if ul_tag:
                    # éå†æ‰€æœ‰liæ ‡ç­¾
                    for li in ul_tag.find_all('li'):
                        link = li.find('a', href=True)
                        if link:
                            href = link.get('href', '')
                            link_text = link.get_text(strip=True)
                            
                            if href and link_text and href != 'javascript:;' and not href.startswith('#'):
                                # è½¬æ¢ä¸ºç»å¯¹URL
                                if not href.startswith('http'):
                                    href = urljoin(self.base_url, href)
                                
                                # åˆ¤æ–­æ–‡ä»¶ç±»å‹
                                is_image = any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'])
                                is_video = any(ext in href.lower() for ext in ['.mp4', '.avi', '.mov', '.wmv', '.mkv', '.flv']) or 'è§†é¢‘' in link_text
                                is_download = any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.zip', '.rar', '.xls', '.xlsx']) or any(keyword in link_text for keyword in ['è¯´æ˜ä¹¦', 'æ‰‹å†Œ', 'èµ„æ–™', 'è§„æ ¼'])
                                
                                if is_image:
                                    # å¤„ç†å›¾ç‰‡é“¾æ¥
                                    img_info = {
                                        'url': href,
                                        'alt': link_text
                                    }
                                    content['images'].append(img_info)
                                    print(f"        æ‰¾åˆ°åº”ç”¨å›¾ç‰‡: {link_text}")
                                    self.download_image(href, link_text, "åº”ç”¨", product_info, product_data)
                                
                                elif is_video:
                                    # å¤„ç†è§†é¢‘é“¾æ¥
                                    content['videos'].append(href)
                                    print(f"        æ‰¾åˆ°åº”ç”¨è§†é¢‘: {link_text}")
                                    self.download_video(href, link_text, "åº”ç”¨", product_info, product_data)
                                
                                elif is_download:
                                    # å¤„ç†ä¸‹è½½æ–‡ä»¶
                                    download_info = {
                                        'url': href,
                                        'title': link_text,
                                        'product_name': product_info['name'],
                                        'type': "åº”ç”¨",
                                        'category': product_data.get('basic_info', {}).get('category', 'æœªåˆ†ç±»')
                                    }
                                    content['download_links'].append(download_info)
                                    print(f"        æ‰¾åˆ°åº”ç”¨èµ„æ–™: {link_text}")
                                    self.download_file(download_info)
            
            # æå–æ–‡æœ¬å†…å®¹
            text_content = tab5_div.get_text(strip=True)
            content['text_content'] = text_content
            
            return content if any([content['images'], content['videos'], content['download_links'], content['text_content']]) else None
            
        except Exception as e:
            print(f"        åº”ç”¨æ¨¡å—è§£æå¤±è´¥: {str(e)}")
            return None

    def extract_application_content_selenium(self, product_info, product_data):
        """ä½¿ç”¨Seleniumæå–åº”ç”¨æ¨¡å—çš„åŠ¨æ€å†…å®¹"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from bs4 import BeautifulSoup
            
            # è®¾ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            
            driver = webdriver.Chrome(options=chrome_options)
            
            try:
                # è®¿é—®äº§å“é¡µé¢
                product_url = f"https://www.cuhnj.com/href/html/prodXq1?product={product_info['id']}"
                driver.get(product_url)
                
                # ç‚¹å‡»åº”ç”¨æ ‡ç­¾é¡µ
                app_tab = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.XPATH, "//li[@data-tab='tab5' or contains(@onclick,'tab5')]"))
                )
                driver.execute_script("arguments[0].click();", app_tab)
                
                # ç­‰å¾…å†…å®¹åŠ è½½
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.ID, "tab5"))
                )
                
                # è·å–æ¸²æŸ“åçš„HTML
                page_html = driver.page_source
                soup = BeautifulSoup(page_html, 'html.parser')
                
                # æå–åº”ç”¨æ¨¡å—å†…å®¹
                tab5_div = soup.find('div', id='tab5')
                if tab5_div:
                    content = {
                        'text_content': '',
                        'images': [],
                        'videos': [],
                        'download_links': []
                    }
                    
                    # æå–æ–‡æœ¬å†…å®¹
                    text_content = tab5_div.get_text(strip=True)
                    content['text_content'] = text_content
                    
                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    all_links = tab5_div.find_all('a', href=True)
                    for link in all_links:
                        href = link.get('href', '')
                        link_text = link.get_text(strip=True)
                        
                        if href and link_text and href != 'javascript:;' and not href.startswith('#'):
                            # è½¬æ¢ä¸ºç»å¯¹URL
                            if not href.startswith('http'):
                                href = urljoin(self.base_url, href)
                            
                            # åˆ¤æ–­æ–‡ä»¶ç±»å‹
                            is_image = any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'])
                            is_video = any(ext in href.lower() for ext in ['.mp4', '.avi', '.mov', '.wmv', '.mkv', '.flv']) or 'è§†é¢‘' in link_text
                            is_download = any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.zip', '.rar', '.xls', '.xlsx']) or any(keyword in link_text for keyword in ['è¯´æ˜ä¹¦', 'æ‰‹å†Œ', 'èµ„æ–™', 'è§„æ ¼'])
                            
                            if is_image:
                                # å¤„ç†å›¾ç‰‡é“¾æ¥
                                img_info = {
                                    'url': href,
                                    'alt': link_text
                                }
                                content['images'].append(img_info)
                                print(f"        æ‰¾åˆ°åº”ç”¨å›¾ç‰‡: {link_text}")
                                self.download_image(href, link_text, "åº”ç”¨", product_info, product_data)
                            
                            elif is_video:
                                # å¤„ç†è§†é¢‘é“¾æ¥
                                content['videos'].append(href)
                                print(f"        æ‰¾åˆ°åº”ç”¨è§†é¢‘: {link_text}")
                                self.download_video(href, link_text, "åº”ç”¨", product_info, product_data)
                            
                            elif is_download:
                                # å¤„ç†ä¸‹è½½æ–‡ä»¶
                                download_info = {
                                    'url': href,
                                    'title': link_text,
                                    'product_name': product_info['name'],
                                    'type': "åº”ç”¨",
                                    'category': product_data.get('basic_info', {}).get('category', 'æœªåˆ†ç±»')
                                }
                                content['download_links'].append(download_info)
                                print(f"        æ‰¾åˆ°åº”ç”¨èµ„æ–™: {link_text}")
                                self.download_file(download_info)
                    
                    return content
                
            finally:
                driver.quit()
                
        except Exception as e:
            print(f"        åº”ç”¨æ¨¡å—Seleniumè§£æå¤±è´¥: {str(e)}")
            return None
    
    def parse_product_materials_selenium(self, soup, product_data, product_info):
        """ä½¿ç”¨Seleniumè§£æäº§å“èµ„æ–™æ ‡ç­¾é¡µ"""
        download_links = []
        
        # æŸ¥æ‰¾äº§å“èµ„æ–™ä¸‹è½½é“¾æ¥
        tab3_div = soup.find('div', id='tab3')
        if tab3_div:
            print(f"      æ‰¾åˆ°äº§å“èµ„æ–™åŒºåŸŸ")
            
            # æŸ¥æ‰¾æ‰€æœ‰çš„ä¸‹è½½é“¾æ¥
            all_links = tab3_div.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                # è¿‡æ»¤æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥
                if href and link_text and href != 'javascript:;' and not href.startswith('#'):
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    if not href.startswith('http'):
                        href = urljoin(self.base_url, href)
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸‹è½½æ–‡ä»¶ï¼ˆPDFã€DOCç­‰ï¼‰
                    is_download = (
                        href.lower().endswith('.pdf') or 
                        href.lower().endswith('.doc') or 
                        href.lower().endswith('.docx') or
                        href.lower().endswith('.zip') or
                        href.lower().endswith('.rar') or
                        'è¯´æ˜ä¹¦' in link_text or
                        'æ‰‹å†Œ' in link_text or
                        'èµ„æ–™' in link_text or
                        'manual' in link_text.lower() or
                        'specification' in link_text.lower()
                    )
                    
                    if is_download:
                        download_links.append({
                            'url': href,
                            'title': link_text,
                            'product_name': product_info['name'],
                            'type': 'äº§å“èµ„æ–™',
                            'category': product_data.get('basic_info', {}).get('category', 'æœªåˆ†ç±»')
                        })
                        print(f"      æ‰¾åˆ°èµ„æ–™: {link_text} - {href}")
        
        product_data['download_links'] = download_links
    
    def parse_product_materials(self, tab_div, product_data, product_info):
        """è§£æäº§å“èµ„æ–™æ ‡ç­¾é¡µ"""
        download_links = []
        
        # æŸ¥æ‰¾äº§å“èµ„æ–™ä¸‹è½½é“¾æ¥
        shop_ul = tab_div.find('div', class_='shop-nt shop-ul')
        if shop_ul:
            ul_elem = shop_ul.find('ul')
            if ul_elem:
                li_elements = ul_elem.find_all('li')
                
                for li in li_elements:
                    link = li.find('a')
                    if link:
                        download_url = link.get('href', '')
                        if download_url and not download_url.startswith('http'):
                            download_url = urljoin(self.base_url, download_url)
                        
                        material_name = link.get_text(strip=True)
                        
                        if download_url and material_name:
                            download_links.append({
                                'url': download_url,
                                'title': material_name,
                                'product_name': product_info['name'],
                                'type': 'äº§å“èµ„æ–™',
                                'category': product_data.get('basic_info', {}).get('category', 'æœªåˆ†ç±»')
                            })
                            print(f"      æ‰¾åˆ°èµ„æ–™: {material_name} - {download_url}")
        
        # æŸ¥æ‰¾è¯´æ˜ä¹¦å†å²ç‰ˆæœ¬
        history_div = tab_div.find('div', class_='intructHistory')
        if history_div:
            history_ul = history_div.find('ul', class_='listContAll')
            if history_ul:
                history_links = history_ul.find_all('a')
                for link in history_links:
                    download_url = link.get('href', '')
                    if download_url and not download_url.startswith('http'):
                        download_url = urljoin(self.base_url, download_url)
                    
                    material_name = link.get_text(strip=True)
                    
                    if download_url and material_name:
                        download_links.append({
                            'url': download_url,
                            'title': f"å†å²ç‰ˆæœ¬_{material_name}",
                            'product_name': product_info['name'],
                            'type': 'å†å²ç‰ˆæœ¬',
                            'category': product_data.get('basic_info', {}).get('category', 'æœªåˆ†ç±»')
                        })
                        print(f"      æ‰¾åˆ°å†å²ç‰ˆæœ¬: {material_name} - {download_url}")
        
        product_data['download_links'] = download_links
    
    def extract_tab_content_with_download(self, tab_div, tab_name, product_info, product_data):
        """æå–æ ‡ç­¾é¡µå†…å®¹å¹¶ç«‹å³ä¸‹è½½ç›¸å…³æ–‡ä»¶"""
        content = {
            'text_content': '',
            'images': [],
            'videos': [],
            'download_links': []
        }
        
        # æå–æ–‡æœ¬å†…å®¹
        shop_nt = tab_div.find('div', class_='shop-nt')
        if shop_nt:
            # æå–çº¯æ–‡æœ¬
            text_content = shop_nt.get_text(strip=True)
            content['text_content'] = text_content
            
            # æå–å›¾ç‰‡å¹¶ç«‹å³ä¸‹è½½ - åœ¨æ•´ä¸ªtab_divä¸­æŸ¥æ‰¾ï¼Œä¸ä»…ä»…æ˜¯shop_nt
            images = tab_div.find_all('img')
            for img in images:
                img_src = img.get('src', '')
                if img_src and not img_src.startswith('http'):
                    img_src = urljoin(self.base_url, img_src)
                
                img_alt = img.get('alt', '') or f"{tab_name}_å›¾ç‰‡_{len(content['images'])}"
                if img_src:
                    img_info = {
                        'url': img_src,
                        'alt': img_alt
                    }
                    content['images'].append(img_info)
                    
                    # ç«‹å³ä¸‹è½½å›¾ç‰‡
                    self.download_image(img_src, img_alt, tab_name, product_info, product_data)
            
            # æå–è§†é¢‘å¹¶è®°å½•
            videos = shop_nt.find_all(['video', 'iframe'])
            for video in videos:
                video_src = video.get('src', '') or video.get('data-src', '')
                if video_src and not video_src.startswith('http'):
                    video_src = urljoin(self.base_url, video_src)
                
                if video_src:
                    content['videos'].append(video_src)
                    # å¦‚æœæ˜¯è§†é¢‘æ–‡ä»¶ï¼Œä¹Ÿå°è¯•ä¸‹è½½
                    if any(ext in video_src.lower() for ext in ['.mp4', '.avi', '.mov', '.wmv']):
                        video_name = f"{tab_name}_è§†é¢‘_{len(content['videos'])}"
                        self.download_video(video_src, video_name, tab_name, product_info, product_data)
            
            # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥ - åœ¨æ•´ä¸ªtab_divä¸­æŸ¥æ‰¾ï¼Œä¸ä»…ä»…æ˜¯shop_nt
            all_links = tab_div.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                # è¿‡æ»¤æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥
                if href and link_text and href != 'javascript:;' and not href.startswith('#'):
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    if not href.startswith('http'):
                        href = urljoin(self.base_url, href)
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶
                    is_image = (
                        href.lower().endswith('.jpg') or 
                        href.lower().endswith('.jpeg') or 
                        href.lower().endswith('.png') or 
                        href.lower().endswith('.gif') or
                        href.lower().endswith('.bmp') or
                        href.lower().endswith('.webp')
                    )
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯è§†é¢‘æ–‡ä»¶
                    is_video = (
                        href.lower().endswith('.mp4') or 
                        href.lower().endswith('.avi') or 
                        href.lower().endswith('.mov') or 
                        href.lower().endswith('.wmv') or
                        href.lower().endswith('.mkv') or
                        href.lower().endswith('.flv') or
                        'è§†é¢‘' in link_text
                    )
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸‹è½½æ–‡ä»¶
                    is_download = (
                        href.lower().endswith('.pdf') or 
                        href.lower().endswith('.doc') or 
                        href.lower().endswith('.docx') or
                        href.lower().endswith('.zip') or
                        href.lower().endswith('.rar') or
                        href.lower().endswith('.xls') or
                        href.lower().endswith('.xlsx') or
                        'è¯´æ˜ä¹¦' in link_text or
                        'æ‰‹å†Œ' in link_text or
                        'èµ„æ–™' in link_text or
                        'è§„æ ¼' in link_text or
                        'manual' in link_text.lower() or
                        'specification' in link_text.lower()
                    )
                    
                    if is_image:
                        # å¤„ç†å›¾ç‰‡é“¾æ¥
                        img_info = {
                            'url': href,
                            'alt': link_text
                        }
                        content['images'].append(img_info)
                        print(f"        æ‰¾åˆ°{tab_name}å›¾ç‰‡: {link_text}")
                        self.download_image(href, link_text, tab_name, product_info, product_data)
                    
                    elif is_video:
                        # å¤„ç†è§†é¢‘é“¾æ¥
                        content['videos'].append(href)
                        video_name = link_text or f"{tab_name}_è§†é¢‘_{len(content['videos'])}"
                        print(f"        æ‰¾åˆ°{tab_name}è§†é¢‘: {link_text}")
                        self.download_video(href, video_name, tab_name, product_info, product_data)
                    
                    elif is_download:
                        download_info = {
                            'url': href,
                            'title': link_text,
                            'product_name': product_info['name'],
                            'type': tab_name,
                            'category': product_data.get('basic_info', {}).get('category', 'æœªåˆ†ç±»')
                        }
                        content['download_links'].append(download_info)
                        product_data['download_links'].append(download_info)
                        
                        # ç«‹å³ä¸‹è½½æ–‡ä»¶
                        print(f"        æ‰¾åˆ°{tab_name}èµ„æ–™: {link_text}")
                        if self.download_file(download_info):
                            product_data['downloaded_files'].append(download_info)
        
        return content
    
    def extract_tab_content(self, tab_div, tab_name):
        """æå–æ ‡ç­¾é¡µå†…å®¹ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        content = {
            'text_content': '',
            'images': [],
            'videos': []
        }
        
        # æå–æ–‡æœ¬å†…å®¹
        shop_nt = tab_div.find('div', class_='shop-nt')
        if shop_nt:
            # æå–çº¯æ–‡æœ¬
            text_content = shop_nt.get_text(strip=True)
            content['text_content'] = text_content
            
            # æå–å›¾ç‰‡
            images = shop_nt.find_all('img')
            for img in images:
                img_src = img.get('src', '')
                if img_src and not img_src.startswith('http'):
                    img_src = urljoin(self.base_url, img_src)
                
                img_alt = img.get('alt', '')
                if img_src:
                    content['images'].append({
                        'url': img_src,
                        'alt': img_alt
                    })
            
            # æå–è§†é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
            videos = shop_nt.find_all(['video', 'iframe'])
            for video in videos:
                video_src = video.get('src', '') or video.get('data-src', '')
                if video_src and not video_src.startswith('http'):
                    video_src = urljoin(self.base_url, video_src)
                
                if video_src:
                    content['videos'].append(video_src)
        
        return content
    
    def download_image(self, img_url, img_alt, tab_name, product_info, product_data):
        """ä¸‹è½½å›¾ç‰‡"""
        # å¦‚æœå¯ç”¨äº†è·³è¿‡ä¸‹è½½æ¨¡å¼ï¼Œåªè¿”å›æˆåŠŸä¿¡æ¯ä½†ä¸å®é™…ä¸‹è½½
        if self.skip_download:
            print(f"        [è·³è¿‡ä¸‹è½½å›¾ç‰‡] {img_alt}")
            return True
            
        try:
            # æ¸…ç†æ–‡ä»¶å
            img_alt = re.sub(r'[<>:"/\\|?*]', '_', img_alt)
            if not img_alt or not img_alt.strip():
                img_alt = f"{tab_name}_å›¾ç‰‡"
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(img_url)
            path = parsed_url.path
            if '.' in path:
                ext = path.split('.')[-1].lower()
                if ext not in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp']:
                    ext = 'jpg'  # é»˜è®¤æ‰©å±•å
            else:
                ext = 'jpg'
            
            filename = f"{img_alt}.{ext}"
            
            # åˆ›å»ºç›®å½•ç»“æ„ - æŒ‰åˆ†ç±»ç»„ç»‡
            category = "æœªåˆ†ç±»"
            if product_data and 'basic_info' in product_data and 'category' in product_data['basic_info']:
                category = product_data['basic_info']['category']
            
            # æ¸…ç†åˆ†ç±»åç§°å’Œäº§å“åç§°ï¼Œé¿å…æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
            category = re.sub(r'[<>:"/\\|?*]', '_', category)
            product_name = re.sub(r'[<>:"/\\|?*]', '_', product_info['name'])
            
            category_dir = os.path.join(self.download_dir, category)
            product_dir = os.path.join(category_dir, product_name)
            type_dir = os.path.join(product_dir, tab_name)
            os.makedirs(type_dir, exist_ok=True)
            
            filepath = os.path.join(type_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            if os.path.exists(filepath):
                return True
            
            print(f"        æ­£åœ¨ä¸‹è½½å›¾ç‰‡: {filename}")
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(img_url, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            file_size = os.path.getsize(filepath)
            print(f"        å›¾ç‰‡ä¸‹è½½å®Œæˆ: {filename} ({file_size} bytes)")
            
            # è®°å½•ä¸‹è½½çš„å›¾ç‰‡
            image_info = {
                'url': img_url,
                'title': img_alt,
                'filename': filename,
                'path': filepath,
                'size': file_size,
                'product_name': product_info['name'],
                'type': f"{tab_name}_å›¾ç‰‡"
            }
            product_data['downloaded_files'].append(image_info)
            
            return True
            
        except Exception as e:
            print(f"        å›¾ç‰‡ä¸‹è½½å¤±è´¥ {img_url}: {e}")
            return False
    
    def download_video(self, video_url, video_name, tab_name, product_info, product_data):
        """ä¸‹è½½è§†é¢‘"""
        try:
            # æ¸…ç†æ–‡ä»¶å
            video_name = re.sub(r'[<>:"/\\|?*]', '_', video_name)
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(video_url)
            path = parsed_url.path
            if '.' in path:
                ext = path.split('.')[-1].lower()
                if ext not in ['mp4', 'avi', 'mov', 'wmv', 'flv', 'mkv']:
                    ext = 'mp4'  # é»˜è®¤æ‰©å±•å
            else:
                ext = 'mp4'
            
            filename = f"{video_name}.{ext}"
            
            # åˆ›å»ºç›®å½•ç»“æ„ - æŒ‰åˆ†ç±»ç»„ç»‡
            category = "æœªåˆ†ç±»"
            if product_data and 'basic_info' in product_data and 'category' in product_data['basic_info']:
                category = product_data['basic_info']['category']
            
            # æ¸…ç†åˆ†ç±»åç§°å’Œäº§å“åç§°ï¼Œé¿å…æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
            category = re.sub(r'[<>:"/\\|?*]', '_', category)
            product_name = re.sub(r'[<>:"/\\|?*]', '_', product_info['name'])
            
            category_dir = os.path.join(self.download_dir, category)
            product_dir = os.path.join(category_dir, product_name)
            type_dir = os.path.join(product_dir, tab_name)
            os.makedirs(type_dir, exist_ok=True)
            
            filepath = os.path.join(type_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            if os.path.exists(filepath):
                return True
            
            print(f"        æ­£åœ¨ä¸‹è½½è§†é¢‘: {filename}")
            
            # ä¸‹è½½è§†é¢‘
            response = self.session.get(video_url, stream=True, timeout=60)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            file_size = os.path.getsize(filepath)
            print(f"        è§†é¢‘ä¸‹è½½å®Œæˆ: {filename} ({file_size} bytes)")
            
            # è®°å½•ä¸‹è½½çš„è§†é¢‘
            video_info = {
                'url': video_url,
                'title': video_name,
                'filename': filename,
                'path': filepath,
                'size': file_size,
                'product_name': product_info['name'],
                'type': f"{tab_name}_è§†é¢‘"
            }
            product_data['downloaded_files'].append(video_info)
            
            return True
            
        except Exception as e:
            print(f"        è§†é¢‘ä¸‹è½½å¤±è´¥ {video_url}: {e}")
            return False
    
    def download_file(self, download_info):
        """ä¸‹è½½æ–‡ä»¶"""
        # å¦‚æœå¯ç”¨äº†è·³è¿‡ä¸‹è½½æ¨¡å¼ï¼Œåªè¿”å›æˆåŠŸä¿¡æ¯ä½†ä¸å®é™…ä¸‹è½½
        if self.skip_download:
            print(f"        [è·³è¿‡ä¸‹è½½] {download_info['title']}")
            return True
            
        try:
            url = download_info['url']
            filename = download_info['title']
            
            # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
            filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
            
            # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼ï¼Œè®¾ç½®é»˜è®¤åç§°
            if not filename or not filename.strip():
                filename = "äº§å“èµ„æ–™"
            
            # ç¡®ä¿æ–‡ä»¶æœ‰æ‰©å±•å
            if '.' not in filename.split('/')[-1]:
                # å°è¯•ä»URLè·å–æ‰©å±•å
                parsed_url = urlparse(url)
                path = parsed_url.path
                if '.' in path:
                    ext = path.split('.')[-1].lower()
                    if ext in ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'zip', 'rar', 'jpg', 'png']:
                        filename += f'.{ext}'
                else:
                    # é»˜è®¤ä¸ºPDF
                    filename += '.pdf'
            
            # åˆ›å»ºç›®å½•ç»“æ„ - æŒ‰åˆ†ç±»ç»„ç»‡
            category = download_info.get('category', 'æœªåˆ†ç±»')
            product_name = re.sub(r'[<>:"/\\|?*]', '_', download_info['product_name'])
            material_type = download_info.get('type', 'å…¶ä»–èµ„æ–™')
            
            # æ¸…ç†åˆ†ç±»åç§°ï¼Œé¿å…æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
            category = re.sub(r'[<>:"/\\|?*]', '_', category)
            
            # åˆ›å»ºåˆ†ç±»ç›®å½•
            category_dir = os.path.join(self.download_dir, category)
            if not os.path.exists(category_dir):
                os.makedirs(category_dir)
            
            # åˆ›å»ºäº§å“ç›®å½•
            product_dir = os.path.join(category_dir, product_name)
            if not os.path.exists(product_dir):
                os.makedirs(product_dir)
            
            # åˆ›å»ºèµ„æ–™ç±»å‹ç›®å½•
            type_dir = os.path.join(product_dir, material_type)
            if not os.path.exists(type_dir):
                os.makedirs(type_dir)
            
            # å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            filepath = os.path.join(type_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            if os.path.exists(filepath):
                print(f"        æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return True
            
            print(f"        æ­£åœ¨ä¸‹è½½: {filename}")
            
            # ä½¿ç”¨requestsä¸‹è½½æ–‡ä»¶
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' in content_type and 'pdf' not in content_type:
                print(f"        è­¦å‘Š: ä¸‹è½½çš„å¯èƒ½ä¸æ˜¯æ–‡ä»¶ï¼Œè€Œæ˜¯HTMLé¡µé¢")
                return False
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filepath)
            if file_size < 1024:  # å°äº1KBå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                print(f"        è­¦å‘Š: æ–‡ä»¶å¤§å°å¼‚å¸¸ ({file_size} bytes)")
            
            print(f"        ä¸‹è½½å®Œæˆ: {filename} ({file_size} bytes)")
            
            # è®°å½•æ–°æ–‡ä»¶
            file_key = f"{download_info['product_name']}_{filename}"
            if file_key not in self.processed_files:
                self.new_files.append({
                    'filename': filename,
                    'path': filepath,
                    'url': download_info['url'],
                    'size': file_size,
                    'product': download_info['product_name'],
                    'type': download_info.get('type', 'å…¶ä»–èµ„æ–™')
                })
                self.processed_files.add(file_key)
            
            return True
            
        except Exception as e:
            print(f"        ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def save_data(self, data, filename):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        filepath = os.path.join(self.output_dir, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.log("ğŸš€ å¼€å§‹çˆ¬å–åˆ›ä¼˜äº§å“ä¸­å¿ƒ...")
        
        # 1. è·å–æ‰€æœ‰äº§å“
        products = self.parse_main_page()
        if not products:
            self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•äº§å“")
            return
        
        self.log(f"ğŸ“‹ å…±æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
        
        # å¦‚æœæŒ‡å®šäº†é™åˆ¶æ•°é‡ï¼Œåˆ™æˆªå–äº§å“åˆ—è¡¨
        if self.limit:
            products = products[:self.limit]
            self.log(f"ğŸ”„ é™åˆ¶å¤„ç†æ•°é‡: {len(products)} ä¸ªäº§å“")
        
        # 2. ä¿å­˜äº§å“åˆ—è¡¨
        self.save_data(products, 'products_list.json')
        
        # 3. çˆ¬å–æ¯ä¸ªäº§å“çš„è¯¦ç»†ä¿¡æ¯ï¼ˆæ¯è§£æä¸€ä¸ªæ¨¡å—å°±ç«‹å³ä¸‹è½½ï¼‰
        all_products_data = []
        total_downloaded_files = 0
        
        for i, product in enumerate(products, 1):
            self.log(f"ğŸ”„ è¿›åº¦: {i}/{len(products)} - {product['name']}")
            
            # è§£æäº§å“è¯¦æƒ…é¡µï¼ˆåŒ…å«å®æ—¶ä¸‹è½½ï¼‰
            product_data = self.parse_product_page(product)
            
            if product_data:
                # åˆå¹¶åŸºæœ¬ä¿¡æ¯
                full_product_data = {**product, **product_data}
                all_products_data.append(full_product_data)
                
                # ç»Ÿè®¡ä¸‹è½½çš„æ–‡ä»¶æ•°é‡
                downloaded_count = len(product_data.get('downloaded_files', []))
                if downloaded_count > 0:
                    total_downloaded_files += downloaded_count
                    self.log(f"    âœ… äº§å“ {product['name']} å®Œæˆï¼Œä¸‹è½½äº† {downloaded_count} ä¸ªæ–‡ä»¶")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        # 4. ä¿å­˜æ‰€æœ‰äº§å“è¯¦ç»†ä¿¡æ¯
        if all_products_data:
            self.save_data(all_products_data, 'products_detail.json')
            self.log(f"âœ… äº§å“ä¿¡æ¯çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_products_data)} ä¸ªäº§å“è¯¦æƒ…")
            self.log(f"ğŸ“¥ èµ„æ–™ä¸‹è½½å®Œæˆï¼æ€»å…±ä¸‹è½½äº† {total_downloaded_files} ä¸ªæ–‡ä»¶")
        
        # 5. ä¿å­˜å¤„ç†è®°å½•
        self.save_processed_files()
        
        # 6. å‘é€é’‰é’‰é€šçŸ¥
        self.send_completion_notification()
        
        # 7. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(products, all_products_data, total_downloaded_files)
        
        # 8. å…³é—­WebDriver
        self.close_webdriver()
    
    def send_completion_notification(self):
        """å‘é€å®Œæˆé€šçŸ¥"""
        if not self.new_files:
            if not self.is_first_run:
                self.log("ğŸ“¢ æ— æ–°æ–‡ä»¶ï¼Œä¸å‘é€é€šçŸ¥")
            return
        
        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
        message_parts = []
        message_parts.append(f"ğŸ“Š åˆ›ä¼˜çˆ¬è™«å®Œæˆ")
        message_parts.append(f"ğŸ•’ æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        message_parts.append(f"ğŸ“ æ–°ä¸‹è½½æ–‡ä»¶: {len(self.new_files)} ä¸ª")
        
        if self.is_first_run:
            message_parts.append("ğŸ†• é¦–æ¬¡è¿è¡Œï¼Œå·²å»ºç«‹åŸºçº¿")
        
        # æŒ‰äº§å“åˆ†ç»„æ˜¾ç¤ºæ–°æ–‡ä»¶
        product_files = {}
        for file_info in self.new_files:
            product = file_info['product']
            if product not in product_files:
                product_files[product] = []
            product_files[product].append(file_info)
        
        message_parts.append("\nğŸ“‹ æ–°æ–‡ä»¶è¯¦æƒ…:")
        for product, files in product_files.items():
            message_parts.append(f"  ğŸ“‚ {product}: {len(files)} ä¸ªæ–‡ä»¶")
            for file_info in files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                size_mb = file_info['size'] / 1024 / 1024
                message_parts.append(f"    ğŸ“„ {file_info['filename']} ({size_mb:.1f}MB)")
            if len(files) > 3:
                message_parts.append(f"    ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")
        
        message = "\n".join(message_parts)
        self.send_dingtalk_notification(message)
    
    def generate_report(self, products, products_data, total_downloaded_files=0):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            'çˆ¬å–æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S'),
            'æ€»äº§å“æ•°': len(products),
            'æˆåŠŸè§£æäº§å“æ•°': len(products_data),
            'æ€»ä¸‹è½½æ–‡ä»¶æ•°': total_downloaded_files,
            'æ–°æ–‡ä»¶æ•°': len(self.new_files),
            'äº§å“åˆ—è¡¨': [p['name'] for p in products]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        self.save_data(report, 'çˆ¬å–æŠ¥å‘Š.json')
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "="*50)
        print("çˆ¬å–æŠ¥å‘Š")
        print("="*50)
        print(f"çˆ¬å–æ—¶é—´: {report['çˆ¬å–æ—¶é—´']}")
        print(f"æ€»äº§å“æ•°: {report['æ€»äº§å“æ•°']}")
        print(f"æˆåŠŸè§£æäº§å“æ•°: {report['æˆåŠŸè§£æäº§å“æ•°']}")
        print(f"æ€»ä¸‹è½½æ–‡ä»¶æ•°: {report['æ€»ä¸‹è½½æ–‡ä»¶æ•°']}")
        print(f"æ–°æ–‡ä»¶æ•°: {report['æ–°æ–‡ä»¶æ•°']}")
        print("="*50)

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='åˆ›ä¼˜äº§å“ä¸­å¿ƒçˆ¬è™«')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†çš„äº§å“æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--categories', nargs='+', help='æŒ‡å®šè¦çˆ¬å–çš„åˆ†ç±»ç¼–å·ï¼Œå¦‚ï¼š--categories 1 2 3')
    parser.add_argument('--no-webdriver', action='store_true', help='ä¸ä½¿ç”¨WebDriverï¼Œä»…ä½¿ç”¨requests')
    parser.add_argument('--skip-download', action='store_true', help='è·³è¿‡æ–‡ä»¶ä¸‹è½½ï¼Œä»…æå–æ•°æ®ç”¨äºæµ‹è¯•')
    parser.add_argument('--test-url', type=str, help='æµ‹è¯•æŒ‡å®šçš„äº§å“URL')
    
    args = parser.parse_args()
    
    if args.test_url:
        # æµ‹è¯•æŒ‡å®šURL
        spider = ChuangyouSpider(limit=1, skip_download=False)
        try:
            # æ‰‹åŠ¨åˆ›å»ºäº§å“ä¿¡æ¯
            import re
            product_id_match = re.search(r'product=(\d+)', args.test_url)
            if product_id_match:
                product_id = product_id_match.group(1)
                product_info = {
                    'id': product_id,
                    'name': f'Test_Product_{product_id}',
                    'url': args.test_url,
                    'category': 'æµ‹è¯•åˆ†ç±»',
                    'type': 'äº§å“'
                }
                print(f"å¼€å§‹æµ‹è¯•äº§å“: {args.test_url}")
                spider.parse_product_page(product_info)
            else:
                print("æ— æ³•è§£æäº§å“ID")
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            spider.close_webdriver()
    else:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = ChuangyouSpider(
            limit=args.limit,
            categories=args.categories,
            no_webdriver=args.no_webdriver,
            skip_download=args.skip_download
        )
        
        try:
            spider.run()
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
            spider.close_webdriver()
        except Exception as e:
            print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            spider.close_webdriver()
