#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import json
import pickle
import platform
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote
import requests
import chardet

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException
from bs4 import BeautifulSoup

class JTEKTSpider:
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.base_url = "https://www.jtektele.com.cn"
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/å…‰æ´‹"
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "downloads", "å…‰æ´‹")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.base_dir, exist_ok=True)
        
        self.processed_urls = self.load_processed_urls()
        self.new_files = []
        self.debug = True
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆå…¨é‡çˆ¬å–ï¼‰
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_urls.pkl'))
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.ACCESS_TOKEN = "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        self.SECRET = "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb"
        
        # åˆå§‹åŒ–WebDriver
        self.driver = None
        self.init_webdriver()
        
        # ä¸»è¦çˆ¬å–æ¨¡å—
        self.main_modules = [
            {
                'name': 'èµ„æ–™ä¸‹è½½',
                'url': 'https://www.jtektele.com.cn/index.php/download',
                'categories': [
                    {'name': 'äº§å“é€‰å‹æ ·æœ¬', 'url': 'https://www.jtektele.com.cn/index.php/download/8'},
                    {'name': 'NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/9'},
                    {'name': 'SNç³»åˆ—PLCæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/10'},
                    {'name': 'DL05/06ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/11'},
                    {'name': 'DL205/SZç³»åˆ—PLCæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/12'},
                    {'name': 'DL405/SUç³»åˆ—PLCæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/13'},
                    {'name': 'SJ / CLICKç³»åˆ—PLCæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/46'},
                    {'name': 'å…¶å®ƒç³»åˆ—PLCæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/14'},
                    {'name': 'PLCå…±é€šæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/15'},
                    {'name': 'GC-A2/H00ç³»åˆ—è§¦æ‘¸å±æŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/28'},
                    {'name': 'Cmore/EA7Eç³»åˆ—è§¦æ‘¸å±æŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/25'},
                    {'name': 'GC/EA7EAIPç³»åˆ—è§¦æ‘¸å±æŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/16'},
                    {'name': 'æ˜¾ç¤ºè®¾å®šå•å…ƒæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/29'},
                    {'name': 'ç¼–ç å™¨ç›¸å…³æŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/30'},
                    {'name': 'KSD-A3ä¼ºæœç³»ç»ŸæŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/45'},
                    {'name': 'å˜é¢‘å™¨äº§å“æŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/31'},
                    {'name': 'å…¶ä»–äº§å“æŠ€æœ¯èµ„æ–™', 'url': 'https://www.jtektele.com.cn/index.php/download/32'},
                    {'name': 'è½¯ä»¶ä¸‹è½½', 'url': 'https://www.jtektele.com.cn/index.php/download/33'},
                    {'name': 'è®¤è¯æ ‡å¿—ä¸‹è½½', 'url': 'https://www.jtektele.com.cn/index.php/download/39'}
                ]
            },
            {
                'name': 'æ•™å­¦è§†é¢‘',
                'url': 'https://www.jtektele.com.cn/index.php/video',
                'categories': [
                    {'name': 'NK1ç³»åˆ—PLCæ•™å­¦è§†é¢‘', 'url': 'https://www.jtektele.com.cn/index.php/video/36'},
                    {'name': 'ç¼–ç å™¨ç³»åˆ—äº§å“ä»‹ç»è§†é¢‘', 'url': 'https://www.jtektele.com.cn/index.php/video/41'},
                    {'name': 'GC-A2ç³»åˆ—è§¦æ‘¸å±ä»‹ç»è§†é¢‘', 'url': 'https://www.jtektele.com.cn/index.php/video/37'},
                    {'name': 'NK0ç³»åˆ—PLCæ•™å­¦è§†é¢‘', 'url': 'https://www.jtektele.com.cn/index.php/video/38'},
                    {'name': 'YKANæ¬¡ä¸–ä»£HMIä»‹ç»è§†é¢‘', 'url': 'https://www.jtektele.com.cn/index.php/video/40'},
                    {'name': 'è¿œç¨‹I/Oå•å…ƒ', 'url': 'https://www.jtektele.com.cn/index.php/video/42'},
                    {'name': 'JXç³»åˆ—PLCä»‹ç»è§†é¢‘', 'url': 'https://www.jtektele.com.cn/index.php/video/43'},
                    {'name': 'ä¼ºæœäº§å“ä»‹ç»è§†é¢‘', 'url': 'https://www.jtektele.com.cn/index.php/video/44'}
                ]
            }
        ]
        
    def init_webdriver(self):
        """åˆå§‹åŒ–Chrome WebDriver"""
        try:
            # æ£€æµ‹ç³»ç»Ÿæ¶æ„
            system = platform.system()
            machine = platform.machine()
            
            # ç¡®å®šchromedriverè·¯å¾„
            # åœ¨æœåŠ¡å™¨ä¸ŠæŸ¥æ‰¾chromedriverç›®å½•
            if os.path.exists("/srv/crawler/chromedriver_downloads"):
                chromedriver_dir = "/srv/crawler/chromedriver_downloads"
            else:
                chromedriver_dir = os.path.join(os.getcwd(), "chromedriver_downloads")
            
            if system == "Darwin":  # macOS
                if machine == "arm64":
                    chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_mac-arm64", "chromedriver-mac-arm64", "chromedriver")
                else:
                    chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_mac-x64", "chromedriver-mac-x64", "chromedriver")
            elif system == "Linux":
                chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_linux64", "chromedriver-linux64", "chromedriver")
            elif system == "Windows":
                chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_win64", "chromedriver-win64", "chromedriver.exe")
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ: {system}")
            
            if not os.path.exists(chromedriver_path):
                raise Exception(f"ChromeDriveræœªæ‰¾åˆ°: {chromedriver_path}")
            
            # è®¾ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # åˆå§‹åŒ–WebDriver
            service = Service(executable_path=chromedriver_path)
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(10)
            
            self.log("âœ… Chrome WebDriveråˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.log(f"âŒ WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URLåˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_urls.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_urls(self):
        """ä¿å­˜å·²å¤„ç†çš„URLåˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_urls.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_urls, f)
    
    def visit_page(self, url, retry_count=3):
        """è®¿é—®é¡µé¢å¹¶è¿”å›BeautifulSoupå¯¹è±¡"""
        for attempt in range(retry_count):
            try:
                self.log(f"ğŸ”„ è®¿é—®é¡µé¢ (å°è¯•{attempt+1}/{retry_count}): {url}")
                self.driver.get(url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                WebDriverWait(self.driver, 30).until(
                    lambda driver: driver.execute_script("return document.readyState") == "complete"
                )
                
                # é¢å¤–ç­‰å¾…JavaScriptæ‰§è¡Œ
                time.sleep(3)
                
                # å°è¯•æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                # è·å–é¡µé¢æºç 
                page_source = self.driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')
                
                return soup
                
            except Exception as e:
                self.log(f"âŒ é¡µé¢è®¿é—®å¤±è´¥ (å°è¯•{attempt+1}): {str(e)}")
                time.sleep(5)
        
        self.log(f"âŒ é¡µé¢è®¿é—®å®Œå…¨å¤±è´¥: {url}")
        return None
    
    def find_download_links(self, soup, page_url):
        """ä»é¡µé¢ä¸­æŸ¥æ‰¾ä¸‹è½½é“¾æ¥"""
        downloads = []
        
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾å…·æœ‰downloadå±æ€§çš„é“¾æ¥ï¼ˆä¸»è¦ä¸‹è½½åŒºåŸŸï¼‰
            download_links = soup.find_all('a', {'download': True, 'href': True})
            self.log(f"ğŸ” æ‰¾åˆ° {len(download_links)} ä¸ªå¸¦downloadå±æ€§çš„é“¾æ¥")
            
            for link in download_links:
                href = link.get('href', '')
                download_name = link.get('download', '')
                
                # è·å–æ˜¾ç¤ºæ–‡æœ¬
                text_element = link.find('p')
                display_text = text_element.get_text().strip() if text_element else download_name
                
                if href and display_text:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = urljoin(self.base_url, href)
                    elif not href.startswith('http'):
                        full_url = urljoin(page_url, href)
                    else:
                        full_url = href
                    
                    downloads.append({
                        'title': display_text,
                        'url': full_url,
                        'filename': download_name,
                        'type': 'direct_download'
                    })
                    
                    self.log(f"   âœ… æ‰¾åˆ°ä¸‹è½½: {display_text} -> {full_url}")
            
            # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–‡ä»¶æ‰©å±•åçš„é“¾æ¥
            all_links = soup.find_all('a', href=True)
            self.log(f"ğŸ” æ€»å…±æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text().strip()
                
                # æŸ¥æ‰¾åŒ…å«æ–‡æ¡£æ‰©å±•åçš„é“¾æ¥
                if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar']):
                    full_url = href if href.startswith('http') else urljoin(page_url, href)
                    
                    # é¿å…é‡å¤
                    if not any(d['url'] == full_url for d in downloads):
                        downloads.append({
                            'title': text or "ä¸‹è½½æ–‡ä»¶",
                            'url': full_url,
                            'filename': '',
                            'type': 'document_link'
                        })
                        
                        self.log(f"   âœ… æ‰¾åˆ°æ–‡æ¡£: {text} -> {full_url}")
            
            # æ–¹æ³•3: æŸ¥æ‰¾æ•™è‚²è§†é¢‘é“¾æ¥ï¼ˆç‰¹å®šç»“æ„ï¼‰
            video_div = soup.find('div', class_='inner_fl_video')
            if video_div:
                self.log(f"ğŸ” æ‰¾åˆ°æ•™è‚²è§†é¢‘åŒºåŸŸ")
                video_links = video_div.find_all('a', href=True)
                
                for link in video_links:
                    href = link.get('href', '')
                    video_txt_div = link.find('div', class_='video_txt')
                    title = "æ•™å­¦è§†é¢‘"
                    
                    if video_txt_div:
                        p_tag = video_txt_div.find('p')
                        if p_tag:
                            title = p_tag.get_text().strip()
                    
                    if href and 'videoshow' in href:
                        full_url = href if href.startswith('http') else urljoin(page_url, href)
                        
                        # é¿å…é‡å¤
                        if not any(d['url'] == full_url for d in downloads):
                            downloads.append({
                                'title': title,
                                'url': full_url,
                                'filename': '',
                                'type': 'video_page'
                            })
                            
                            self.log(f"   âœ… æ‰¾åˆ°æ•™è‚²è§†é¢‘é¡µé¢: {title} -> {full_url}")
            
            # æ–¹æ³•4: æŸ¥æ‰¾å…¶ä»–è§†é¢‘é“¾æ¥ï¼ˆå¯¹äºæ™®é€šè§†é¢‘é¡µé¢ï¼‰
            if '/video/' in page_url or 'videoshow' in page_url:
                # æŸ¥æ‰¾è§†é¢‘ç›¸å…³çš„é“¾æ¥
                video_links = soup.find_all('a', href=True)
                for link in video_links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # æŸ¥æ‰¾åŒ…å«è§†é¢‘æ–‡ä»¶çš„é“¾æ¥æˆ–è§†é¢‘å¹³å°é“¾æ¥
                    if any(ext in href.lower() for ext in ['.mp4', '.avi', '.mov', '.wmv', '.flv']) or \
                       any(platform in href.lower() for platform in ['youtube', 'youku', 'bilibili']):
                        
                        full_url = href if href.startswith('http') else urljoin(page_url, href)
                        
                        # é¿å…é‡å¤
                        if not any(d['url'] == full_url for d in downloads):
                            downloads.append({
                                'title': text or "æ•™å­¦è§†é¢‘",
                                'url': full_url,
                                'filename': '',
                                'type': 'video_direct'
                            })
                            
                            self.log(f"   âœ… æ‰¾åˆ°ç›´æ¥è§†é¢‘: {text} -> {full_url}")
            
            # æ–¹æ³•5: æŸ¥æ‰¾æŒ‰é’®æˆ–å›¾ç‰‡é“¾æ¥ï¼Œå¯èƒ½éšè—ä¸‹è½½é“¾æ¥
            button_links = soup.find_all(['button', 'div', 'span'], onclick=True)
            for element in button_links:
                onclick = element.get('onclick', '')
                if 'download' in onclick.lower() or 'file' in onclick.lower():
                    self.log(f"   ğŸ” æ‰¾åˆ°å¯èƒ½çš„ä¸‹è½½æŒ‰é’®: {onclick}")
            
            # æ–¹æ³•6: æŸ¥æ‰¾iframeæˆ–embedæ ‡ç­¾ï¼ˆå¯èƒ½åŒ…å«æ–‡æ¡£é¢„è§ˆï¼‰
            iframes = soup.find_all(['iframe', 'embed'])
            for iframe in iframes:
                src = iframe.get('src', '')
                if src and any(ext in src.lower() for ext in ['.pdf', '.doc', '.docx']):
                    full_url = src if src.startswith('http') else urljoin(page_url, src)
                    if not any(d['url'] == full_url for d in downloads):
                        downloads.append({
                            'title': "åµŒå…¥æ–‡æ¡£",
                            'url': full_url,
                            'filename': '',
                            'type': 'embedded_document'
                        })
                        self.log(f"   âœ… æ‰¾åˆ°åµŒå…¥æ–‡æ¡£: {full_url}")
            
            if downloads:
                self.log(f"ğŸ“ åœ¨é¡µé¢ä¸­æ‰¾åˆ° {len(downloads)} ä¸ªä¸‹è½½æ–‡ä»¶")
            else:
                self.log(f"âŒ é¡µé¢ä¸­æœªæ‰¾åˆ°ä¸‹è½½æ–‡ä»¶: {page_url}")
                # è¾“å‡ºä¸€äº›è°ƒè¯•ä¿¡æ¯
                self.log(f"ğŸ” é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
                forms = soup.find_all('form')
                self.log(f"ğŸ” é¡µé¢è¡¨å•æ•°é‡: {len(forms)}")
                scripts = soup.find_all('script')
                self.log(f"ğŸ” é¡µé¢è„šæœ¬æ•°é‡: {len(scripts)}")
            
        except Exception as e:
            self.log(f"âŒ æŸ¥æ‰¾ä¸‹è½½é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
        
        return downloads
    
    def find_pagination_links(self, soup, base_url):
        """æŸ¥æ‰¾åˆ†é¡µé“¾æ¥"""
        pagination_links = []
        
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾åˆ†é¡µå¯¼èˆªåŒºåŸŸ
            pagination_div = soup.find('div', class_='pagination')
            if not pagination_div:
                # å°è¯•å…¶ä»–å¯èƒ½çš„åˆ†é¡µç±»å
                pagination_div = soup.find('div', class_='page')
                if not pagination_div:
                    pagination_div = soup.find('ul', class_='pagination')
                    if not pagination_div:
                        pagination_div = soup.find('div', class_='pages')
            
            if pagination_div:
                self.log(f"ğŸ” æ‰¾åˆ°åˆ†é¡µå¯¼èˆªåŒºåŸŸ: {pagination_div.get('class', 'unknown')}")
                
                # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥
                page_links = pagination_div.find_all('a', href=True)
                
                for link in page_links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # è¿‡æ»¤æ‰"ä¸Šä¸€é¡µ"ã€"ä¸‹ä¸€é¡µ"ç­‰å¯¼èˆªé“¾æ¥ï¼Œåªä¿ç•™é¡µç é“¾æ¥
                    if text.isdigit() and href:
                        # æ„å»ºå®Œæ•´çš„åˆ†é¡µURL
                        if href.startswith('/'):
                            full_url = urljoin(self.base_url, href)
                        elif not href.startswith('http'):
                            full_url = urljoin(base_url, href)
                        else:
                            full_url = href
                        
                        # é¿å…é‡å¤
                        if full_url not in [p['url'] for p in pagination_links]:
                            pagination_links.append({
                                'page': int(text),
                                'url': full_url,
                                'text': text
                            })
                
                # æŒ‰é¡µç æ’åº
                pagination_links.sort(key=lambda x: x['page'])
                
                if pagination_links:
                    self.log(f"ğŸ“„ æ‰¾åˆ° {len(pagination_links)} ä¸ªåˆ†é¡µé“¾æ¥")
                    for page_info in pagination_links:
                        self.log(f"   ğŸ“„ ç¬¬{page_info['page']}é¡µ: {page_info['url']}")
                else:
                    self.log(f"âš ï¸ åˆ†é¡µå¯¼èˆªåŒºåŸŸä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¡µç é“¾æ¥")
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ†é¡µé“¾æ¥
            if not pagination_links:
                self.log(f"ğŸ” å°è¯•å¤‡ç”¨æ–¹æ³•æŸ¥æ‰¾åˆ†é¡µé“¾æ¥")
                
                # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ï¼Œå¯»æ‰¾åˆ†é¡µæ¨¡å¼
                all_links = soup.find_all('a', href=True)
                page_patterns = []
                
                for link in all_links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # æŸ¥æ‰¾åˆ†é¡µURLæ¨¡å¼
                    if text.isdigit() and href:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†é¡µURLï¼ˆåŒ…å«downloadå’Œæ•°å­—ï¼‰
                        if '/download/' in href and any(char.isdigit() for char in href):
                            # æ„å»ºå®Œæ•´çš„åˆ†é¡µURL
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            elif not href.startswith('http'):
                                full_url = urljoin(base_url, href)
                            else:
                                full_url = href
                            
                            # é¿å…é‡å¤
                            if full_url not in [p['url'] for p in pagination_links]:
                                page_patterns.append({
                                    'page': int(text),
                                    'url': full_url,
                                    'text': text
                                })
                
                # å¦‚æœæ‰¾åˆ°åˆ†é¡µæ¨¡å¼ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                if page_patterns:
                    page_patterns.sort(key=lambda x: x['page'])
                    pagination_links.extend(page_patterns)
                    self.log(f"ğŸ“„ å¤‡ç”¨æ–¹æ³•æ‰¾åˆ° {len(page_patterns)} ä¸ªåˆ†é¡µé“¾æ¥")
                    for page_info in page_patterns:
                        self.log(f"   ğŸ“„ ç¬¬{page_info['page']}é¡µ: {page_info['url']}")
            
            # æ–¹æ³•3: æ‰‹åŠ¨æ„å»ºåˆ†é¡µURLï¼ˆé’ˆå¯¹å…‰æ´‹ç½‘ç«™çš„ç‰¹æ®Šæƒ…å†µï¼‰
            if not pagination_links and '/download/' in base_url:
                self.log(f"ğŸ” å°è¯•æ‰‹åŠ¨æ„å»ºåˆ†é¡µURL")
                
                # ä»åŸºç¡€URLä¸­æå–åˆ†ç±»ID
                import re
                match = re.search(r'/download/(\d+)', base_url)
                if match:
                    category_id = match.group(1)
                    self.log(f"   ğŸ” æå–åˆ°åˆ†ç±»ID: {category_id}")
                    
                    # å°è¯•æ„å»ºå‰å‡ é¡µçš„URL
                    for page_num in range(2, 6):  # å°è¯•2-5é¡µ
                        page_url = f"{self.base_url}/index.php/download/{category_id}/{page_num}"
                        
                        # æ£€æŸ¥é¡µé¢æ˜¯å¦å­˜åœ¨ï¼ˆç®€å•éªŒè¯ï¼‰
                        try:
                            test_response = requests.head(page_url, timeout=5)
                            if test_response.status_code == 200:
                                pagination_links.append({
                                    'page': page_num,
                                    'url': page_url,
                                    'text': str(page_num)
                                })
                                self.log(f"   ğŸ“„ æ‰‹åŠ¨æ„å»ºç¬¬{page_num}é¡µ: {page_url}")
                        except:
                            continue
                    
                    if pagination_links:
                        self.log(f"ğŸ“„ æ‰‹åŠ¨æ„å»ºæ‰¾åˆ° {len(pagination_links)} ä¸ªåˆ†é¡µé“¾æ¥")
            
            if not pagination_links:
                self.log(f"â„¹ï¸ é¡µé¢ä¸­æœªæ‰¾åˆ°åˆ†é¡µå¯¼èˆª")
                
        except Exception as e:
            self.log(f"âŒ æŸ¥æ‰¾åˆ†é¡µé“¾æ¥æ—¶å‡ºé”™: {str(e)}")
        
        return pagination_links
    
    def process_category_with_pagination(self, module_name, category):
        """å¤„ç†åˆ†ç±»é¡µé¢ï¼ŒåŒ…æ‹¬åˆ†é¡µå†…å®¹"""
        category_name = category['name']
        category_url = category['url']
        
        if category_url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†åˆ†ç±»: {category_name}")
            return
        
        self.log(f"ğŸ“‹ å¤„ç†åˆ†ç±»: {module_name} -> {category_name}")
        
        # å¤„ç†ç¬¬ä¸€é¡µ
        soup = self.visit_page(category_url)
        if not soup:
            return
        
        # æŸ¥æ‰¾åˆ†é¡µé“¾æ¥
        pagination_links = self.find_pagination_links(soup, category_url)
        
        # åˆ›å»ºæ¨¡å—ç›®å½•
        safe_category_name = category_name.replace('/', '_').replace('\\', '_')
        folder_path = os.path.join(self.base_dir, module_name, safe_category_name)
        
        total_downloads = 0
        
        # å¤„ç†ç¬¬ä¸€é¡µ
        self.log(f"ğŸ“„ å¤„ç†ç¬¬1é¡µ: {category_url}")
        downloads_page1 = self.find_download_links(soup, category_url)
        if downloads_page1:
            total_downloads += len(downloads_page1)
            self.log(f"ğŸš€ ç¬¬1é¡µæ‰¾åˆ° {len(downloads_page1)} ä¸ªæ–‡ä»¶")
            self.process_downloads(downloads_page1, folder_path)
        
        # å¤„ç†å…¶ä»–åˆ†é¡µ
        for page_info in pagination_links:
            page_url = page_info['url']
            page_num = page_info['page']
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡æ­¤åˆ†é¡µ
            if page_url in self.processed_urls:
                self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†åˆ†é¡µ: ç¬¬{page_num}é¡µ")
                continue
            
            self.log(f"ğŸ“„ å¤„ç†ç¬¬{page_num}é¡µ: {page_url}")
            
            # è®¿é—®åˆ†é¡µ
            page_soup = self.visit_page(page_url)
            if not page_soup:
                continue
            
            # æŸ¥æ‰¾åˆ†é¡µä¸­çš„ä¸‹è½½é“¾æ¥
            page_downloads = self.find_download_links(page_soup, page_url)
            if page_downloads:
                total_downloads += len(page_downloads)
                self.log(f"ğŸš€ ç¬¬{page_num}é¡µæ‰¾åˆ° {len(page_downloads)} ä¸ªæ–‡ä»¶")
                self.process_downloads(page_downloads, folder_path)
            
            # æ ‡è®°åˆ†é¡µä¸ºå·²å¤„ç†
            self.processed_urls.add(page_url)
            
            # åˆ†é¡µé—´å»¶è¿Ÿ
            time.sleep(2)
        
        if total_downloads > 0:
            self.log(f"âœ… åˆ†ç±» {category_name} å¤„ç†å®Œæˆï¼Œå…±æ‰¾åˆ° {total_downloads} ä¸ªæ–‡ä»¶")
        else:
            self.log(f"âš ï¸ åˆ†ç±» {category_name} æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
        
        # æ ‡è®°ä¸»åˆ†ç±»ä¸ºå·²å¤„ç†
        self.processed_urls.add(category_url)
    
    def process_downloads(self, downloads, folder_path):
        """å¤„ç†ä¸‹è½½åˆ—è¡¨"""
        for download in downloads:
            try:
                title = download['title']
                url = download['url']
                file_type = download.get('type', 'unknown')
                
                # å¤„ç†è§†é¢‘é¡µé¢
                if file_type == 'video_page':
                    actual_video_url = self.process_video_page(url, title, folder_path)
                    if actual_video_url and actual_video_url != url:
                        # å¦‚æœæ‰¾åˆ°äº†å®é™…è§†é¢‘URLï¼Œæ›´æ–°ä¸‹è½½ä¿¡æ¯
                        url = actual_video_url
                        file_type = 'video_direct'
                
                if file_type in ['video_link', 'video_page', 'video_direct']:
                    # ä¿å­˜è§†é¢‘ä¿¡æ¯
                    video_info = {
                        'title': title,
                        'url': url,
                        'original_url': download['url'] if url != download['url'] else url,
                        'category': os.path.basename(folder_path),
                        'module': os.path.basename(os.path.dirname(folder_path)),
                        'crawl_time': datetime.now().isoformat()
                    }
                    
                elif file_type in ['direct_download', 'document_link', 'embedded_document']:
                    # ä¸‹è½½æ–‡ä»¶
                    filename = self.generate_clean_filename(
                        title, 
                        download.get('filename', ''), 
                        url
                    )
                    
                    self.download_file(url, filename, folder_path)
                    
            except Exception as e:
                self.log(f"âŒ å¤„ç†ä¸‹è½½é¡¹æ—¶å‡ºé”™: {str(e)}")
                continue
            
            time.sleep(1)  # ä¸‹è½½é—´éš”
    
    def process_video_page(self, video_url, title, folder_path=None):
        """å¤„ç†è§†é¢‘é¡µé¢ï¼Œæå–å®é™…è§†é¢‘ä¸‹è½½é“¾æ¥"""
        try:
            self.log(f"ğŸ¥ å¤„ç†è§†é¢‘é¡µé¢: {title}")
            soup = self.visit_page(video_url)
            
            if not soup:
                return None
            
            # æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶é“¾æ¥
            video_sources = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾videoæ ‡ç­¾çš„src
            video_tags = soup.find_all('video')
            for video in video_tags:
                src = video.get('src', '')
                if src:
                    full_url = src if src.startswith('http') else urljoin(video_url, src)
                    video_sources.append(full_url)
                    self.log(f"   ğŸ“¹ æ‰¾åˆ°videoæ ‡ç­¾src: {full_url}")
                
                # æŸ¥æ‰¾sourceæ ‡ç­¾ - é‡ç‚¹å¤„ç†è¿™ç§ç»“æ„
                sources = video.find_all('source')
                for source in sources:
                    src = source.get('src', '')
                    if src:
                        full_url = src if src.startswith('http') else urljoin(video_url, src)
                        video_sources.append(full_url)
                        self.log(f"   ğŸ“¹ æ‰¾åˆ°sourceæ ‡ç­¾src: {full_url}")
            
            # æ–¹æ³•2: å•ç‹¬æŸ¥æ‰¾sourceæ ‡ç­¾ï¼ˆé˜²æ­¢åµŒå¥—é—æ¼ï¼‰
            all_sources = soup.find_all('source', {'type': 'video/mp4'})
            for source in all_sources:
                src = source.get('src', '')
                if src:
                    full_url = src if src.startswith('http') else urljoin(video_url, src)
                    if full_url not in video_sources:
                        video_sources.append(full_url)
                        self.log(f"   ğŸ“¹ æ‰¾åˆ°ç‹¬ç«‹sourceæ ‡ç­¾: {full_url}")
            
            # æ–¹æ³•3: æŸ¥æ‰¾iframeä¸­çš„è§†é¢‘
            iframes = soup.find_all('iframe')
            for iframe in iframes:
                src = iframe.get('src', '')
                if src and any(platform in src.lower() for platform in ['youtube', 'youku', 'bilibili']):
                    video_sources.append(src)
                    self.log(f"   ğŸ“¹ æ‰¾åˆ°iframeè§†é¢‘: {src}")
            
            # æ–¹æ³•4: æŸ¥æ‰¾JavaScriptä¸­çš„è§†é¢‘URL
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    # æŸ¥æ‰¾å¸¸è§çš„è§†é¢‘URLæ¨¡å¼ï¼Œç‰¹åˆ«å…³æ³¨å…‰æ´‹ç½‘ç«™çš„uploadè·¯å¾„
                    import re
                    video_patterns = [
                        r'src["\']?\s*:\s*["\']([^"\']+\.(?:mp4|avi|mov|wmv|flv))["\']',
                        r'url["\']?\s*:\s*["\']([^"\']+\.(?:mp4|avi|mov|wmv|flv))["\']',
                        r'["\']([^"\']*\.(?:mp4|avi|mov|wmv|flv))["\']',
                        # ç‰¹åˆ«é’ˆå¯¹å…‰æ´‹ç½‘ç«™çš„uploadè·¯å¾„
                        r'["\']([^"\']*upload[^"\']*\.mp4)["\']',
                        r'["\']([^"\']*\/upload\/[^"\']*\.mp4)["\']'
                    ]
                    
                    for pattern in video_patterns:
                        matches = re.findall(pattern, script.string, re.IGNORECASE)
                        for match in matches:
                            full_url = match if match.startswith('http') else urljoin(video_url, match)
                            if full_url not in video_sources:
                                video_sources.append(full_url)
                                self.log(f"   ğŸ“¹ æ‰¾åˆ°JSä¸­çš„è§†é¢‘: {full_url}")
            
            # æ–¹æ³•5: æŸ¥æ‰¾é¡µé¢HTMLä¸­çš„uploadè·¯å¾„è§†é¢‘
            import re
            page_content = str(soup)
            upload_patterns = [
                r'(["\']?[^"\']*upload[^"\']*\.mp4["\']?)',
                r'(["\']?\/upload\/[^"\']*\.mp4["\']?)',
                r'(upload\/[^"\']*\.mp4)'
            ]
            
            for pattern in upload_patterns:
                matches = re.findall(pattern, page_content, re.IGNORECASE)
                for match in matches:
                    # æ¸…ç†å¼•å·
                    clean_match = match.strip('\'"')
                    full_url = clean_match if clean_match.startswith('http') else urljoin(video_url, clean_match)
                    if full_url not in video_sources and '.mp4' in full_url:
                        video_sources.append(full_url)
                        self.log(f"   ğŸ“¹ æ‰¾åˆ°uploadè·¯å¾„è§†é¢‘: {full_url}")
            
            # å»é‡å¹¶è¿‡æ»¤æœ‰æ•ˆçš„MP4é“¾æ¥
            valid_sources = []
            for source in video_sources:
                if source not in valid_sources and source.endswith('.mp4'):
                    valid_sources.append(source)
            
            if valid_sources:
                self.log(f"   âœ… æ‰¾åˆ° {len(valid_sources)} ä¸ªæœ‰æ•ˆè§†é¢‘æº")
                # ç›´æ¥ä¸‹è½½è§†é¢‘æ–‡ä»¶
                video_source = valid_sources[0]
                filename = self.generate_clean_filename(title, url=video_source)
                
                # åˆ›å»ºè§†é¢‘ä¸‹è½½ç›®å½•
                video_dir = folder_path if folder_path else os.path.join(self.base_dir, "è§†é¢‘")
                
                # ä¸‹è½½è§†é¢‘æ–‡ä»¶
                success = self.download_file(video_source, filename, video_dir)
                if success:
                    return video_source
                else:
                    self.log(f"âŒ è§†é¢‘ä¸‹è½½å¤±è´¥: {title}")
                    return None
            else:
                # è°ƒè¯•ï¼šä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿åˆ†æ
                if self.debug:
                    debug_file = f"debug_video_page_{int(time.time())}.html"
                    debug_path = os.path.join(self.base_dir, debug_file)
                    with open(debug_path, 'w', encoding='utf-8') as f:
                        f.write(str(soup))
                    self.log(f"   ğŸ” å·²ä¿å­˜é¡µé¢å†…å®¹åˆ°: {debug_path}")
                
                self.log(f"   âŒ æœªæ‰¾åˆ°è§†é¢‘æºï¼Œè¿”å›é¡µé¢é“¾æ¥")
                return video_url  # å¦‚æœæ‰¾ä¸åˆ°ç›´æ¥è§†é¢‘ï¼Œè¿”å›é¡µé¢é“¾æ¥
                
        except Exception as e:
            self.log(f"âŒ å¤„ç†è§†é¢‘é¡µé¢æ—¶å‡ºé”™: {str(e)}")
            return video_url

    def download_file(self, url, filename, folder_path):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            # åˆ›å»ºç›®å½•
            os.makedirs(folder_path, exist_ok=True)
            file_path = os.path.join(folder_path, filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å¤§å°åˆç†ï¼ˆé¿å…ä¸‹è½½æŸåçš„æ–‡ä»¶ï¼‰
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                if file_size > 1024:  # å¦‚æœæ–‡ä»¶å¤§äº1KBï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆæ–‡ä»¶
                    self.log(f"ğŸ“ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                    return True
                else:
                    self.log(f"ğŸ”„ æ–‡ä»¶å­˜åœ¨ä½†å¤§å°å¼‚å¸¸({file_size}å­—èŠ‚)ï¼Œé‡æ–°ä¸‹è½½: {filename}")
                    os.remove(file_path)
            
            # ä¸‹è½½æ–‡ä»¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': self.base_url
            }
            
            response = requests.get(url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(file_path)
            self.log(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} ({file_size} bytes)")
            
            self.new_files.append({
                'filename': filename,
                'path': file_path,
                'url': url,
                'size': file_size
            })
            
            return True
            
        except Exception as e:
            self.log(f"âŒ ä¸‹è½½å¤±è´¥ {filename}: {str(e)}")
            return False
    
    def generate_clean_filename(self, title, download_filename="", url=""):
        """ç”Ÿæˆæ¸…æ´çš„æ–‡ä»¶å"""
        try:
            # å¦‚æœæœ‰æŒ‡å®šçš„ä¸‹è½½æ–‡ä»¶åï¼Œä¼˜å…ˆä½¿ç”¨
            if download_filename:
                return download_filename
            
            # å¦åˆ™ä»æ ‡é¢˜ç”Ÿæˆæ–‡ä»¶å
            # ä¿ç•™"/"å­—ç¬¦ï¼Œä½†æ›¿æ¢å…¶ä»–ç‰¹æ®Šå­—ç¬¦
            clean_title = re.sub(r'[^\w\s\-\u4e00-\u9fff\/]', '', title)
            clean_title = re.sub(r'\s+', '_', clean_title.strip())
            
            # æˆªæ–­è¿‡é•¿çš„æ–‡ä»¶å
            if len(clean_title) > 100:
                clean_title = clean_title[:100]
            
            # ä»URLè·å–æ–‡ä»¶æ‰©å±•å
            if url:
                parsed_url = urlparse(url)
                path = parsed_url.path
                ext = os.path.splitext(path)[1]
                
                if not ext:
                    ext = '.pdf'  # é»˜è®¤ä¸ºPDF
            else:
                ext = '.pdf'
            
            return f"{clean_title}{ext}"
            
        except Exception as e:
            self.log(f"âš ï¸ æ–‡ä»¶åç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"document_{int(time.time())}.pdf"
    

    def process_category_page(self, module_name, category):
        """å¤„ç†åˆ†ç±»é¡µé¢ï¼ˆç°åœ¨è°ƒç”¨åˆ†é¡µå¤„ç†æ–¹æ³•ï¼‰"""
        # ä½¿ç”¨æ–°çš„åˆ†é¡µå¤„ç†æ–¹æ³•
        self.process_category_with_pagination(module_name, category)
    
    def clear_video_module_progress(self):
        """æ¸…é™¤è§†é¢‘æ¨¡å—çš„è¿›åº¦è®°å½•ï¼Œå…è®¸é‡æ–°çˆ¬å–"""
        try:
            # æ‰¾åˆ°æ‰€æœ‰è§†é¢‘ç›¸å…³çš„URLå¹¶æ¸…é™¤
            video_urls_to_clear = []
            
            # æ•™å­¦è§†é¢‘æ¨¡å—çš„ä¸»é¡µ
            video_module = None
            for module in self.main_modules:
                if module['name'] == 'æ•™å­¦è§†é¢‘':
                    video_module = module
                    break
            
            if video_module:
                # æ·»åŠ ä¸»é¡µé¢URL
                video_urls_to_clear.append(video_module['url'])
                
                # æ·»åŠ æ‰€æœ‰åˆ†ç±»URL
                for category in video_module['categories']:
                    video_urls_to_clear.append(category['url'])
                
                # ä»å·²å¤„ç†åˆ—è¡¨ä¸­ç§»é™¤è¿™äº›URL
                for url in video_urls_to_clear:
                    if url in self.processed_urls:
                        self.processed_urls.remove(url)
                        self.log(f"ğŸ”„ æ¸…é™¤è¿›åº¦è®°å½•: {url}")
                
                self.log(f"âœ… å·²æ¸…é™¤ {len(video_urls_to_clear)} ä¸ªè§†é¢‘æ¨¡å—çš„è¿›åº¦è®°å½•")
                return True
            else:
                self.log("âŒ æœªæ‰¾åˆ°æ•™å­¦è§†é¢‘æ¨¡å—")
                return False
                
        except Exception as e:
            self.log(f"âŒ æ¸…é™¤è§†é¢‘æ¨¡å—è¿›åº¦æ—¶å‡ºé”™: {str(e)}")
            return False
    
    def process_main_page(self, module):
        """å¤„ç†ä¸»é¡µé¢ï¼ˆå¯èƒ½åŒ…å«ä¸€äº›é€šç”¨ä¸‹è½½ï¼‰"""
        module_name = module['name']
        module_url = module['url']
        
        if module_url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†ä¸»é¡µ: {module_name}")
            return
        
        self.log(f"ğŸŒ å¤„ç†ä¸»é¡µ: {module_name}")
        
        soup = self.visit_page(module_url)
        if not soup:
            return
        
        # æŸ¥æ‰¾ä¸»é¡µé¢çš„ä¸‹è½½é“¾æ¥
        downloads = self.find_download_links(soup, module_url)
        
        if downloads:
            # åˆ›å»ºä¸»æ¨¡å—ç›®å½•ï¼Œä¿å­˜é€šç”¨æ–‡ä»¶åˆ°å¯¹åº”çš„äº§å“åˆ†ç±»
            folder_path = os.path.join(self.base_dir, module_name, "é€šç”¨èµ„æ–™")
            
            self.log(f"ğŸš€ å¤„ç†ä¸»é¡µ {len(downloads)} ä¸ªæ–‡ä»¶åˆ°: {folder_path}")
            
            for download in downloads:
                if download['type'] == 'video_link':
                    # ä¿å­˜è§†é¢‘ä¿¡æ¯
                    video_info = {
                        'title': download['title'],
                        'url': download['url'],
                        'category': "é€šç”¨èµ„æ–™",
                        'module': module_name,
                        'crawl_time': datetime.now().isoformat()
                    }

                    
                elif download['type'] in ['direct_download', 'document_link']:
                    # ä¸‹è½½æ–‡ä»¶
                    filename = self.generate_clean_filename(
                        download['title'], 
                        download.get('filename', ''), 
                        download['url']
                    )
                    
                    self.download_file(download['url'], filename, folder_path)
                
                time.sleep(1)
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        self.processed_urls.add(module_url)
    
    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥"""
        try:
            # è¿™é‡Œéœ€è¦é…ç½®ä½ çš„é’‰é’‰æœºå™¨äººwebhookåœ°å€
            webhook_url = os.environ.get('DINGTALK_WEBHOOK', '')
            
            if not webhook_url:
                self.log("âš ï¸ æœªé…ç½®é’‰é’‰webhookï¼Œè·³è¿‡é€šçŸ¥")
                return
            
            data = {
                "msgtype": "text",
                "text": {
                    "content": message
                }
            }
            
            response = requests.post(webhook_url, json=data, timeout=10)
            
            if response.status_code == 200:
                self.log("âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å‡ºé”™: {str(e)}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        start_time = datetime.now()
        
        try:
            self.log("ğŸš€ å¼€å§‹è¿è¡Œå…‰æ´‹ï¼ˆæ·å¤ªæ ¼ç‰¹ï¼‰æ–‡æ¡£çˆ¬è™«")
            
            # çˆ¬å–æ¯ä¸ªä¸»è¦æ¨¡å—
            for module in self.main_modules:
                module_name = module['name']
                self.log(f"ğŸ“‚ å¼€å§‹å¤„ç†æ¨¡å—: {module_name}")
                
                # å¤„ç†ä¸»é¡µé¢
                self.process_main_page(module)
                time.sleep(2)
                
                # å¤„ç†å„ä¸ªåˆ†ç±»
                for category in module['categories']:
                    self.process_category_page(module_name, category)
                    time.sleep(3)  # åˆ†ç±»é—´å»¶è¿Ÿ
                
                self.log(f"âœ… å®Œæˆæ¨¡å—: {module_name}")
                time.sleep(5)  # æ¨¡å—é—´å»¶è¿Ÿ
            
            # ä¿å­˜è¿›åº¦
            self.save_processed_urls()
            
            # ç»Ÿè®¡ç»“æœ
            end_time = datetime.now()
            duration = end_time - start_time
            total_files = len(self.new_files)
            
            self.log(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±ä¸‹è½½ {total_files} ä¸ªæ–°æ–‡ä»¶ï¼Œè€—æ—¶ {duration}")
            
            # å‘é€é’‰é’‰é€šçŸ¥
            if self.new_files:
                notification_message = f"""å…‰æ´‹ï¼ˆæ·å¤ªæ ¼ç‰¹ï¼‰çˆ¬è™«å®Œæˆï¼
ğŸ“Š çˆ¬å–ç»Ÿè®¡ï¼š
â€¢ æ–°æ–‡ä»¶æ•°é‡ï¼š{total_files}
â€¢ çˆ¬å–è€—æ—¶ï¼š{duration}
â€¢ å®Œæˆæ—¶é—´ï¼š{end_time.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“ æ–°ä¸‹è½½æ–‡ä»¶ï¼ˆå‰10ä¸ªï¼‰ï¼š"""
                
                for i, file_info in enumerate(self.new_files[:10]):
                    notification_message += f"\n{i+1}. {file_info['filename']} ({file_info['size']} bytes)"
                
                if len(self.new_files) > 10:
                    notification_message += f"\n... è¿˜æœ‰ {len(self.new_files) - 10} ä¸ªæ–‡ä»¶"
                
                self.send_dingtalk_notification(notification_message)
                
                self.log("ğŸ“ æ–°ä¸‹è½½çš„æ–‡ä»¶:")
                for file_info in self.new_files[:10]:
                    self.log(f"   ğŸ“„ {file_info['filename']} ({file_info['size']} bytes)")
                
                if len(self.new_files) > 10:
                    self.log(f"   ... è¿˜æœ‰ {len(self.new_files) - 10} ä¸ªæ–‡ä»¶")
            else:
                notification_message = f"""å…‰æ´‹ï¼ˆæ·å¤ªæ ¼ç‰¹ï¼‰çˆ¬è™«å®Œæˆï¼
ğŸ“Š æœ¬æ¬¡æœªå‘ç°æ–°æ–‡ä»¶
â€¢ çˆ¬å–è€—æ—¶ï¼š{duration}
â€¢ å®Œæˆæ—¶é—´ï¼š{end_time.strftime('%Y-%m-%d %H:%M:%S')}"""
                
                self.send_dingtalk_notification(notification_message)
                self.log("â„¹ï¸ æœ¬æ¬¡çˆ¬å–æœªå‘ç°æ–°æ–‡ä»¶")
            
        except Exception as e:
            error_message = f"å…‰æ´‹ï¼ˆæ·å¤ªæ ¼ç‰¹ï¼‰çˆ¬è™«è¿è¡Œå‡ºé”™ï¼š{str(e)}"
            self.log(f"âŒ {error_message}")
            self.send_dingtalk_notification(error_message)
            
        finally:
            # å…³é—­WebDriver
            if self.driver:
                self.driver.quit()
                self.log("ğŸ”’ WebDriverå·²å…³é—­")

def test_single_category(category_url=None):
    """æµ‹è¯•å•ä¸ªåˆ†ç±»çš„çˆ¬å–åŠŸèƒ½"""
    spider = JTEKTSpider()
    
    try:
        # é»˜è®¤æµ‹è¯•URL
        test_url = category_url or "https://www.jtektele.com.cn/index.php/download/8"
        
        spider.log(f"ğŸ§ª æµ‹è¯•å•åˆ†ç±»çˆ¬å–åŠŸèƒ½")
        spider.log(f"ğŸ“‹ æµ‹è¯•URL: {test_url}")
        
        # åˆ›å»ºæµ‹è¯•åˆ†ç±»é…ç½®
        test_category = {
            'name': 'æµ‹è¯•åˆ†ç±»',
            'url': test_url
        }
        
        # å¤„ç†æµ‹è¯•åˆ†ç±»
        spider.process_category_page("æµ‹è¯•æ¨¡å—", test_category)
        
        if spider.new_files:
            spider.log(f"âœ… æµ‹è¯•æˆåŠŸï¼æ‰¾åˆ° {len(spider.new_files)} ä¸ªæ–‡ä»¶")
            for file_info in spider.new_files:
                spider.log(f"   ğŸ“„ {file_info['filename']}")
        else:
            spider.log(f"âš ï¸ æµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ–°æ–‡ä»¶")
        
    except Exception as e:
        spider.log(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    finally:
        if spider.driver:
            spider.driver.quit()

def test_pagination(category_url=None):
    """æµ‹è¯•åˆ†é¡µåŠŸèƒ½"""
    spider = JTEKTSpider()
    
    try:
        # é»˜è®¤æµ‹è¯•NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™
        test_url = category_url or "https://www.jtektele.com.cn/index.php/download/9"
        
        spider.log(f"ğŸ§ª æµ‹è¯•åˆ†é¡µåŠŸèƒ½")
        spider.log(f"ğŸ“‹ æµ‹è¯•URL: {test_url}")
        
        # åˆ›å»ºæµ‹è¯•åˆ†ç±»é…ç½®
        test_category = {
            'name': 'NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™',
            'url': test_url
        }
        
        # å¤„ç†æµ‹è¯•åˆ†ç±»ï¼ˆåŒ…å«åˆ†é¡µï¼‰
        spider.process_category_with_pagination("æµ‹è¯•æ¨¡å—", test_category)
        
        if spider.new_files:
            spider.log(f"âœ… åˆ†é¡µæµ‹è¯•æˆåŠŸï¼æ‰¾åˆ° {len(spider.new_files)} ä¸ªæ–‡ä»¶")
            for file_info in spider.new_files:
                spider.log(f"   ğŸ“„ {file_info['filename']}")
        else:
            spider.log(f"âš ï¸ åˆ†é¡µæµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ–°æ–‡ä»¶")
        
    except Exception as e:
        spider.log(f"âŒ åˆ†é¡µæµ‹è¯•å¤±è´¥: {str(e)}")
    finally:
        if spider.driver:
            spider.driver.quit()

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼
        test_url = sys.argv[2] if len(sys.argv) > 2 else None
        test_single_category(test_url)
    elif len(sys.argv) > 1 and sys.argv[1] == "test_pagination":
        # åˆ†é¡µæµ‹è¯•æ¨¡å¼
        test_url = sys.argv[2] if len(sys.argv) > 2 else None
        test_pagination(test_url)
    elif len(sys.argv) > 1 and sys.argv[1] == "test_video":
        # è§†é¢‘æµ‹è¯•æ¨¡å¼
        spider = JTEKTSpider()
        try:
            # æµ‹è¯•è§†é¢‘é¡µé¢å¤„ç†
            test_video_url = "https://www.jtektele.com.cn/index.php/videoshow/2"  # NK1å·¥ç¨‹æ•°æ®è¯»å‡º
            spider.log("ğŸ§ª æµ‹è¯•è§†é¢‘é¡µé¢å¤„ç†åŠŸèƒ½")
            spider.log(f"ğŸ“¹ æµ‹è¯•è§†é¢‘URL: {test_video_url}")
            
            # å¤„ç†è§†é¢‘é¡µé¢
            video_source = spider.process_video_page(test_video_url, "NK1ç³»åˆ—PLCæ•™å­¦è§†é¢‘")
            
            if video_source and video_source.endswith('.mp4'):
                spider.log(f"âœ… è§†é¢‘æµ‹è¯•æˆåŠŸï¼è§†é¢‘å·²ä¸‹è½½å®Œæˆ")
                
            else:
                spider.log("âš ï¸ è§†é¢‘æµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æœ‰æ•ˆçš„MP4è§†é¢‘æº")
                
        except Exception as e:
            spider.log(f"âŒ è§†é¢‘æµ‹è¯•å¤±è´¥: {str(e)}")
        finally:
            if spider.driver:
                spider.driver.quit()
                spider.log("ğŸ”’ WebDriverå·²å…³é—­")
    elif len(sys.argv) > 1 and sys.argv[1] == "videos":
        # è§†é¢‘æ¨¡å—çˆ¬å–æ¨¡å¼
        spider = JTEKTSpider()
        try:
            spider.log("ğŸš€ å¼€å§‹è¿è¡Œå…‰æ´‹ï¼ˆæ·å¤ªæ ¼ç‰¹ï¼‰æ•™å­¦è§†é¢‘çˆ¬è™«")
            
            # åªå¤„ç†æ•™å­¦è§†é¢‘æ¨¡å—
            video_module = None
            for module in spider.main_modules:
                if module['name'] == 'æ•™å­¦è§†é¢‘':
                    video_module = module
                    break
            
            if video_module:
                spider.log(f"ğŸ“‚ å¼€å§‹å¤„ç†æ¨¡å—: {video_module['name']}")
                
                # å¤„ç†ä¸»é¡µé¢
                spider.process_main_page(video_module)
                time.sleep(2)
                
                # å¤„ç†å„ä¸ªåˆ†ç±»
                for category in video_module['categories']:
                    spider.process_category_page(video_module['name'], category)
                    time.sleep(3)  # åˆ†ç±»é—´å»¶è¿Ÿ
                
                spider.log("âœ… æ•™å­¦è§†é¢‘æ¨¡å—å¤„ç†å®Œæˆ")
            else:
                spider.log("âŒ æœªæ‰¾åˆ°æ•™å­¦è§†é¢‘æ¨¡å—é…ç½®")
                
        except Exception as e:
            spider.log(f"âŒ è§†é¢‘çˆ¬å–å¤±è´¥: {str(e)}")
        finally:
            if spider.driver:
                spider.driver.quit()
                spider.log("ğŸ”’ WebDriverå·²å…³é—­")
    elif len(sys.argv) > 1 and sys.argv[1] == "videos_force":
        # å¼ºåˆ¶é‡æ–°çˆ¬å–è§†é¢‘æ¨¡å—æ¨¡å¼
        spider = JTEKTSpider()
        try:
            spider.log("ğŸš€ å¼€å§‹å¼ºåˆ¶é‡æ–°çˆ¬å–å…‰æ´‹ï¼ˆæ·å¤ªæ ¼ç‰¹ï¼‰æ•™å­¦è§†é¢‘")
            
            # æ¸…é™¤è§†é¢‘æ¨¡å—çš„è¿›åº¦è®°å½•
            if spider.clear_video_module_progress():
                spider.log("ğŸ”„ è¿›åº¦è®°å½•å·²æ¸…é™¤ï¼Œå¼€å§‹é‡æ–°çˆ¬å–")
                
                # åªå¤„ç†æ•™å­¦è§†é¢‘æ¨¡å—
                video_module = None
                for module in spider.main_modules:
                    if module['name'] == 'æ•™å­¦è§†é¢‘':
                        video_module = module
                        break
                
                if video_module:
                    spider.log(f"ğŸ“‚ å¼€å§‹å¤„ç†æ¨¡å—: {video_module['name']}")
                    
                    # å¤„ç†ä¸»é¡µé¢
                    spider.process_main_page(video_module)
                    time.sleep(2)
                    
                    # å¤„ç†å„ä¸ªåˆ†ç±»
                    for category in video_module['categories']:
                        spider.process_category_page(video_module['name'], category)
                        time.sleep(3)  # åˆ†ç±»é—´å»¶è¿Ÿ
                    
                    spider.log("âœ… æ•™å­¦è§†é¢‘æ¨¡å—é‡æ–°çˆ¬å–å®Œæˆ")
                else:
                    spider.log("âŒ æœªæ‰¾åˆ°æ•™å­¦è§†é¢‘æ¨¡å—é…ç½®")
            else:
                spider.log("âŒ æ¸…é™¤è¿›åº¦è®°å½•å¤±è´¥")
                
        except Exception as e:
            spider.log(f"âŒ å¼ºåˆ¶è§†é¢‘çˆ¬å–å¤±è´¥: {str(e)}")
        finally:
            if spider.driver:
                spider.driver.quit()
                spider.log("ğŸ”’ WebDriverå·²å…³é—­")
    elif len(sys.argv) > 1 and sys.argv[1] == "nk1_pagination":
        # ä¸“é—¨æµ‹è¯•NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™çš„åˆ†é¡µåŠŸèƒ½
        spider = JTEKTSpider()
        try:
            spider.log("ğŸš€ å¼€å§‹æµ‹è¯•NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™åˆ†é¡µåŠŸèƒ½")
            
            # æŸ¥æ‰¾NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™åˆ†ç±»
            nk1_category = None
            for module in spider.main_modules:
                if module['name'] == 'èµ„æ–™ä¸‹è½½':
                    for category in module['categories']:
                        if 'NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™' in category['name']:
                            nk1_category = category
                            break
                    if nk1_category:
                        break
            
            if nk1_category:
                spider.log(f"ğŸ“‚ æ‰¾åˆ°åˆ†ç±»: {nk1_category['name']}")
                spider.log(f"ğŸ”— åˆ†ç±»URL: {nk1_category['url']}")
                
                # æ¸…é™¤æ­¤åˆ†ç±»çš„è¿›åº¦è®°å½•
                if nk1_category['url'] in spider.processed_urls:
                    spider.processed_urls.remove(nk1_category['url'])
                    spider.log("ğŸ”„ å·²æ¸…é™¤åˆ†ç±»è¿›åº¦è®°å½•")
                
                # å¤„ç†åˆ†ç±»ï¼ˆåŒ…å«åˆ†é¡µï¼‰
                spider.process_category_with_pagination("èµ„æ–™ä¸‹è½½", nk1_category)
                
                if spider.new_files:
                    spider.log(f"âœ… NK1åˆ†é¡µæµ‹è¯•æˆåŠŸï¼å…±æ‰¾åˆ° {len(spider.new_files)} ä¸ªæ–‡ä»¶")
                    for file_info in spider.new_files:
                        spider.log(f"   ğŸ“„ {file_info['filename']} ({file_info['size']} bytes)")
                else:
                    spider.log("âš ï¸ NK1åˆ†é¡µæµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ–°æ–‡ä»¶")
            else:
                spider.log("âŒ æœªæ‰¾åˆ°NK0/NK1ç³»åˆ—PLCæŠ€æœ¯èµ„æ–™åˆ†ç±»")
                
        except Exception as e:
            spider.log(f"âŒ NK1åˆ†é¡µæµ‹è¯•å¤±è´¥: {str(e)}")
        finally:
            if spider.driver:
                spider.driver.quit()
                spider.log("ï¿½ï¿½ WebDriverå·²å…³é—­")
    elif len(sys.argv) > 1 and sys.argv[1] == "encoder_pagination":
        # ä¸“é—¨æµ‹è¯•ç¼–ç å™¨ç›¸å…³æŠ€æœ¯èµ„æ–™çš„åˆ†é¡µåŠŸèƒ½
        spider = JTEKTSpider()
        try:
            spider.log("ğŸš€ å¼€å§‹æµ‹è¯•ç¼–ç å™¨ç›¸å…³æŠ€æœ¯èµ„æ–™åˆ†é¡µåŠŸèƒ½")
            
            # æŸ¥æ‰¾ç¼–ç å™¨ç›¸å…³æŠ€æœ¯èµ„æ–™åˆ†ç±»
            encoder_category = None
            for module in spider.main_modules:
                if module['name'] == 'èµ„æ–™ä¸‹è½½':
                    for category in module['categories']:
                        if 'ç¼–ç å™¨ç›¸å…³æŠ€æœ¯èµ„æ–™' in category['name']:
                            encoder_category = category
                            break
                    if encoder_category:
                        break
            
            if encoder_category:
                spider.log(f"ğŸ“‚ æ‰¾åˆ°åˆ†ç±»: {encoder_category['name']}")
                spider.log(f"ğŸ”— åˆ†ç±»URL: {encoder_category['url']}")
                
                # æ¸…é™¤æ­¤åˆ†ç±»çš„è¿›åº¦è®°å½•
                if encoder_category['url'] in spider.processed_urls:
                    spider.processed_urls.remove(encoder_category['url'])
                    spider.log("ğŸ”„ å·²æ¸…é™¤åˆ†ç±»è¿›åº¦è®°å½•")
                
                # å¤„ç†åˆ†ç±»ï¼ˆåŒ…å«åˆ†é¡µï¼‰
                spider.process_category_with_pagination("èµ„æ–™ä¸‹è½½", encoder_category)
                
                if spider.new_files:
                    spider.log(f"âœ… ç¼–ç å™¨åˆ†é¡µæµ‹è¯•æˆåŠŸï¼å…±æ‰¾åˆ° {len(spider.new_files)} ä¸ªæ–‡ä»¶")
                    for file_info in spider.new_files:
                        spider.log(f"   ğŸ“„ {file_info['filename']} ({file_info['size']} bytes)")
                else:
                    spider.log("âš ï¸ ç¼–ç å™¨åˆ†é¡µæµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ–°æ–‡ä»¶")
            else:
                spider.log("âŒ æœªæ‰¾åˆ°ç¼–ç å™¨ç›¸å…³æŠ€æœ¯èµ„æ–™åˆ†ç±»")
                
        except Exception as e:
            spider.log(f"âŒ ç¼–ç å™¨åˆ†é¡µæµ‹è¯•å¤±è´¥: {str(e)}")
        finally:
            if spider.driver:
                spider.driver.quit()
                spider.log("ğŸ”’ WebDriverå·²å…³é—­")
    else:
        # æ­£å¸¸è¿è¡Œ
        spider = JTEKTSpider()
        spider.run()
