#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import json
import pickle
import platform
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote
import requests
import chardet

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException
from bs4 import BeautifulSoup

class NordSeleniumSpiderV2:
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.base_url = "https://www.nord.cn"
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/è¯ºå¾·"
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "downloads", "è¯ºå¾·")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.base_dir, exist_ok=True)
        
        self.processed_urls = self.load_processed_urls()
        self.new_files = []
        self.debug = True
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆå…¨é‡çˆ¬å–ï¼‰
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_urls.pkl'))
        
        # åˆå§‹åŒ–WebDriver
        self.driver = None
        self.init_webdriver()
        
    def init_webdriver(self):
        """åˆå§‹åŒ–Chrome WebDriver"""
        try:
            # æ£€æµ‹ç³»ç»Ÿæ¶æ„
            system = platform.system()
            machine = platform.machine()
            
            # ç¡®å®šchromedriverè·¯å¾„
            # åœ¨æœåŠ¡å™¨ä¸ŠæŸ¥æ‰¾chromedriverç›®å½•
            if os.path.exists("/srv/crawler/chromedriver_downloads"):
                chromedriver_dir = "/srv/crawler/chromedriver_downloads"
            else:
                chromedriver_dir = os.path.join(os.getcwd(), "chromedriver_downloads")
            
            if system == "Darwin":  # macOS
                if machine == "arm64":
                    chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_mac-arm64", "chromedriver-mac-arm64", "chromedriver")
                else:
                    chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_mac-x64", "chromedriver-mac-x64", "chromedriver")
            elif system == "Linux":
                chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_linux64", "chromedriver-linux64", "chromedriver")
            elif system == "Windows":
                chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_win64", "chromedriver-win64", "chromedriver.exe")
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ: {system}")
            
            if not os.path.exists(chromedriver_path):
                raise Exception(f"ChromeDriveræœªæ‰¾åˆ°: {chromedriver_path}")
            
            # è®¾ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # åˆå§‹åŒ–WebDriver
            service = Service(executable_path=chromedriver_path)
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(10)
            
            self.log("âœ… Chrome WebDriveråˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.log(f"âŒ WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def should_download_language(self, language):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸‹è½½è¯¥è¯­è¨€çš„æ–‡æ¡£"""
        if not language or language == "æœªçŸ¥è¯­è¨€":
            return True  # å¯¹äºæœªçŸ¥è¯­è¨€çš„æ–‡æ¡£ï¼Œé»˜è®¤ä¸‹è½½ï¼ˆå¯èƒ½æ˜¯è‹±æ–‡æˆ–ä¸­æ–‡ï¼‰
        
        # åªä¸‹è½½è‹±è¯­å’Œä¸­æ–‡æ–‡æ¡£
        target_languages = [
            'è‹±è¯­', 'English', 'EN', 'en',
            'ä¸­æ–‡', 'æ±‰è¯­', 'Chinese', 'ZH', 'zh', 'CN', 'cn',
            'ç¾å¼è‹±è¯­', 'American English', 'US English'
        ]
        
        return any(target_lang in language for target_lang in target_languages)
    
    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URLåˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_urls.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_urls(self):
        """ä¿å­˜å·²å¤„ç†çš„URLåˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_urls.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_urls, f)
    
    def visit_page(self, url, retry_count=3):
        """è®¿é—®é¡µé¢å¹¶è¿”å›BeautifulSoupå¯¹è±¡"""
        for attempt in range(retry_count):
            try:
                self.log(f"ğŸ”„ è®¿é—®é¡µé¢ (å°è¯•{attempt+1}/{retry_count}): {url}")
                self.driver.get(url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                WebDriverWait(self.driver, 30).until(
                    lambda driver: driver.execute_script("return document.readyState") == "complete"
                )
                
                # é¢å¤–ç­‰å¾…JavaScriptæ‰§è¡Œ
                time.sleep(3)
                
                # å°è¯•æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                # è·å–é¡µé¢æºç 
                page_source = self.driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')
                
                return soup
                
            except Exception as e:
                self.log(f"âŒ é¡µé¢è®¿é—®å¤±è´¥ (å°è¯•{attempt+1}): {str(e)}")
                time.sleep(5)
        
        self.log(f"âŒ é¡µé¢è®¿é—®å®Œå…¨å¤±è´¥: {url}")
        return None
    
    def get_main_categories(self):
        """è·å–ä¸»è¦äº§å“åˆ†ç±»"""
        try:
            # ä¸ºäº†ç¡®ä¿ç”µæœºåˆ†ç±»èƒ½è¢«çˆ¬å–ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨åˆ†ç±»
            self.log("ğŸ” ç›´æ¥ä½¿ç”¨å¤‡ç”¨åˆ†ç±»ï¼ˆåŒ…å«ç”µæœºåˆ†ç±»ï¼‰")
            return self.get_fallback_categories()
            
        except Exception as e:
            self.log(f"âŒ è·å–åˆ†ç±»å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†ç±»")
            return self.get_fallback_categories()
    
    def get_fallback_categories(self):
        """å¤‡ç”¨åˆ†ç±»åˆ—è¡¨"""
        return [
            {
                'name': 'å‡é€Ÿç”µæœº',
                'url': 'https://www.nord.cn/cn/products/geared-motors/geared-motors.jsp'
            },
            {
                'name': 'é©±åŠ¨ç”µå­è®¾å¤‡',
                'url': 'https://www.nord.cn/cn/products/drive-electronics/drive-electronics.jsp'
            },
            {
                'name': 'å·¥ä¸šé½¿è½®ç®±',
                'url': 'https://www.nord.cn/cn/products/industrial-gear-units/industrial-gear-units.jsp'
            },
            {
                'name': 'ç”µæœº',
                'url': 'https://www.nord.cn/cn/products/motors/motors.jsp'
            }
        ]
    
    def find_product_links(self, soup, base_url):
        """æŸ¥æ‰¾äº§å“é“¾æ¥"""
        product_links = []
        
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾äº§å“å¡ç‰‡ä¸­çš„ä¸»è¦é“¾æ¥
            product_groups = soup.find_all('article', class_='product-group')
            
            for article in product_groups:
                # ä»headerä¸­çš„h3 > aè·å–äº§å“åå’Œé“¾æ¥
                header = article.find('header')
                if header:
                    h3_link = header.find('h3').find('a') if header.find('h3') else None
                    if h3_link:
                        href = h3_link.get('href', '')
                        text = h3_link.get_text().strip()
                        
                        if href and text:
                            # æ„å»ºå®Œæ•´URL
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            elif not href.startswith('http'):
                                # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦åŸºäºå½“å‰é¡µé¢URLæ„å»º
                                full_url = urljoin(base_url, href)
                            else:
                                full_url = href
                            
                            product_links.append({
                                'name': text,
                                'url': full_url
                            })
                            self.log(f"   âœ… æ‰¾åˆ°äº§å“: {text} -> {href}")
            
            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°äº§å“å¡ç‰‡ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•ä½œä¸ºå¤‡ç”¨
            if not product_links:
                self.log("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æŸ¥æ‰¾äº§å“é“¾æ¥")
                all_links = soup.find_all('a', href=True)
                
                for link in all_links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # è¿‡æ»¤æ¡ä»¶ï¼š
                    # 1. æ˜¯äº§å“ç›¸å…³é“¾æ¥
                    # 2. æœ‰å®é™…æ–‡æœ¬å†…å®¹
                    # 3. ä¸æ˜¯åˆ†ç±»é¡µé¢æœ¬èº«
                    if (href.startswith('/cn/products/') and 
                        len(text) > 2 and 
                        text not in ['äº§å“å±•ç¤º', 'å‡é€Ÿç”µæœº', 'ç”µæœº', 'å·¥ä¸šé½¿è½®ç®±', 'äº§å“è¯¦æƒ…'] and
                        not href.endswith('geared-motors.jsp') and
                        not href.endswith('motors.jsp') and
                        not href.endswith('drive-electronics.jsp')):
                        
                        full_url = urljoin(base_url, href)
                        
                        # é¿å…é‡å¤
                        if not any(p['url'] == full_url for p in product_links):
                            product_links.append({
                                'name': text,
                                'url': full_url
                            })
            
            self.log(f"ğŸ” æ‰¾åˆ° {len(product_links)} ä¸ªäº§å“é“¾æ¥")
            
        except Exception as e:
            self.log(f"âŒ æŸ¥æ‰¾äº§å“é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
        
        return product_links
    
    def find_sub_products(self, soup, base_url):
        """æŸ¥æ‰¾å­äº§å“é“¾æ¥"""
        sub_products = []
        
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾"äº§å“è¯¦æƒ…"æŒ‰é’®é“¾æ¥ - é’ˆå¯¹DuoDriveç­‰æ¦‚è§ˆé¡µé¢
            detail_links = soup.find_all('a', string=lambda text: text and 'äº§å“è¯¦æƒ…' in text.strip())
            
            for link in detail_links:
                href = link.get('href', '')
                if href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = urljoin(self.base_url, href)
                    elif not href.startswith('http'):
                        full_url = urljoin(base_url, href)
                    else:
                        full_url = href
                    
                    # ä»é¡µé¢æ ‡é¢˜è·å–äº§å“åç§°
                    product_name = "äº§å“è¯¦æƒ…"
                    title_tag = soup.find('h1')
                    if title_tag:
                        product_name = title_tag.get_text().strip() + "_è¯¦æƒ…"
                    
                    sub_products.append({
                        'name': product_name,
                        'url': full_url
                    })
                    self.log(f"   âœ… æ‰¾åˆ°äº§å“è¯¦æƒ…é¡µ: {product_name} -> {href}")
            
            # æ–¹æ³•2: åœ¨overviewé¡µé¢æŸ¥æ‰¾å…¶ä»–å­äº§å“é“¾æ¥
            if not sub_products:
                all_links = soup.find_all('a', href=True)
                
                for link in all_links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # æŸ¥æ‰¾å­äº§å“é“¾æ¥ï¼ˆé€šå¸¸ä»¥äº§å“åå¼€å¤´ï¼‰
                    if (href.startswith('/cn/products/') and 
                        len(text) > 2 and
                        not href.endswith('-overview.jsp') and  # ä¸æ˜¯æ¦‚è§ˆé¡µé¢
                        not href.endswith('geared-motors.jsp') and  # ä¸æ˜¯ä¸»åˆ†ç±»é¡µé¢
                        text not in ['äº§å“å±•ç¤º', 'å‡é€Ÿç”µæœº', 'ç”µæœº', 'å·¥ä¸šé½¿è½®ç®±']):
                        
                        full_url = urljoin(base_url, href)
                        
                        # ä»URLè·¯å¾„æå–æ›´å‡†ç¡®çš„äº§å“åç§°
                        product_name = self.extract_product_name_from_url(href, text)
                        
                        # é¿å…é‡å¤
                        if not any(sp['url'] == full_url for sp in sub_products):
                            sub_products.append({
                                'name': product_name,
                                'url': full_url
                            })
                            self.log(f"   âœ… æ‰¾åˆ°å­äº§å“: {product_name} -> {href}")
            
            self.log(f"ğŸ” æ‰¾åˆ° {len(sub_products)} ä¸ªå­äº§å“")
            
        except Exception as e:
            self.log(f"âŒ æŸ¥æ‰¾å­äº§å“æ—¶å‡ºé”™: {str(e)}")
        
        return sub_products
    
    def extract_product_name_from_url(self, href, fallback_text):
        """ä»URLè·¯å¾„ä¸­æå–äº§å“åç§°"""
        try:
            # ä»URLè·¯å¾„ä¸­æå–äº§å“åç§°
            path_parts = href.strip('/').split('/')
            
            # å¯»æ‰¾å…·ä½“çš„äº§å“æ–‡ä»¶å
            if len(path_parts) > 0:
                product_file = path_parts[-1]
                
                # å»æ‰.jspæ‰©å±•å
                if product_file.endswith('.jsp'):
                    product_file = product_file[:-4]
                
                # æ ¹æ®ç‰¹å®šçš„äº§å“æ¨¡å¼æå–åç§°
                if 'unicase' in product_file.lower():
                    if 'bevel' in product_file.lower():
                        return "UNICASE-ä¼é½¿è½®å‡é€Ÿç”µæœº"
                    elif 'helical' in product_file.lower():
                        return "UNICASE-æ–œé½¿è½®å‡é€Ÿç”µæœº"
                    else:
                        return "UNICASEå‡é€Ÿç”µæœº"
                        
                elif 'nordbloc1' in product_file.lower() or 'nordbloc.1' in product_file.lower():
                    if 'bevel' in product_file.lower():
                        return "NORDBLOC.1-ä¼é½¿è½®å‡é€Ÿç”µæœº"
                    elif 'helical' in product_file.lower():
                        return "NORDBLOC.1-æ–œé½¿è½®å‡é€Ÿç”µæœº"
                    else:
                        return "NORDBLOC.1å‡é€Ÿç”µæœº"
                        
                elif 'standard-helical' in product_file.lower():
                    return "æ ‡å‡†æ–œé½¿è½®å‡é€Ÿç”µæœº"
                    
                elif 'duodrive' in product_file.lower():
                    return "DuoDrive"
                    
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæ¨¡å¼ï¼Œå°è¯•ä»URLä¸­æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†
                else:
                    # å°†è¿å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œé¦–å­—æ¯å¤§å†™
                    formatted_name = product_file.replace('-', ' ').title()
                    if formatted_name and len(formatted_name) > 2:
                        return formatted_name
            
            # å¦‚æœURLæå–å¤±è´¥ï¼Œä½¿ç”¨fallback_text
            return fallback_text + "_è¯¦æƒ…"
            
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›fallbackæ–‡æœ¬
            return fallback_text + "_è¯¦æƒ…"
    
    def get_actual_product_name(self, url):
        """è®¿é—®äº§å“é¡µé¢è·å–çœŸå®çš„äº§å“åç§°"""
        try:
            self.log(f"ğŸ” è·å–äº§å“çœŸå®åç§°: {url}")
            
            soup = self.visit_page(url)
            if not soup:
                return None
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(1)
            
            # é‡æ–°è·å–æœ€æ–°çš„é¡µé¢å†…å®¹
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æ–¹æ³•1: æŸ¥æ‰¾headerä¸­çš„h3æ ‡é¢˜
            header_h3 = soup.find('header', class_='row')
            if header_h3:
                h3_tag = header_h3.find('h3')
                if h3_tag:
                    product_name = h3_tag.get_text().strip()
                    if product_name:
                        self.log(f"   ğŸ“‹ ä»header h3è·å–: {product_name}")
                        return product_name
            
            # æ–¹æ³•2: æŸ¥æ‰¾æ™®é€šçš„h3æ ‡é¢˜
            h3_tags = soup.find_all('h3')
            for h3 in h3_tags:
                text = h3.get_text().strip()
                # ç¡®ä¿æ˜¯äº§å“åç§°ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
                if any(keyword in text for keyword in ['UNICASE', 'NORDBLOC', 'å‡é€Ÿ', 'ç”µæœº', 'é½¿è½®']):
                    self.log(f"   ğŸ“‹ ä»h3æ ‡ç­¾è·å–: {text}")
                    return text
            
            # æ–¹æ³•3: æŸ¥æ‰¾h1æ ‡é¢˜
            h1_tag = soup.find('h1')
            if h1_tag:
                product_name = h1_tag.get_text().strip()
                if product_name:
                    self.log(f"   ğŸ“‹ ä»h1è·å–: {product_name}")
                    return product_name
            
            # æ–¹æ³•4: æŸ¥æ‰¾é¡µé¢æ ‡é¢˜
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text().strip()
                # ä»æ ‡é¢˜ä¸­æå–äº§å“åç§°ï¼ˆå»æ‰ç½‘ç«™åç­‰ï¼‰
                if ' - ' in title_text:
                    product_name = title_text.split(' - ')[0].strip()
                    if product_name:
                        self.log(f"   ğŸ“‹ ä»titleè·å–: {product_name}")
                        return product_name
            
            self.log(f"   âš ï¸ æœªèƒ½è·å–åˆ°äº§å“åç§°")
            return None
            
        except Exception as e:
            self.log(f"   âŒ è·å–äº§å“åç§°æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def click_downloads_tab(self):
        """ç‚¹å‡»ä¸‹è½½æ ‡ç­¾é¡µ"""
        try:
            # æŸ¥æ‰¾ä¸‹è½½æ ‡ç­¾é¡µ
            downloads_tab = self.driver.find_element(By.CSS_SELECTOR, 
                'a[href="#downloads"], #downloads-tab, [aria-controls="downloads"]')
            
            if downloads_tab:
                # æ»šåŠ¨åˆ°æ ‡ç­¾é¡µä½ç½®
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", downloads_tab)
                time.sleep(1)
                
                # ç‚¹å‡»ä¸‹è½½æ ‡ç­¾é¡µ
                downloads_tab.click()
                time.sleep(2)
                self.log("âœ… æˆåŠŸç‚¹å‡»ä¸‹è½½æ ‡ç­¾é¡µ")
                return True
                
        except Exception as e:
            self.log(f"âš ï¸ ç‚¹å‡»ä¸‹è½½æ ‡ç­¾é¡µå¤±è´¥: {str(e)}")
            return False

    def expand_accordion_sections(self):
        """å±•å¼€æ‰‹é£ç´å¼æŠ˜å å†…å®¹"""
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æŠ˜å æŒ‰é’®
            collapse_buttons = self.driver.find_elements(By.CSS_SELECTOR, 
                '[data-toggle="collapse"], .btn.pl-accordion__btn, .collapsed')
            
            self.log(f"ğŸ” æ‰¾åˆ° {len(collapse_buttons)} ä¸ªå¯èƒ½çš„æŠ˜å æŒ‰é’®")
            
            for button in collapse_buttons:
                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æŠ˜å çŠ¶æ€
                    if 'collapsed' in button.get_attribute('class') or \
                       button.get_attribute('aria-expanded') == 'false':
                        
                        # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                        self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                        time.sleep(0.5)
                        
                        # ç‚¹å‡»å±•å¼€
                        button.click()
                        time.sleep(1)
                        self.log(f"âœ… å±•å¼€æŠ˜å åŒºåŸŸ: {button.text[:50]}...")
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            self.log(f"âš ï¸ å±•å¼€æŠ˜å å†…å®¹æ—¶å‡ºé”™: {str(e)}")

    def process_document_detail_page(self, detail_url, title, category, base_folder_path):
        """å¤„ç†æ–‡æ¡£èµ„æ–™è¯¦æƒ…é¡µé¢ï¼Œæå–ä¸‹è½½é“¾æ¥"""
        try:
            self.log(f"ğŸ“‹ å¤„ç†æ–‡æ¡£è¯¦æƒ…é¡µ: {title}")
            
            # è®¿é—®è¯¦æƒ…é¡µé¢
            soup = self.visit_page(detail_url)
            if not soup:
                self.log(f"âŒ æ— æ³•è®¿é—®è¯¦æƒ…é¡µ: {detail_url}")
                return []
            
            downloads = []
            
            # æŸ¥æ‰¾ä¸‹è½½è¡¨æ ¼
            download_tables = soup.find_all('table', class_=lambda x: x and 'pl-table' in x)
            
            for table in download_tables:
                # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦åŒ…å«ä¸‹è½½ç›¸å…³å†…å®¹
                table_text = table.get_text().lower()
                if any(keyword in table_text for keyword in ['ä¸‹è½½', 'pdf', 'download']):
                    
                    # æŸ¥æ‰¾è¡¨æ ¼ä¸­çš„ä¸‹è½½é“¾æ¥
                    pdf_links = table.find_all('a', href=lambda x: x and 'media.nord.cn' in x and '.pdf' in x)
                    
                    for link in pdf_links:
                        href = link.get('href', '')
                        if href:
                            # è·å–è¯­è¨€ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                            row = link.find_parent('tr')
                            language = "æœªçŸ¥è¯­è¨€"
                            if row:
                                th_elements = row.find_all('th')
                                if th_elements:
                                    language = th_elements[0].get_text().strip()
                            
                            # è¯­è¨€è¿‡æ»¤ï¼šåªä¸‹è½½è‹±è¯­å’Œä¸­æ–‡æ–‡æ¡£
                            if self.should_download_language(language):
                                # æ„å»ºæ–‡ä»¶æ ‡é¢˜
                                file_title = f"{title}_{language}" if language != "æœªçŸ¥è¯­è¨€" else title
                                
                                downloads.append({
                                    'title': file_title,
                                    'url': href,
                                    'category': category,
                                    'module': 'æ–‡æ¡£è¯¦æƒ…é¡µ',
                                    'language': language
                                })
                                
                                self.log(f"   âœ… æ‰¾åˆ°ä¸‹è½½: {file_title} -> {href}")
                            else:
                                self.log(f"   â­ï¸ è·³è¿‡éç›®æ ‡è¯­è¨€: {title}_{language} ({language})")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼ä¸­çš„é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
            if not downloads:
                all_pdf_links = soup.find_all('a', href=lambda x: x and 'media.nord.cn' in x and '.pdf' in x)
                for link in all_pdf_links:
                    href = link.get('href', '')
                    if href:
                        # å¯¹äºæ²¡æœ‰æ˜ç¡®è¯­è¨€ä¿¡æ¯çš„é“¾æ¥ï¼Œé»˜è®¤ä¸‹è½½ï¼ˆå¯èƒ½æ˜¯è‹±æ–‡æˆ–ä¸­æ–‡ï¼‰
                        downloads.append({
                            'title': title,
                            'url': href,
                            'category': category,
                            'module': 'æ–‡æ¡£è¯¦æƒ…é¡µ',
                            'language': 'æœªçŸ¥è¯­è¨€'
                        })
                        self.log(f"   âœ… æ‰¾åˆ°PDFé“¾æ¥: {title} -> {href}")
            
            # ä¸‹è½½æ‰¾åˆ°çš„æ–‡ä»¶
            if downloads:
                self.log(f"ğŸš€ åœ¨è¯¦æƒ…é¡µä¸­æ‰¾åˆ° {len(downloads)} ä¸ªä¸‹è½½æ–‡ä»¶")
                
                for download in downloads:
                    doc_category = self.get_document_category(download['category'], download['title'])
                    
                    # å¦‚æœdoc_categoryä¸ºNoneï¼Œç›´æ¥æ”¾åˆ°äº§å“æ ¹ç›®å½•ï¼Œå‡å°‘å±‚çº§
                    if doc_category:
                        folder_path = os.path.join(base_folder_path, doc_category)
                    else:
                        folder_path = base_folder_path
                    
                    filename = self.generate_clean_filename(download['url'], download['title'])
                    self.download_file(download['url'], filename, folder_path)
                    time.sleep(1)  # ä¸‹è½½é—´éš”
            else:
                self.log(f"âš ï¸ è¯¦æƒ…é¡µä¸­æœªæ‰¾åˆ°ä¸‹è½½æ–‡ä»¶: {detail_url}")
            
            return downloads
            
        except Exception as e:
            self.log(f"âŒ å¤„ç†æ–‡æ¡£è¯¦æƒ…é¡µæ—¶å‡ºé”™: {str(e)}")
            return []

    def find_download_modules(self, soup, page_url):
        """æŸ¥æ‰¾ä¸‹è½½æ¨¡å—"""
        downloads = []
        
        try:
            # å…ˆå°è¯•ç‚¹å‡»ä¸‹è½½æ ‡ç­¾é¡µ
            self.click_downloads_tab()
            time.sleep(2)
            
            # å†å±•å¼€æ‰€æœ‰æŠ˜å å†…å®¹
            self.expand_accordion_sections()
            time.sleep(2)
            
            # é‡æ–°è·å–é¡µé¢æºç 
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æ–¹æ³•1: ä¸“é—¨æŸ¥æ‰¾ä¸‹è½½æ ‡ç­¾é¡µå†…å®¹
            downloads_tab = soup.find('div', {'id': 'downloads', 'role': 'tabpanel'})
            if downloads_tab:
                self.log("ğŸ¯ æ‰¾åˆ°ä¸‹è½½æ ‡ç­¾é¡µï¼Œå¼€å§‹è§£æå†…å®¹")
                
                # æŸ¥æ‰¾æ‰‹é£ç´å¼å†…å®¹
                accordion_cards = downloads_tab.find_all('div', class_='card pl-accordion__card')
                self.log(f"ğŸ” æ‰¾åˆ° {len(accordion_cards)} ä¸ªä¸‹è½½åˆ†ç±»")
                
                for card in accordion_cards:
                    # è·å–åˆ†ç±»åç§°
                    header = card.find('h3', class_='pl-accordion__card-headline')
                    category = header.get_text().strip() if header else "æœªçŸ¥åˆ†ç±»"
                    
                    # æŸ¥æ‰¾è¯¥åˆ†ç±»ä¸‹çš„æ–‡æ¡£
                    teasers = card.find_all('div', class_='teaser-document')
                    self.log(f"ğŸ“‹ {category} åˆ†ç±»ä¸­æ‰¾åˆ° {len(teasers)} ä¸ªæ–‡æ¡£")
                    
                    for teaser in teasers:
                        # è·å–æ–‡æ¡£æ ‡é¢˜
                        title_element = teaser.find('h4', class_='teaser-title')
                        if title_element:
                            title_link = title_element.find('a')
                            doc_title = title_link.get_text().strip() if title_link else "æœªçŸ¥æ–‡æ¡£"
                        else:
                            doc_title = "æœªçŸ¥æ–‡æ¡£"
                        
                        # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
                        download_links = teaser.find_all('a', class_='icon-download')
                        for dl_link in download_links:
                            href = dl_link.get('href', '')
                            link_text = dl_link.get_text().strip()
                            
                            if href and 'media.nord.cn' in href:
                                file_url = href if href.startswith('http') else urljoin(self.base_url, href)
                                title = f"{doc_title} - {link_text}" if link_text else doc_title
                                
                                downloads.append({
                                    'title': title,
                                    'url': file_url,
                                    'module': f'{category}åˆ†ç±»',
                                    'category': category
                                })
                                
                        # æŸ¥æ‰¾è¯¦æƒ…é¡µé“¾æ¥
                        detail_links = teaser.find_all('a', class_='icon-link')
                        for detail_link in detail_links:
                            href = detail_link.get('href', '')
                            if href and not href.startswith('http'):
                                detail_url = urljoin(page_url, href)
                                downloads.append({
                                    'title': f"{doc_title} - è¯¦æƒ…é¡µ",
                                    'url': detail_url,
                                    'module': f'{category}è¯¦æƒ…',
                                    'category': category,
                                    'is_detail_page': True
                                })
            
            # æ–¹æ³•2: æŸ¥æ‰¾ä¸‹è½½æ¨¡å—/åŒºåŸŸï¼ˆåŸæœ‰é€»è¾‘ä¿ç•™ä½œä¸ºå¤‡ç”¨ï¼‰
            download_sections = soup.find_all(['div', 'section'], class_=lambda x: x and any(
                keyword in x.lower() for keyword in ['download', 'document', 'catalog', 'brochure', 'teaser']
            ))
            
            for section in download_sections:
                # åœ¨æ¯ä¸ªä¸‹è½½åŒºåŸŸä¸­æŸ¥æ‰¾æ–‡ä»¶é“¾æ¥
                links = section.find_all('a', href=True)
                
                for link in links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # æŸ¥æ‰¾åŒ…å«æ–‡ä»¶çš„é“¾æ¥
                    if any(domain in href for domain in ['media.nord.cn', 'pdf', 'doc', 'xlsx']):
                        
                        # è·å–æ–‡ä»¶ç±»å‹å’Œæ ‡é¢˜
                        file_url = href if href.startswith('http') else urljoin(self.base_url, href)
                        title = text or "æ–‡æ¡£"
                        
                        # é¿å…é‡å¤
                        if not any(d['url'] == file_url for d in downloads):
                            downloads.append({
                                'title': title,
                                'url': file_url,
                                'module': 'ä¸‹è½½æ¨¡å—'
                            })
            
            # æ–¹æ³•3: ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ä¸‹è½½åŸŸåçš„é“¾æ¥
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text().strip()
                
                if 'media.nord.cn' in href:
                    file_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    title = text or "è¯ºå¾·æ–‡æ¡£"
                    
                    # é¿å…é‡å¤
                    if not any(d['url'] == file_url for d in downloads):
                        downloads.append({
                            'title': title,
                            'url': file_url,
                            'module': 'ç›´æ¥é“¾æ¥'
                        })
            
            if downloads:
                self.log(f"ğŸ“ åœ¨é¡µé¢ä¸­æ‰¾åˆ° {len(downloads)} ä¸ªä¸‹è½½æ–‡ä»¶")
                for i, download in enumerate(downloads[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                    category = download.get('category', '')
                    category_info = f" ({category})" if category else ""
                    self.log(f"   {i+1}. {download['title']}{category_info}")
            else:
                self.log(f"âŒ é¡µé¢ä¸­æœªæ‰¾åˆ°ä¸‹è½½æ–‡ä»¶: {page_url}")
            
        except Exception as e:
            self.log(f"âŒ æŸ¥æ‰¾ä¸‹è½½æ¨¡å—æ—¶å‡ºé”™: {str(e)}")
        
        return downloads
    
    def download_file(self, url, filename, folder_path):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            # åˆ›å»ºç›®å½•
            os.makedirs(folder_path, exist_ok=True)
            file_path = os.path.join(folder_path, filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å¤§å°åˆç†ï¼ˆé¿å…ä¸‹è½½æŸåçš„æ–‡ä»¶ï¼‰
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                if file_size > 1024:  # å¦‚æœæ–‡ä»¶å¤§äº1KBï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆæ–‡ä»¶
                    self.log(f"ğŸ“ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                    return True
                else:
                    self.log(f"ğŸ”„ æ–‡ä»¶å­˜åœ¨ä½†å¤§å°å¼‚å¸¸({file_size}å­—èŠ‚)ï¼Œé‡æ–°ä¸‹è½½: {filename}")
                    os.remove(file_path)
            
            # ä¸‹è½½æ–‡ä»¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': self.base_url
            }
            
            response = requests.get(url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(file_path)
            self.log(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} ({file_size} bytes)")
            
            self.new_files.append({
                'filename': filename,
                'path': file_path,
                'url': url,
                'size': file_size
            })
            
            return True
            
        except Exception as e:
            self.log(f"âŒ ä¸‹è½½å¤±è´¥ {filename}: {str(e)}")
            return False
    
    def generate_filename(self, url, title, module_name="", category=""):
        """ç”Ÿæˆæ–‡ä»¶å"""
        try:
            # ä»URLä¸­æå–æ–‡ä»¶åå’Œæ‰©å±•å
            parsed_url = urlparse(url)
            filename_from_url = os.path.basename(parsed_url.path)
            
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            extension = '.pdf'  # é»˜è®¤æ‰©å±•å
            if filename_from_url and '.' in filename_from_url:
                ext = filename_from_url.split('.')[-1].lower()
                if ext in ['pdf', 'doc', 'docx', 'xlsx', 'png', 'jpg', 'jpeg', 'webp']:
                    extension = f'.{ext}'
            elif '.pdf' in url.lower():
                extension = '.pdf'
            elif '.doc' in url.lower():
                extension = '.doc'
            elif '.xlsx' in url.lower():
                extension = '.xlsx'
            elif '.png' in url.lower():
                extension = '.png'
            elif '.webp' in url.lower():
                extension = '.webp'
            
            # å¦‚æœURLä¸­æœ‰æœ‰æ•ˆçš„æ–‡ä»¶åä¸”ä¸æ˜¯æ•°å­—IDï¼Œç›´æ¥ä½¿ç”¨
            if filename_from_url and '.' in filename_from_url and not filename_from_url.split('.')[0].isdigit():
                filename = unquote(filename_from_url)
                # æ·»åŠ åˆ†ç±»å‰ç¼€
                if category and category not in filename:
                    filename = f"{category}_{filename}"
                return filename
            
            # å¦åˆ™ä»æ ‡é¢˜ç”Ÿæˆæ–‡ä»¶å
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_', 'ï¼ˆ', 'ï¼‰', '(', ')', 'â€“')).strip()
            safe_title = re.sub(r'\s+', '_', safe_title)  # å°†ç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
            safe_title = safe_title[:80]  # é€‚å½“å¢åŠ é•¿åº¦é™åˆ¶
            
            # æ„å»ºæ–‡ä»¶å
            filename_parts = []
            if category and category not in safe_title:
                safe_category = "".join(c for c in category if c.isalnum() or c in (' ', '-', '_')).strip()
                safe_category = re.sub(r'\s+', '_', safe_category)
                filename_parts.append(safe_category)
            
            if module_name and module_name not in safe_title:
                safe_module = "".join(c for c in module_name if c.isalnum() or c in (' ', '-', '_')).strip()
                safe_module = re.sub(r'\s+', '_', safe_module)
                filename_parts.append(safe_module)
            
            filename_parts.append(safe_title)
            
            # ç»„åˆæ–‡ä»¶å
            filename = "_".join(filter(None, filename_parts)) + extension
            filename = re.sub(r'_{2,}', '_', filename)  # å»é™¤é‡å¤ä¸‹åˆ’çº¿
            
            return filename
            
        except Exception as e:
            self.log(f"âš ï¸ æ–‡ä»¶åç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"document_{int(time.time())}.pdf"
    
    def filter_valid_downloads(self, downloads):
        """è¿‡æ»¤æœ‰æ•ˆçš„ä¸‹è½½æ–‡ä»¶ï¼Œåªä¿ç•™PDFæ–‡æ¡£"""
        valid_downloads = []
        for download in downloads:
            # è·³è¿‡è¯¦æƒ…é¡µé“¾æ¥
            if download.get('is_detail_page', False):
                continue
                
            url = download.get('url', '')
            title = download.get('title', '').lower()
            
            # è·³è¿‡ä¸éœ€è¦çš„æ–‡ä»¶ç±»å‹
            if any(ext in url.lower() for ext in ['.jsp', '.png', '.jpg', '.jpeg', '.webp', '.gif']):
                continue
                
            # è·³è¿‡æ— ç”¨çš„æ–‡ä»¶åå’Œå•†å“ä»‹ç»ç›¸å…³å†…å®¹
            skip_keywords = ['æ–‡æ¡£', 'å›¾ç‰‡', 'bild', 'image', 'å•†å“ä»‹ç»', 'äº§å“ä»‹ç»', 'product_intro', 
                           'intro', 'ä»‹ç»', 'æ¦‚è¿°', 'overview_intro', 'äº§å“æ¦‚è§ˆ']
            if any(keyword in title for keyword in skip_keywords):
                continue
                
            # åªä¿ç•™PDFæ–‡ä»¶æˆ–æ˜ç¡®çš„æ–‡æ¡£é“¾æ¥
            if '.pdf' in url.lower() or any(keyword in title for keyword in ['manual', 'catalogue', 'æ‰‹å†Œ', 'æ ·æœ¬', 'é…ä»¶', 'é›¶ä»¶', 'spare', 'parts']):
                valid_downloads.append(download)
        
        return valid_downloads
    
    def get_document_category(self, category, title):
        """æ ¹æ®æ–‡æ¡£åˆ†ç±»å’Œæ ‡é¢˜ç¡®å®šå­æ–‡ä»¶å¤¹åç§° - ç®€åŒ–æ–‡ä»¶å¤¹ç»“æ„"""
        category = category.lower() if category else ''
        title = title.lower() if title else ''
        
        # è·³è¿‡å•†å“ä»‹ç»ç›¸å…³å†…å®¹
        if any(keyword in category or keyword in title for keyword in ['å•†å“ä»‹ç»', 'äº§å“ä»‹ç»', 'ä»‹ç»', 'æ¦‚è¿°']):
            return None  # ä¸åˆ›å»ºæ–‡ä»¶å¤¹ï¼Œç›´æ¥è·³è¿‡
        
        # ç®€åŒ–åˆ†ç±»ï¼Œå‡å°‘åµŒå¥—å±‚çº§
        # æ“ä½œæ‰‹å†Œç›¸å…³
        if any(keyword in category or keyword in title for keyword in ['æ“ä½œæ‰‹å†Œ', 'manual', 'maintenance', 'ç»´æŠ¤']):
            return 'æ“ä½œæ‰‹å†Œ'
        
        # å¤‡ä»¶ç›¸å…³  
        elif any(keyword in category or keyword in title for keyword in ['å¤‡ä»¶', 'spare', 'parts', 'é…ä»¶', 'é›¶ä»¶']):
            return 'å¤‡ä»¶'
            
        # é€‰å‹æ ·æœ¬ç›¸å…³
        elif any(keyword in category or keyword in title for keyword in ['é€‰å‹æ ·æœ¬', 'catalogue', 'æ ·æœ¬', 'catalog']):
            return 'é€‰å‹æ ·æœ¬'
            
        # å®£ä¼ èµ„æ–™ç›¸å…³
        elif any(keyword in category or keyword in title for keyword in ['å®£ä¼ èµ„æ–™', 'brochure', 'å®£ä¼ ', 'flyer']):
            return 'å®£ä¼ èµ„æ–™'
            
        # å…¶ä»–æ–‡æ¡£ - ç»Ÿä¸€æ”¾åˆ°æ ¹ç›®å½•ï¼Œå‡å°‘å±‚çº§
        else:
            return None  # ç›´æ¥æ”¾åˆ°äº§å“æ ¹ç›®å½•
    
    def generate_clean_filename(self, url, title, max_length=100):
        """ç”Ÿæˆæ¸…æ´çš„æ–‡ä»¶åï¼Œä¸åŒ…å«åˆ†ç±»å‰ç¼€"""
        try:
            # æ¸…ç†æ ‡é¢˜
            clean_title = re.sub(r'[^\w\s\-\u4e00-\u9fff]', '', title)
            clean_title = re.sub(r'\s+', '_', clean_title.strip())
            
            # å»æ‰ä¸€äº›å¸¸è§çš„å‰ç¼€
            prefixes_to_remove = ['æ“ä½œæ‰‹å†Œåˆ†ç±»_æ“ä½œæ‰‹å†Œ_', 'å¤‡ä»¶åˆ†ç±»_å¤‡ä»¶_', 'é€‰å‹æ ·æœ¬åˆ†ç±»_é€‰å‹æ ·æœ¬_', 'å®£ä¼ èµ„æ–™åˆ†ç±»_å®£ä¼ èµ„æ–™_']
            for prefix in prefixes_to_remove:
                if clean_title.startswith(prefix):
                    clean_title = clean_title[len(prefix):]
                    break
            
            # å»æ‰æ— ç”¨è¯æ±‡
            words_to_remove = ['ä¸‹è½½', 'è¯¦æƒ…', 'æ–‡æ¡£èµ„æ–™è¯¦æƒ…', 'æ–‡æ¡£', '_ä¸‹è½½_', '_è¯¦æƒ…_', '_ä¸‹è½½', '_è¯¦æƒ…']
            for word in words_to_remove:
                clean_title = clean_title.replace(word, '')
            
            # æ¸…ç†è¿ç»­çš„ä¸‹åˆ’çº¿
            clean_title = re.sub(r'_{2,}', '_', clean_title)
            clean_title = clean_title.strip('_')
            
            # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
            if not clean_title:
                clean_title = f"document_{int(time.time())}"
            
            # æˆªæ–­è¿‡é•¿çš„æ–‡ä»¶å
            if len(clean_title) > max_length:
                clean_title = clean_title[:max_length]
            
            # ä»URLè·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            path = parsed_url.path
            ext = os.path.splitext(path)[1]
            
            if not ext:
                ext = '.pdf'  # é»˜è®¤ä¸ºPDF
            
            return f"{clean_title}{ext}"
            
        except Exception as e:
            self.log(f"âš ï¸ æ–‡ä»¶åç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"document_{int(time.time())}.pdf"
    
    def process_product_detail_page(self, url, category_name, product_name, parent_product=None):
        """å¤„ç†äº§å“è¯¦æƒ…é¡µé¢"""
        if url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†é¡µé¢: {url}")
            return
        
        # å¦‚æœæ˜¯å­äº§å“ï¼Œæ˜¾ç¤ºå®Œæ•´çš„å±‚çº§ä¿¡æ¯
        if parent_product:
            self.log(f"ğŸ“„ å¤„ç†äº§å“è¯¦æƒ…é¡µ: {parent_product}_{product_name}")
            display_name = f"{parent_product}_{product_name}"
        else:
            self.log(f"ğŸ“„ å¤„ç†äº§å“è¯¦æƒ…é¡µ: {product_name}")
            display_name = product_name
            
        soup = self.visit_page(url)
        
        if not soup:
            return
        
        # æŸ¥æ‰¾ä¸‹è½½æ¨¡å—
        downloads = self.find_download_modules(soup, url)
        
        # ä¸‹è½½æ–‡ä»¶
        if downloads:
            # ç®€åŒ–æ–‡ä»¶å¤¹ç»“æ„ - å‡å°‘å±‚çº§åµŒå¥—
            if parent_product:
                # å°†çˆ¶äº§å“å’Œå­äº§å“åç§°åˆå¹¶ï¼Œé¿å…è¿‡æ·±çš„ç›®å½•ç»“æ„
                combined_name = f"{parent_product}_{product_name}"
                base_folder_path = os.path.join(self.base_dir, category_name, combined_name)
            else:
                base_folder_path = os.path.join(self.base_dir, category_name, product_name)
            
            # åˆ†ç¦»ç›´æ¥ä¸‹è½½æ–‡ä»¶å’Œè¯¦æƒ…é¡µé“¾æ¥
            file_downloads = self.filter_valid_downloads(downloads)
            detail_pages = [d for d in downloads if d.get('is_detail_page', False)]
            
            self.log(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(file_downloads)} ä¸ªç›´æ¥æ–‡ä»¶åˆ°: {base_folder_path}")
            if detail_pages:
                self.log(f"ğŸ“‹ å‘ç° {len(detail_pages)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥ï¼Œå°†é€ä¸ªå¤„ç†")
            
            # å¤„ç†ç›´æ¥ä¸‹è½½æ–‡ä»¶
            for download in file_downloads:
                module_name = download.get('module', '')
                category = download.get('category', '')
                
                # æ ¹æ®æ–‡æ¡£åˆ†ç±»åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
                doc_category = self.get_document_category(category, download['title'])
                
                # å¦‚æœdoc_categoryä¸ºNoneï¼Œç›´æ¥æ”¾åˆ°äº§å“æ ¹ç›®å½•ï¼Œå‡å°‘å±‚çº§
                if doc_category:
                    folder_path = os.path.join(base_folder_path, doc_category)
                else:
                    folder_path = base_folder_path
                
                # ç”Ÿæˆæ¸…æ´çš„æ–‡ä»¶åï¼ˆä¸åŒ…å«åˆ†ç±»å‰ç¼€ï¼‰
                filename = self.generate_clean_filename(download['url'], download['title'])
                self.download_file(download['url'], filename, folder_path)
                time.sleep(1)  # ä¸‹è½½é—´éš”
            
            # å¤„ç†è¯¦æƒ…é¡µé“¾æ¥
            for detail_page in detail_pages:
                detail_title = detail_page['title'].replace(' - è¯¦æƒ…é¡µ', '').strip()
                category = detail_page.get('category', '')
                
                # å¤„ç†æ–‡æ¡£è¯¦æƒ…é¡µ
                self.process_document_detail_page(
                    detail_page['url'], 
                    detail_title, 
                    category, 
                    base_folder_path
                )
                time.sleep(2)  # è¯¦æƒ…é¡µå¤„ç†é—´éš”
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        self.processed_urls.add(url)
    
    def process_product_with_fallback(self, url, category_name, product_name):
        """å¤„ç†äº§å“é¡µé¢ï¼ˆå…ˆå°è¯•æŸ¥æ‰¾å­äº§å“ï¼Œæ— å­äº§å“åˆ™ä½œä¸ºè¯¦æƒ…é¡µå¤„ç†ï¼‰"""
        if url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†é¡µé¢: {url}")
            return
        
        self.log(f"ğŸ” åˆ†æäº§å“é¡µé¢: {product_name}")
        soup = self.visit_page(url)
        
        if not soup:
            return
        
        # å…ˆæŸ¥æ‰¾å­äº§å“
        sub_products = self.find_sub_products(soup, url)
        
        if sub_products:
            # æœ‰å­äº§å“ï¼ŒæŒ‰æ¦‚è§ˆé¡µé¢å¤„ç†
            self.log(f"ğŸ“‹ æ‰¾åˆ° {len(sub_products)} ä¸ªå­äº§å“ï¼ŒæŒ‰æ¦‚è§ˆé¡µé¢å¤„ç†")
            self.process_product_overview_page(url, category_name, product_name)
        else:
            # æ²¡æœ‰å­äº§å“ï¼ŒæŒ‰è¯¦æƒ…é¡µé¢å¤„ç†
            self.log(f"ğŸ“„ æ— å­äº§å“ï¼ŒæŒ‰è¯¦æƒ…é¡µé¢å¤„ç†")
            self.process_product_detail_page(url, category_name, product_name)
    
    def process_product_overview_page(self, url, category_name, product_name):
        """å¤„ç†äº§å“æ¦‚è§ˆé¡µé¢ï¼ˆå¯èƒ½æœ‰å­äº§å“ï¼‰"""
        if url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†é¡µé¢: {url}")
            return
        
        self.log(f"ğŸ“‹ å¤„ç†äº§å“æ¦‚è§ˆé¡µ: {product_name}")
        soup = self.visit_page(url)
        
        if not soup:
            return
        
        # å…ˆå¤„ç†å½“å‰é¡µé¢çš„ä¸‹è½½
        downloads = self.find_download_modules(soup, url)
        if downloads:
            base_folder_path = os.path.join(self.base_dir, category_name, product_name)
            
            # åˆ†ç¦»ç›´æ¥ä¸‹è½½æ–‡ä»¶å’Œè¯¦æƒ…é¡µé“¾æ¥
            file_downloads = self.filter_valid_downloads(downloads)
            detail_pages = [d for d in downloads if d.get('is_detail_page', False)]
            
            # å¤„ç†ç›´æ¥ä¸‹è½½æ–‡ä»¶
            for download in file_downloads:
                category = download.get('category', '')
                doc_category = self.get_document_category(category, download['title'])
                
                # å¦‚æœdoc_categoryä¸ºNoneï¼Œç›´æ¥æ”¾åˆ°äº§å“æ ¹ç›®å½•ï¼Œå‡å°‘å±‚çº§
                if doc_category:
                    folder_path = os.path.join(base_folder_path, doc_category)
                else:
                    folder_path = base_folder_path
                
                filename = self.generate_clean_filename(download['url'], download['title'])
                self.download_file(download['url'], filename, folder_path)
                time.sleep(1)
            
            # å¤„ç†è¯¦æƒ…é¡µé“¾æ¥
            for detail_page in detail_pages:
                detail_title = detail_page['title'].replace(' - è¯¦æƒ…é¡µ', '').strip()
                category = detail_page.get('category', '')
                
                # å¤„ç†æ–‡æ¡£è¯¦æƒ…é¡µ
                self.process_document_detail_page(
                    detail_page['url'], 
                    detail_title, 
                    category, 
                    base_folder_path
                )
                time.sleep(2)  # è¯¦æƒ…é¡µå¤„ç†é—´éš”
        
        # æŸ¥æ‰¾å­äº§å“
        sub_products = self.find_sub_products(soup, url)
        
        if sub_products:
            self.log(f"ğŸ” åœ¨ {product_name} ä¸­æ‰¾åˆ° {len(sub_products)} ä¸ªå­äº§å“")
            
            for i, sub_product in enumerate(sub_products[:10]):  # é™åˆ¶æ•°é‡
                self.log(f"ğŸ”„ å¤„ç†å­äº§å“ {i+1}/{len(sub_products[:10])}: {sub_product['name']}")
                
                # è·å–å­äº§å“çš„çœŸå®åç§°
                actual_name = self.get_actual_product_name(sub_product['url'])
                
                # å¤„ç†å­äº§å“çš„è¯¦æƒ…é¡µ - ä¸ºæ¯ä¸ªå­äº§å“åˆ›å»ºç‹¬ç«‹ç›®å½•
                if actual_name:
                    # ä½¿ç”¨çœŸå®åç§°
                    sub_folder_name = actual_name
                    self.log(f"âœ… ä½¿ç”¨çœŸå®äº§å“åç§°: {actual_name}")
                else:
                    # ä½¿ç”¨å¤‡ç”¨åç§°
                    clean_sub_name = sub_product['name'].replace('_è¯¦æƒ…', '').replace('è¯¦æƒ…', '').strip()
                    if not clean_sub_name:
                        clean_sub_name = f"å­äº§å“{i+1}"
                    sub_folder_name = clean_sub_name
                    self.log(f"âš ï¸ ä½¿ç”¨å¤‡ç”¨åç§°: {sub_folder_name}")
                
                self.process_product_detail_page(sub_product['url'], category_name, sub_folder_name, parent_product=product_name)
                time.sleep(2)
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        self.processed_urls.add(url)
    
    def crawl_category(self, category):
        """çˆ¬å–å•ä¸ªåˆ†ç±»"""
        category_name = category['name']
        category_url = category['url']
        
        self.log(f"ğŸš€ å¼€å§‹çˆ¬å–åˆ†ç±»: {category_name}")
        
        soup = self.visit_page(category_url)
        if not soup:
            return
        
        # å…ˆå¤„ç†åˆ†ç±»é¡µé¢æœ¬èº«çš„ä¸‹è½½
        downloads = self.find_download_modules(soup, category_url)
        if downloads:
            folder_path = os.path.join(self.base_dir, category_name)
            for download in downloads:
                filename = self.generate_filename(download['url'], download['title'])
                self.download_file(download['url'], filename, folder_path)
                time.sleep(1)
        
        # æŸ¥æ‰¾äº§å“é“¾æ¥
        product_links = self.find_product_links(soup, category_url)
        
        if product_links:
            self.log(f"ğŸ“‹ åœ¨ {category_name} åˆ†ç±»ä¸­æ‰¾åˆ° {len(product_links)} ä¸ªäº§å“")
            
            for i, product in enumerate(product_links[:10]):  # é™åˆ¶æ•°é‡é¿å…è¿‡åº¦çˆ¬å–
                self.log(f"ğŸ”„ å¤„ç†äº§å“ {i+1}/{len(product_links[:10])}: {product['name']}")
                
                # åˆ¤æ–­æ˜¯æ¦‚è§ˆé¡µé¢è¿˜æ˜¯è¯¦æƒ…é¡µé¢
                if 'overview' in product['url']:
                    # æ¦‚è§ˆé¡µé¢ï¼Œå¯èƒ½æœ‰å­äº§å“
                    self.process_product_overview_page(product['url'], category_name, product['name'])
                else:
                    # å…ˆå°è¯•ä½œä¸ºæ¦‚è§ˆé¡µé¢å¤„ç†ï¼ˆæŸ¥æ‰¾å­äº§å“ï¼‰
                    # å¦‚æœæ²¡æœ‰å­äº§å“ï¼Œåˆ™ä½œä¸ºè¯¦æƒ…é¡µé¢å¤„ç†
                    self.process_product_with_fallback(product['url'], category_name, product['name'])
                
                time.sleep(3)  # äº§å“é—´å»¶è¿Ÿ
        
        self.log(f"âœ… å®Œæˆåˆ†ç±»: {category_name}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            self.log("ğŸš€ å¼€å§‹è¿è¡Œè¯ºå¾·æ–‡æ¡£çˆ¬è™« (V2ç‰ˆæœ¬)")
            
            # è·å–ä¸»è¦åˆ†ç±»
            categories = self.get_main_categories()
            
            if not categories:
                self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»ï¼Œé€€å‡º")
                return
            
            self.log(f"ğŸ“‹ æ‰¾åˆ° {len(categories)} ä¸ªä¸»è¦åˆ†ç±»")
            
            # çˆ¬å–æ¯ä¸ªåˆ†ç±»
            for category in categories:
                self.crawl_category(category)
                time.sleep(5)  # åˆ†ç±»é—´å»¶è¿Ÿ
            
            # ä¿å­˜è¿›åº¦
            self.save_processed_urls()
            
            # ç»Ÿè®¡ç»“æœ
            total_files = len(self.new_files)
            self.log(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±ä¸‹è½½ {total_files} ä¸ªæ–°æ–‡ä»¶")
            
            if self.new_files:
                self.log("ğŸ“ æ–°ä¸‹è½½çš„æ–‡ä»¶:")
                for file_info in self.new_files[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                    self.log(f"   ğŸ“„ {file_info['filename']} ({file_info['size']} bytes)")
                
                if len(self.new_files) > 10:
                    self.log(f"   ... è¿˜æœ‰ {len(self.new_files) - 10} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            self.log(f"âŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {str(e)}")
            
        finally:
            # å…³é—­WebDriver
            if self.driver:
                self.driver.quit()
                self.log("ğŸ”’ WebDriverå·²å…³é—­")

def test_detail_page_processing(detail_url=None):
    """æµ‹è¯•æ–‡æ¡£è¯¦æƒ…é¡µå¤„ç†åŠŸèƒ½"""
    spider = NordSeleniumSpiderV2()
    
    try:
        # é»˜è®¤æµ‹è¯•URLï¼ˆç”¨æˆ·æä¾›çš„ç¤ºä¾‹ï¼‰
        test_url = detail_url or "https://www.nord.cn/cn/service/documentation/manuals/details/b1033a.jsp"
        
        spider.log(f"ğŸ§ª æµ‹è¯•æ–‡æ¡£è¯¦æƒ…é¡µå¤„ç†åŠŸèƒ½")
        spider.log(f"ğŸ“‹ æµ‹è¯•URL: {test_url}")
        
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_folder = os.path.join(spider.base_dir, "æµ‹è¯•", "è¯¦æƒ…é¡µæµ‹è¯•")
        
        # å¤„ç†è¯¦æƒ…é¡µ
        downloads = spider.process_document_detail_page(
            test_url, 
            "B1033A_Universal_Worm_Gear_Units_Kits", 
            "æ“ä½œæ‰‹å†Œ", 
            test_folder
        )
        
        if downloads:
            spider.log(f"âœ… æµ‹è¯•æˆåŠŸï¼æ‰¾åˆ° {len(downloads)} ä¸ªä¸‹è½½æ–‡ä»¶")
            for download in downloads:
                spider.log(f"   ğŸ“„ {download['title']} -> {download['url']}")
        else:
            spider.log(f"âš ï¸ æµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä¸‹è½½æ–‡ä»¶")
        
    except Exception as e:
        spider.log(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    finally:
        if spider.driver:
            spider.driver.quit()

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼
        test_url = sys.argv[2] if len(sys.argv) > 2 else None
        test_detail_page_processing(test_url)
    else:
        # æ­£å¸¸è¿è¡Œ
        spider = NordSeleniumSpiderV2()
        spider.run()
