#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import os, time, json, requests, chardet
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote, quote
import re
import pickle
from pathlib import Path
import hashlib
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import hmac
import base64
import urllib.parse

class HitachiUltimateSpider: # æ—¥ç«‹çˆ¬è™«
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.hitachi-iec.cn/',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        self.base_url = "https://www.hitachi-iec.cn"
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰
        self.base_dir = "/srv/downloads/approved/æ—¥ç«‹"
        self.processed_urls = self.load_processed_urls()
        self.new_files = []
        self.debug = True
        self.config = self.load_config()
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.ACCESS_TOKEN = "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        self.SECRET = "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb"
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆå…¨é‡çˆ¬å–ï¼‰
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_urls.pkl'))
        
        # å®Œæ•´çš„äº§å“ç›®å½• - æ›´æ–°ä¸ºå®é™…å¯ç”¨çš„URL
        self.product_categories = {
            'inverters': {
                'name': 'å˜é¢‘å™¨',
                'products': [
                    {'name': 'SJ-P1å…¨çƒç‰ˆé«˜æ€§èƒ½çŸ¢é‡å‹å˜é¢‘å™¨', 'url': 'https://www.hitachi-iec.cn/ch/product/trans/sjp1/index.html'},
                    {'name': 'SH1é«˜æ€§èƒ½å…¨å…¼å®¹å‹å˜é¢‘å™¨', 'url': 'https://www.hitachi-iec.cn/ch/product/trans/sh1/index.html'},
                    {'name': 'NH1é«˜æ€§èƒ½æ ‡å‡†çŸ¢é‡å‹å˜é¢‘å™¨', 'url': 'https://www.hitachi-iec.cn/ch/product/trans/nh1/index.html'},
                    {'name': 'LH1å¤šç”¨é€”é€šç”¨çŸ¢é‡å‹å˜é¢‘å™¨', 'url': 'https://www.hitachi-iec.cn/ch/product/trans/lh1/index.html'},
                    {'name': 'Ps-H100ä¹¦æœ¬å¼é«˜æ€§èƒ½å˜é¢‘å™¨', 'url': 'https://www.hitachi-iec.cn/ch/product/trans/psh100/index.html'},
                    {'name': 'WJ-C1ç´§å‡‘é«˜æ€§èƒ½å˜é¢‘å™¨', 'url': 'https://www.hitachi-iec.cn/ch/product/trans/wjc1/index.html'},
                    {'name': 'Cs-H100å°å‹é€šç”¨çŸ¢é‡å‹å˜é¢‘å™¨', 'url': 'https://www.hitachi-iec.cn/ch/product/trans/csh100/index.html'}
                ]
            },
            'marking': {
                'name': 'æ ‡è¯†è®¾å¤‡',
                'products': [
                    {'name': 'UXç³»åˆ—å·¥ä¸šå–·ç æœº', 'url': 'https://www.hitachi-iec.cn/ch/product/print/ux/index.html'},
                    {'name': 'Gç³»åˆ—å·¥ä¸šå–·ç æœº', 'url': 'https://www.hitachi-iec.cn/ch/product/print/g/index.html'},
                    {'name': 'RX2ç³»åˆ—å·¥ä¸šå–·ç æœº', 'url': 'https://www.hitachi-iec.cn/ch/product/print/rx/index.html'}
                ]
            },
            'motor': {
                'name': 'ç”µæœº',
                'products': [
                    {'name': 'å·¥ä¸šç”µæœºäº§å“', 'url': 'https://www.hitachi-iec.cn/ch/product/driver/index.html'}
                ]
            },
            'plc': {
                'name': 'å¯ç¼–ç¨‹æ§åˆ¶å™¨',
                'products': [
                    {'name': 'HXç³»åˆ—PLC', 'url': 'https://www.hitachi-iec.cn/ch/product/plc/kzq/hx/index.html'},
                    {'name': 'MICRO-EHVç³»åˆ—PLC', 'url': 'https://www.hitachi-iec.cn/ch/product/plc/kzq/microehv/index.html'},
                    {'name': 'EH-150 EHVç³»åˆ—PLC', 'url': 'https://www.hitachi-iec.cn/ch/product/plc/kzq/eh150ehv/index.html'}
                ]
            },
            'blower': {
                'name': 'é¼“é£æœº',
                'products': [
                    {'name': 'å·¥ä¸šé¼“é£æœº', 'url': 'https://www.hitachi-iec.cn/ch/product/fan/index.html'}
                ]
            },
            'hoist': {
                'name': 'æ—¥ç«‹ç”µåŠ¨è‘«èŠ¦',
                'products': [
                    {'name': 'æ—¥ç«‹é’¢ä¸ç»³è‘«èŠ¦', 'url': 'https://www.hitachi-iec.cn/ch/product/ddhl/g/index.html'},
                    {'name': 'æ—¥ç«‹ç¯é“¾è‘«èŠ¦', 'url': 'https://www.hitachi-iec.cn/ch/product/ddhl/h/index.html'}
                ]
            }
        }
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = "config.json"
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            if self.debug:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "spider_settings": {
                "max_concurrent_downloads": 10,
                "download_timeout": 60,
                "retry_times": 3,
                "delay_between_requests": 1
            },
            "notification_settings": {
                "enable_email": False,
                "enable_console": True
            },
            "schedule_settings": {
                "enable_schedule": True,
                "check_times": ["09:00", "13:00", "17:00"]
            },
            "output_settings": {
                "generate_html_report": True,
                "generate_pdf_features": True,
                "compress_old_files": False
            }
        }
        
    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URL"""
        urls_file = Path(self.base_dir) / 'processed_urls.pkl'
        if urls_file.exists():
            try:
                with open(urls_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
        
    def save_processed_urls(self):
        """ä¿å­˜å·²å¤„ç†çš„URL"""
        urls_file = Path(self.base_dir) / 'processed_urls.pkl'
        urls_file.parent.mkdir(parents=True, exist_ok=True)
        with open(urls_file, 'wb') as f:
            pickle.dump(self.processed_urls, f)

    def clean_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶å"""
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = filename.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
        return filename[:100].strip()

    def safe_request(self, url, **kwargs):
        """å®‰å…¨çš„ç½‘ç»œè¯·æ±‚"""
        try:
            time.sleep(self.config['spider_settings']['delay_between_requests'])
            response = self.session.get(url, timeout=self.config['spider_settings']['download_timeout'], **kwargs)
            response.raise_for_status()
            return response
        except Exception as e:
            if self.debug:
                print(f"è¯·æ±‚å¤±è´¥: {url} - {str(e)}")
            return None

    def download_image(self, img_url, save_path):
        """ä¸‹è½½å›¾ç‰‡æ–‡ä»¶"""
        try:
            response = self.safe_request(img_url)
            if response and response.status_code == 200:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                with open(save_path, 'wb') as f:
                    f.write(response.content)
                return True
        except Exception as e:
            if self.debug:
                print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url} - {str(e)}")
        return False

    def extract_product_features_with_images(self, soup, product_name, product_url):
        """æå–äº§å“ç‰¹ç‚¹ä¿¡æ¯ï¼ŒåŒ…æ‹¬å›¾ç‰‡"""
        features = {
            'product_name': product_name,
            'product_url': product_url,
            'feature_images': [],
            'feature_sections': [],
            'all_content': '',
            'page_structure': []
        }
        
        try:
            # æŸ¥æ‰¾äº§å“ç‰¹ç‚¹ç›¸å…³çš„å›¾ç‰‡
            feature_images = []
            images = soup.find_all('img')
            
            for img in images:
                src = img.get('src', '')
                alt = img.get('alt', '')
                
                # æŸ¥æ‰¾ç‰¹ç‚¹ç›¸å…³çš„å›¾ç‰‡ - æ‰©å¤§æœç´¢èŒƒå›´
                if (any(keyword in src.lower() for keyword in ['feature', 'ç‰¹ç‚¹', 'point', '1_', '2_', 'pic']) or
                    any(keyword in alt.lower() for keyword in ['ç‰¹ç‚¹', 'feature']) or
                    'images/' in src):  # äº§å“é¡µé¢çš„imagesç›®å½•é€šå¸¸åŒ…å«ç‰¹ç‚¹å›¾ç‰‡
                    
                    if not src.startswith('http'):
                        img_url = urljoin(product_url, src)
                    else:
                        img_url = src
                    
                    feature_images.append({
                        'url': img_url,
                        'alt': alt or 'äº§å“ç‰¹ç‚¹å›¾ç‰‡',
                        'src': src
                    })
            
            features['feature_images'] = feature_images
            
            # åˆ†æé¡µé¢ç»“æ„ï¼ŒæŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_sections = []
            
            # æŸ¥æ‰¾åŒ…å«äº§å“ç‰¹ç‚¹çš„divæˆ–section
            for element in soup.find_all(['div', 'section', 'article']):
                element_text = element.get_text().strip()
                
                # å¦‚æœå…ƒç´ åŒ…å«ç‰¹ç‚¹ç›¸å…³å†…å®¹
                if (len(element_text) > 50 and
                    any(keyword in element_text for keyword in ['ç‰¹ç‚¹', 'åŠŸèƒ½', 'æ€§èƒ½', 'ä¼˜åŠ¿', 'åº”ç”¨'])):
                    
                    main_sections.append({
                        'tag': element.name,
                        'text': element_text[:500],
                        'html': str(element)[:1000]
                    })
            
            features['feature_sections'] = main_sections
            
        except Exception as e:
            if self.debug:
                print(f"æå–é¡µé¢ä¿¡æ¯å¤±è´¥: {e}")
        
        return features

    def create_product_features_pdf(self, features, product_dir):
        """åˆ›å»ºäº§å“ç‰¹ç‚¹PDFæ–‡ä»¶"""
        try:
            # ä¸‹è½½ç‰¹ç‚¹ç›¸å…³å›¾ç‰‡
            image_files = []
            images_dir = os.path.join(product_dir, 'images')
            os.makedirs(images_dir, exist_ok=True)
            
            print(f"ğŸ“¸ å¼€å§‹ä¸‹è½½äº§å“ç‰¹ç‚¹å›¾ç‰‡...")
            for i, img_info in enumerate(features['feature_images']):
                img_filename = f"feature_image_{i+1:02d}.png"
                img_path = os.path.join(images_dir, img_filename)
                
                if self.download_image(img_info['url'], img_path):
                    image_files.append({
                        'filename': img_filename,
                        'alt': img_info['alt'],
                        'path': img_path,
                        'relative_path': f"images/{img_filename}"
                    })
                    print(f"  âœ“ {img_info['alt'] or f'å›¾ç‰‡{i+1}'}")
                else:
                    print(f"  âœ— ä¸‹è½½å¤±è´¥: {img_info['url']}")
            
            # åˆ›å»ºHTMLå†…å®¹
            html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{features['product_name']} - äº§å“ç‰¹ç‚¹è¯¦æƒ…</title>
    <style>
        @page {{
            size: A4;
            margin: 20mm 15mm;
        }}
        body {{
            font-family: "Microsoft YaHei", "SimHei", "PingFang SC", Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            color: #333;
            font-size: 14px;
        }}
        .header {{
            text-align: center;
            border-bottom: 3px solid #0066cc;
            padding-bottom: 20px;
            margin-bottom: 30px;
            page-break-after: avoid;
        }}
        .product-title {{
            font-size: 28px;
            font-weight: bold;
            color: #0066cc;
            margin-bottom: 10px;
        }}
        .subtitle {{
            font-size: 20px;
            color: #666;
            margin-bottom: 15px;
        }}
        .meta-info {{
            font-size: 12px;
            color: #666;
            margin-bottom: 20px;
            line-height: 1.4;
        }}
        .section {{
            margin-bottom: 40px;
            page-break-inside: avoid;
        }}
        .section-title {{
            font-size: 18px;
            font-weight: bold;
            color: #0066cc;
            border-left: 4px solid #0066cc;
            padding-left: 10px;
            margin-bottom: 20px;
            page-break-after: avoid;
        }}
        .feature-images {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .feature-image-container {{
            text-align: center;
            page-break-inside: avoid;
        }}
        .feature-image {{
            max-width: 100%;
            max-height: 400px;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .image-caption {{
            font-size: 12px;
            color: #666;
            text-align: center;
            margin-top: 8px;
            font-style: italic;
        }}
        .content-text {{
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            border-left: 4px solid #ffc107;
        }}
        .highlight {{
            background-color: #fff3cd;
            padding: 15px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
            border-radius: 4px;
        }}
        .footer {{
            border-top: 2px solid #ddd;
            padding-top: 20px;
            margin-top: 40px;
            font-size: 12px;
            color: #666;
            text-align: center;
            page-break-inside: avoid;
        }}
        .info-grid {{
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }}
        .info-item {{
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            border-left: 3px solid #0066cc;
        }}
        .feature-count {{
            background: linear-gradient(135deg, #0066cc, #004499);
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            display: inline-block;
            font-size: 12px;
            font-weight: bold;
            margin-bottom: 15px;
        }}
    </style>
</head>
<body>
    <div class="header">
        <div class="product-title">{features['product_name']}</div>
        <div class="subtitle">äº§å“ç‰¹ç‚¹è¯¦ç»†èµ„æ–™</div>
        <div class="meta-info">
            ğŸ“ æ•°æ®æ¥æº: <a href="{features['product_url']}">{features['product_url']}</a><br>
            ğŸ•’ ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}<br>
            ğŸ› ï¸ ç”Ÿæˆå·¥å…·: æ—¥ç«‹ç»ˆæçˆ¬è™« v1.0
        </div>
    </div>

    <div class="section">
        <div class="section-title">ğŸ“‹ äº§å“æ¦‚å†µ</div>
        <div class="info-grid">
            <div class="info-item">
                <strong>ğŸ“¸ ç‰¹ç‚¹å›¾ç‰‡æ•°é‡:</strong> {len(image_files)} å¼ 
            </div>
            <div class="info-item">
                <strong>ğŸ“ å†…å®¹æ®µè½:</strong> {len(features['feature_sections'])} ä¸ª
            </div>
        </div>
        <div class="highlight">
            <strong>ğŸ“– è¯´æ˜:</strong> æ—¥ç«‹äº§å“é¡µé¢çš„äº§å“ç‰¹ç‚¹ä¸»è¦ä»¥å›¾ç‰‡å½¢å¼å±•ç¤ºè¯¦ç»†çš„åŠŸèƒ½è¯´æ˜å’ŒæŠ€æœ¯å‚æ•°ã€‚
            ä¸‹æ–¹çš„ç‰¹ç‚¹å›¾ç‰‡åŒ…å«äº†å®Œæ•´çš„äº§å“åŠŸèƒ½ä»‹ç»ï¼Œè¯·ä»”ç»†æŸ¥çœ‹å›¾ç‰‡å†…å®¹äº†è§£å…·ä½“ç‰¹ç‚¹ã€‚
        </div>
    </div>

    <div class="section">
        <div class="section-title">ğŸ¯ äº§å“ç‰¹ç‚¹å›¾ç‰‡å±•ç¤º</div>
        <div class="feature-count">æ€»è®¡ {len(image_files)} å¼ äº§å“ç‰¹ç‚¹å›¾ç‰‡</div>
        <div class="feature-images">
"""
            
            # æ·»åŠ ç‰¹ç‚¹å›¾ç‰‡
            for i, img_info in enumerate(image_files, 1):
                html_content += f"""
            <div class="feature-image-container">
                <img src="{img_info['relative_path']}" alt="{img_info['alt']}" class="feature-image">
                <div class="image-caption">å›¾ç‰‡ {i}: {img_info['alt']}</div>
            </div>
"""
            
            html_content += """
        </div>
    </div>
"""
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹
            if features['feature_sections']:
                html_content += f"""
    <div class="section">
        <div class="section-title">ğŸ“ äº§å“ç‰¹ç‚¹æ–‡å­—è¯´æ˜</div>
        <div class="feature-count">å‘ç° {len(features['feature_sections'])} ä¸ªç›¸å…³å†…å®¹æ®µè½</div>
"""
                for i, section in enumerate(features['feature_sections'], 1):
                    html_content += f"""
        <div class="content-text">
            <h4>ğŸ“„ å†…å®¹æ®µè½ {i}</h4>
            <p>{section['text']}</p>
        </div>
"""
                html_content += """
    </div>
"""
            else:
                html_content += """
    <div class="section">
        <div class="section-title">ğŸ“ å†…å®¹è¯´æ˜</div>
        <div class="content-text">
            <p>è¯¥äº§å“é¡µé¢çš„ç‰¹ç‚¹ä¿¡æ¯ä¸»è¦é€šè¿‡ä¸Šæ–¹çš„å›¾ç‰‡å±•ç¤ºã€‚å›¾ç‰‡ä¸­åŒ…å«äº†è¯¦ç»†çš„äº§å“åŠŸèƒ½è¯´æ˜ã€æŠ€æœ¯å‚æ•°å’Œåº”ç”¨åœºæ™¯ç­‰ä¿¡æ¯ã€‚</p>
        </div>
    </div>
"""
            
            html_content += f"""
    <div class="footer">
        <p><strong>æ—¥ç«‹äº§æœºç³»ç»Ÿ(ä¸­å›½)æœ‰é™å…¬å¸</strong></p>
        <p>äº§å“èµ„æ–™è‡ªåŠ¨æå–å·¥å…· | ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>Â© ç‰ˆæƒä¿¡æ¯è¯·å‚è€ƒåŸç½‘ç«™ | æœ¬æ–‡æ¡£ä»…ä¾›å‚è€ƒ</p>
    </div>
</body>
</html>
"""
            
            # ä¿å­˜HTMLæ–‡ä»¶
            html_file = os.path.join(product_dir, f"{features['product_name']}_äº§å“ç‰¹ç‚¹.html")
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"âœ“ HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {os.path.basename(html_file)}")
            
            # ç”ŸæˆPDF - ä¼˜å…ˆä½¿ç”¨å›¾ç‰‡ç›´æ¥åˆå¹¶æ–¹æ³•
            pdf_file = os.path.join(product_dir, f"{features['product_name']}_äº§å“ç‰¹ç‚¹.pdf")
            
            # æ–¹æ³•1: ç›´æ¥åˆå¹¶å›¾ç‰‡ä¸ºPDFï¼ˆæ¨èï¼Œæ›´å¯é ï¼‰
            if self.convert_images_to_pdf(image_files, pdf_file, features['product_name']):
                # PDFç”ŸæˆæˆåŠŸååˆ é™¤HTMLæ–‡ä»¶
                try:
                    if os.path.exists(html_file):
                        os.remove(html_file)
                        print(f"ğŸ§¹ å·²åˆ é™¤HTMLæ–‡ä»¶: {os.path.basename(html_file)}")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤HTMLæ–‡ä»¶å¤±è´¥: {e}")
                return pdf_file, len(image_files)
            
            # æ–¹æ³•2: HTMLè½¬PDFï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
            print("ğŸ”„ å°è¯•HTMLè½¬PDFæ–¹æ³•...")
            if self.convert_html_to_pdf(html_file, pdf_file):
                print(f"âœ“ PDFæ–‡ä»¶å·²ç”Ÿæˆ: {os.path.basename(pdf_file)}")
                # PDFç”ŸæˆæˆåŠŸååˆ é™¤HTMLæ–‡ä»¶
                try:
                    if os.path.exists(html_file):
                        os.remove(html_file)
                        print(f"ğŸ§¹ å·²åˆ é™¤HTMLæ–‡ä»¶: {os.path.basename(html_file)}")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤HTMLæ–‡ä»¶å¤±è´¥: {e}")
                return pdf_file, len(image_files)
            else:
                print(f"âš ï¸  æ‰€æœ‰PDFè½¬æ¢æ–¹æ³•éƒ½å¤±è´¥ï¼Œä¿ç•™HTMLæ–‡ä»¶ä¾›æ‰‹åŠ¨è½¬æ¢")
                return html_file, len(image_files)
            
        except Exception as e:
            print(f"âœ— ç”Ÿæˆäº§å“ç‰¹ç‚¹PDFå¤±è´¥: {str(e)}")
            return None, 0

    def convert_images_to_pdf(self, image_files, pdf_file, product_name):
        """å°†å›¾ç‰‡ç›´æ¥åˆå¹¶ä¸ºPDF - æ›´å¯é çš„æ–¹æ³•"""
        try:
            from PIL import Image
            
            if not image_files:
                print("âš ï¸  æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œæ— æ³•ç”ŸæˆPDF")
                return False
            
            # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„å›¾ç‰‡
            valid_images = []
            for img_info in image_files:
                img_path = img_info['path']
                if os.path.exists(img_path):
                    try:
                        # æ‰“å¼€å¹¶è½¬æ¢å›¾ç‰‡
                        img = Image.open(img_path)
                        # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆPDFéœ€è¦ï¼‰
                        if img.mode != 'RGB':
                            img = img.convert('RGB')
                        valid_images.append(img)
                        print(f"  âœ“ æ·»åŠ å›¾ç‰‡: {img_info['alt']}")
                    except Exception as e:
                        print(f"  âœ— å›¾ç‰‡å¤„ç†å¤±è´¥: {img_path} - {e}")
                else:
                    print(f"  âœ— å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
            
            if not valid_images:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶")
                return False
            
            # åˆ›å»ºPDF
            print(f"ğŸ“„ æ­£åœ¨åˆå¹¶ {len(valid_images)} å¼ å›¾ç‰‡ä¸ºPDF...")
            
            # ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºåŸºç¡€ï¼Œå…¶ä»–å›¾ç‰‡è¿½åŠ 
            first_image = valid_images[0]
            other_images = valid_images[1:] if len(valid_images) > 1 else []
            
            # ä¿å­˜ä¸ºPDF
            first_image.save(
                pdf_file, 
                "PDF", 
                resolution=100.0,
                save_all=True, 
                append_images=other_images
            )
            
            # è·å–PDFæ–‡ä»¶å¤§å°
            pdf_size = os.path.getsize(pdf_file)
            print(f"âœ… PDFç”ŸæˆæˆåŠŸ: {os.path.basename(pdf_file)}")
            print(f"ğŸ“Š PDFå¤§å°: {pdf_size/1024:.1f} KB")
            print(f"ğŸ“„ åŒ…å«å›¾ç‰‡: {len(valid_images)} å¼ ")
            
            # å…³é—­å›¾ç‰‡å¯¹è±¡
            for img in valid_images:
                img.close()
            
            # PDFç”ŸæˆæˆåŠŸåï¼Œåˆ é™¤åŸå§‹å›¾ç‰‡æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
            print(f"ğŸ§¹ æ¸…ç†åŸå§‹å›¾ç‰‡æ–‡ä»¶...")
            deleted_count = 0
            for img_info in image_files:
                try:
                    if os.path.exists(img_info['path']):
                        os.remove(img_info['path'])
                        deleted_count += 1
                        print(f"  âœ“ å·²åˆ é™¤: {img_info['filename']}")
                except Exception as e:
                    print(f"  âœ— åˆ é™¤å¤±è´¥: {img_info['filename']} - {e}")
            
            # å°è¯•åˆ é™¤imagesç›®å½•ï¼ˆå¦‚æœä¸ºç©ºï¼‰
            try:
                images_dir = os.path.dirname(image_files[0]['path']) if image_files else None
                if images_dir and os.path.exists(images_dir) and not os.listdir(images_dir):
                    os.rmdir(images_dir)
                    print(f"  âœ“ å·²åˆ é™¤ç©ºç›®å½•: images/")
            except:
                pass
            
            print(f"âœ… å·²æ¸…ç† {deleted_count} ä¸ªå›¾ç‰‡æ–‡ä»¶")
            return True
            
        except ImportError:
            print("âŒ PIL/Pillowæœªå®‰è£…ï¼Œæ— æ³•ç”ŸæˆPDF")
            print("ğŸ’¡ è¯·è¿è¡Œ: pip install Pillow")
            return False
        except Exception as e:
            print(f"âŒ å›¾ç‰‡è½¬PDFå¤±è´¥: {e}")
            return False

    def convert_html_to_pdf(self, html_file, pdf_file):
        """å°†HTMLè½¬æ¢ä¸ºPDF - å¤‡ç”¨æ–¹æ³•"""
        try:
            # ä½¿ç”¨weasyprintè½¬æ¢
            import weasyprint
            
            # è®¾ç½®CSSå­—ä½“é…ç½®ä»¥æ”¯æŒä¸­æ–‡
            font_config = weasyprint.text.fonts.FontConfiguration()
            
            html_doc = weasyprint.HTML(filename=html_file)
            html_doc.write_pdf(pdf_file, font_config=font_config)
            
            return True
            
        except ImportError:
            print("âš ï¸  weasyprintæœªå®‰è£…ï¼Œæ— æ³•ç”ŸæˆPDFæ–‡ä»¶")
            return False
        except Exception as e:
            print(f"HTMLè½¬PDFå¤±è´¥: {e}")
            return False

    def find_downloadable_files(self, soup, base_url):
        """æŸ¥æ‰¾å¯ä¸‹è½½çš„æ–‡ä»¶é“¾æ¥"""
        downloads = []
        
        pdf_keywords = ['pdf', 'æ ·æœ¬', 'æ‰‹å†Œ', 'æŠ€æœ¯æ‰‹å†Œ', 'äº§å“æ ·æœ¬', 'è¯´æ˜ä¹¦', 'ä¸‹è½½', 'manual', 'catalog', 'datasheet']
        
        for link in soup.find_all('a', href=True):
            href = link.get('href', '').strip()
            link_text = link.get_text().strip()
            
            if not href:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹è½½é“¾æ¥
            is_download = False
            
            # ç›´æ¥PDFé“¾æ¥
            if href.lower().endswith('.pdf'):
                is_download = True
            # åŒ…å«ä¸‹è½½å…³é”®è¯çš„é“¾æ¥æ–‡æœ¬
            elif any(keyword in link_text.lower() for keyword in pdf_keywords):
                is_download = True
            # åŒ…å«ä¸‹è½½å…³é”®è¯çš„URL
            elif any(keyword in href.lower() for keyword in ['pdf', 'download', 'file', 'doc']):
                is_download = True
            
            if is_download:
                # è½¬æ¢ä¸ºç»å¯¹URL
                if href.startswith('//'):
                    href = 'https:' + href
                elif href.startswith('/'):
                    href = urljoin(base_url, href)
                elif not href.startswith('http'):
                    href = urljoin(base_url, href)
                
                downloads.append({
                    'url': href,
                    'title': link_text or 'æœªå‘½åæ–‡ä»¶',
                    'type': 'PDF' if '.pdf' in href.lower() else 'æ–‡æ¡£'
                })
        
        return downloads

    def download_file(self, file_url, save_path, file_title=""):
        """ä¸‹è½½æ–‡ä»¶ - æ”¹è¿›ç‰ˆï¼Œæ”¯æŒåˆ†å—ä¸‹è½½å’Œè¿›åº¦æ˜¾ç¤ºï¼Œå¢é‡é€»è¾‘"""
        try:
            # å¢é‡é€»è¾‘ï¼šé¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨
            if os.path.exists(save_path):
                print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {file_title}")
                return False
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡
            if file_url in self.processed_urls:
                print(f"URLå·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {file_title}")
                return False
            
            print(f"ğŸ”„ ä¸‹è½½: {file_title}")
            print(f"ğŸ“ é“¾æ¥: {file_url}")
            
            # ä½¿ç”¨æµå¼ä¸‹è½½
            with self.session.get(file_url, stream=True, timeout=self.config['spider_settings']['download_timeout']) as response:
                response.raise_for_status()
                
                total_size = int(response.headers.get('content-length', 0))
                downloaded_size = 0
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=1024):
                        if chunk:
                            f.write(chunk)
                            downloaded_size += len(chunk)
                            if total_size > 0:
                                progress = (downloaded_size / total_size) * 100
                                print(f"\rğŸ“¥ ä¸‹è½½è¿›åº¦: {progress:.1f}% ({downloaded_size/1024:.1f}KB/{total_size/1024:.1f}KB)", end='')
                print()  # æ¢è¡Œ
            
            file_size = os.path.getsize(save_path)
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {file_title}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size/1024:.1f} KB")
            print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {save_path}")
            
            # æ–‡ä»¶ä¿¡æ¯ä¸å†ä¿å­˜ä¸ºtxtæ–‡ä»¶ï¼Œä»…åœ¨æ§åˆ¶å°æ˜¾ç¤º
            
            self.processed_urls.add(file_url)
            self.new_files.append({
                'type': 'PDF' if '.pdf' in file_url.lower() else 'æ–‡æ¡£',
                'title': file_title,
                'path': save_path,
                'url': file_url,
                'size': file_size
            })
            return True
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¼‚å¸¸: {file_title} - {str(e)}")
            return False

    def process_product_page(self, category_name, product_name, product_url):
        """å¤„ç†å•ä¸ªäº§å“é¡µé¢ - å®Œæ•´ç‰ˆ"""
        try:
            print(f"\n{'='*80}")
            print(f"ğŸ” å¤„ç†äº§å“: {category_name} -> {product_name}")
            print(f"ğŸ”— é¡µé¢é“¾æ¥: {product_url}")
            print(f"{'='*80}")
            
            response = self.safe_request(product_url)
            if not response:
                print(f"âœ— æ— æ³•è®¿é—®é¡µé¢: {product_url}")
                return 0
            
            # æ£€æµ‹ç¼–ç 
            response.encoding = chardet.detect(response.content)['encoding'] or 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # åˆ›å»ºäº§å“ç›®å½•
            product_dir = os.path.join(self.base_dir, category_name, self.clean_filename(product_name))
            os.makedirs(product_dir, exist_ok=True)
            
            downloaded_count = 0
            
            # 1. æå–äº§å“ç‰¹ç‚¹å¹¶ç”ŸæˆPDFï¼ˆå¢é‡é€»è¾‘ï¼‰
            print("ğŸ“‹ æ£€æŸ¥äº§å“ç‰¹ç‚¹PDF...")
            
            # æ£€æŸ¥PDFæ˜¯å¦å·²å­˜åœ¨
            expected_pdf_path = os.path.join(product_dir, f"{self.clean_filename(product_name)}_äº§å“ç‰¹ç‚¹.pdf")
            
            if os.path.exists(expected_pdf_path):
                print(f"äº§å“ç‰¹ç‚¹PDFå·²å­˜åœ¨ï¼Œè·³è¿‡: {os.path.basename(expected_pdf_path)}")
            else:
                print("ğŸ“‹ æå–äº§å“ç‰¹ç‚¹å’Œç”ŸæˆPDF...")
                features = self.extract_product_features_with_images(soup, product_name, product_url)
                
                if self.config['output_settings']['generate_pdf_features']:
                    pdf_file, image_count = self.create_product_features_pdf(features, product_dir)
                    if pdf_file:
                        self.new_files.append({
                            'type': 'äº§å“ç‰¹ç‚¹PDF',
                            'title': f"{product_name} - äº§å“ç‰¹ç‚¹è¯¦æƒ…",
                            'path': pdf_file,
                            'url': product_url,
                            'size': os.path.getsize(pdf_file) if os.path.exists(pdf_file) else 0
                        })
                        print(f"âœ… äº§å“ç‰¹ç‚¹PDFå·²ç”Ÿæˆ (åŒ…å«{image_count}å¼ å›¾ç‰‡)")
            
            # 2. æŸ¥æ‰¾å¹¶ä¸‹è½½PDFæ–‡ä»¶
            print("ğŸ“ æŸ¥æ‰¾æŠ€æœ¯æ‰‹å†Œå’Œæ ·æœ¬æ–‡ä»¶...")
            download_files = self.find_downloadable_files(soup, product_url)
            
            if download_files:
                print(f"âœ… æ‰¾åˆ° {len(download_files)} ä¸ªPDFæ–‡ä»¶")
                
                for i, file_info in enumerate(download_files, 1):
                    file_url = file_info['url']
                    file_title = file_info['title']
                    file_type = file_info['type']
                    
                    print(f"\nğŸ“„ [{i}/{len(download_files)}] å¤„ç†PDFæ–‡ä»¶")
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    filename = self.clean_filename(file_title)
                    if not filename.lower().endswith('.pdf') and file_type == 'PDF':
                        filename += '.pdf'
                    elif not '.' in filename:
                        filename += '.pdf'
                    
                    save_path = os.path.join(product_dir, filename)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                    if os.path.exists(save_path):
                        print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                        continue
                    
                    # ä¸‹è½½æ–‡ä»¶
                    if self.download_file(file_url, save_path, file_title):
                        downloaded_count += 1
                    
                    time.sleep(1)  # ä¸‹è½½é—´éš”
            else:
                print("âš ï¸  æœªæ‰¾åˆ°PDFä¸‹è½½æ–‡ä»¶")
            
            print(f"\nâœ… {product_name} å¤„ç†å®Œæˆ")
            print(f"ğŸ“Š ä¸‹è½½PDFæ–‡ä»¶: {downloaded_count} ä¸ª")
            print(f"ğŸ“ ä¿å­˜ç›®å½•: {product_dir}")
            
            return downloaded_count
            
        except Exception as e:
            print(f"âŒ å¤„ç†äº§å“é¡µé¢å¤±è´¥: {product_name} - {str(e)}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return 0

    def crawl_all_products(self):
        """çˆ¬å–æ‰€æœ‰äº§å“"""
        print(f"\n{'='*100}")
        print(f"ğŸš€ æ—¥ç«‹ç»ˆæçˆ¬è™«å¼€å§‹è¿è¡Œ")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ’¾ å­˜å‚¨è·¯å¾„: {self.base_dir}")
        
        if self.is_first_run:
            print(f"ğŸ†• é¦–æ¬¡è¿è¡Œ - æ‰§è¡Œå…¨é‡çˆ¬å–")
        else:
            print(f"ğŸ”„ å¢é‡è¿è¡Œ - åªä¸‹è½½æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶")
        
        print(f"{'='*100}")
        
        total_downloaded = 0
        total_products = sum(len(category['products']) for category in self.product_categories.values())
        processed_products = 0
        
        for category_key, category_info in self.product_categories.items():
            category_name = category_info['name']
            products = category_info['products']
            
            print(f"\n{'='*80}")
            print(f"ğŸ“‚ å¤„ç†åˆ†ç±»: {category_name} ({len(products)}ä¸ªäº§å“)")
            print(f"{'='*80}")
            
            for product in products:
                product_name = product['name']
                product_url = product['url']
                
                processed_products += 1
                print(f"\nè¿›åº¦: {processed_products}/{total_products} äº§å“")
                
                downloaded = self.process_product_page(category_name, product_name, product_url)
                total_downloaded += downloaded
                
                time.sleep(2)  # äº§å“é—´éš”
        
        # ä¿å­˜å¤„ç†è®°å½•
        self.save_processed_urls()
        
    
        if self.new_files:
            self.send_notifications()
        
        return total_downloaded


    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥"""
        try:
            timestamp = str(round(time.time() * 1000))
            string_to_sign = f'{timestamp}\n{self.SECRET}'
            hmac_code = hmac.new(self.SECRET.encode(), string_to_sign.encode(), hashlib.sha256).digest()
            sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))

            url = f'https://oapi.dingtalk.com/robot/send?access_token={self.ACCESS_TOKEN}&timestamp={timestamp}&sign={sign}'
            headers = {'Content-Type': 'application/json'}
            data = {
                "msgtype": "text",
                "text": {"content": message},
                "at": {"isAtAll": False}
            }

            response = requests.post(url, json=data, headers=headers)
            print(f"ğŸ“¨ é’‰é’‰é€šçŸ¥å“åº”ï¼š{response.status_code} {response.text}")
            return response.status_code == 200
        except Exception as e:
            print(f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {e}")
            return False

    def send_notifications(self):
        """å‘é€æ–°å¢æ–‡ä»¶é€šçŸ¥"""
        try:
            if not self.new_files:
                return
            
            # æ§åˆ¶å°é€šçŸ¥
            if self.config.get("notification_settings", {}).get("enable_console", True):
                print(f"\nğŸ‰ çˆ¬å–å®Œæˆé€šçŸ¥:")
                print("=" * 60)
                print(f"ğŸ“Š å‘ç° {len(self.new_files)} ä¸ªæ–°æ–‡ä»¶:")
                
                # æŒ‰ç±»å‹ç»Ÿè®¡
                type_counts = {}
                for file_info in self.new_files:
                    file_type = file_info['type']
                    type_counts[file_type] = type_counts.get(file_type, 0) + 1
                
                for file_type, count in type_counts.items():
                    print(f"  ğŸ“ {file_type}: {count} ä¸ª")
                
                print(f"\nğŸ“‚ æœ€æ–°æ–‡ä»¶é¢„è§ˆ:")
                for file_info in self.new_files[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                    size_str = f" ({file_info['size']/1024:.1f}KB)" if 'size' in file_info else ""
                    print(f"  ğŸ“„ {file_info['title']}{size_str}")
                
                if len(self.new_files) > 5:
                    print(f"  ... è¿˜æœ‰ {len(self.new_files) - 5} ä¸ªæ–‡ä»¶")
                    
                print(f"\nğŸ’¾ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³: {self.base_dir}")
            
            # é’‰é’‰é€šçŸ¥
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            total_files = len(self.new_files)
            success_rate = 100.0  # å‡è®¾å…¨éƒ¨æˆåŠŸ
            
            if self.is_first_run:
                # ç¬¬ä¸€æ¬¡å…¨é‡çˆ¬å–é€šçŸ¥
                message = f"""âœ… æ—¥ç«‹ çˆ¬å–æˆåŠŸï¼Œè¯·åŠæ—¶å®¡æ ¸

ğŸ“Š ä¸‹è½½ç»Ÿè®¡:
  æˆåŠŸä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶
  æ€»æ–‡ä»¶æ•°: {total_files} ä¸ªæ–‡ä»¶
  æˆåŠŸç‡: {success_rate}%

ğŸ“ æ–‡ä»¶å­˜æ”¾è·¯å¾„: /srv/downloads/approved/
â° å®Œæˆæ—¶é—´: {current_time}"""
            else:
                # å¢é‡çˆ¬å–é€šçŸ¥
                message = f"""âœ… æ—¥ç«‹ å¢é‡çˆ¬å–æˆåŠŸï¼Œè¯·åŠæ—¶å®¡æ ¸

ğŸ“Š ä¸‹è½½ç»Ÿè®¡:
  æˆåŠŸä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶
  æ€»æ–‡ä»¶æ•°: {total_files} ä¸ªæ–‡ä»¶
  æˆåŠŸç‡: {success_rate}%
æ–‡ä»¶æ˜ç»†ï¼š"""
                
                # æ·»åŠ æ–‡ä»¶æ˜ç»†
                for file_info in self.new_files:
                    # æ„å»ºç›¸å¯¹è·¯å¾„ï¼ˆä»æ—¥ç«‹å¼€å§‹ï¼‰
                    relative_path = file_info['path'].replace('/srv/downloads/approved/', '')
                    message += f"\n{relative_path}"
                
                message += f"""

ğŸ“ æ–‡ä»¶å­˜æ”¾è·¯å¾„: /srv/downloads/approved/
â° å®Œæˆæ—¶é—´: {current_time}"""
            
            # å‘é€é’‰é’‰é€šçŸ¥
            self.send_dingtalk_notification(message)
            
        except Exception as e:
            print(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")

    def run_once(self):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çˆ¬å–"""
        start_time = datetime.now()
        
        try:
            downloaded_count = self.crawl_all_products()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            print(f"\nğŸ‰ æ—¥ç«‹ç»ˆæçˆ¬è™«ä»»åŠ¡å®Œæˆï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {duration}")
            print(f"ğŸ“Š æ€»å¤„ç†: {downloaded_count} ä¸ªæ–‡ä»¶")
            print(f"ğŸ†• æ–°å¢æ–‡ä»¶: {len(self.new_files)} ä¸ª")
            print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {os.path.abspath(self.base_dir)}")
            print(f"âœ¨ åŠŸèƒ½ç‰¹è‰²: PDFä¸‹è½½ + äº§å“ç‰¹ç‚¹å›¾ç‰‡æå– + è‡ªåŠ¨PDFç”Ÿæˆ")
            
            return downloaded_count
            
        except Exception as e:
            print(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return 0

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ—¥ç«‹ç»ˆæçˆ¬è™«å¯åŠ¨...")
    print("ğŸ¯ åŠŸèƒ½: PDFä¸‹è½½ + äº§å“ç‰¹ç‚¹å›¾ç‰‡æå– + è‡ªåŠ¨PDFç”Ÿæˆ + å¢é‡æ›´æ–° + è‡ªåŠ¨é€šçŸ¥")
    print("ğŸ“‹ æ¶µç›–: å˜é¢‘å™¨ã€æ ‡è¯†è®¾å¤‡ã€ç”µæœºã€PLCã€é¼“é£æœºã€ç”µåŠ¨è‘«èŠ¦")
    print("âœ¨ ç‰¹è‰²: å°†äº§å“ç‰¹ç‚¹å›¾ç‰‡æ•´åˆç”ŸæˆPDFæ–‡æ¡£")
    
    spider = HitachiUltimateSpider()
    
    # ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
    os.makedirs(spider.base_dir, exist_ok=True)
    
    try:
        downloaded_count = spider.run_once()
        
        if downloaded_count > 0:
            print(f"\nâœ… ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
            print(f"ğŸ’¡ æç¤º: äº§å“ç‰¹ç‚¹å·²ä¿å­˜ä¸ºPDFæ–‡ä»¶ï¼Œå¯ç›´æ¥æŸ¥çœ‹")
        else:
            print(f"\nâš ï¸  æ²¡æœ‰ä¸‹è½½åˆ°æ–°æ–‡ä»¶ï¼Œå¯èƒ½æ‰€æœ‰å†…å®¹éƒ½æ˜¯æœ€æ–°çš„")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"ğŸ’¥ è¿è¡Œå‡ºé”™: {str(e)}")

if __name__ == '__main__':
    main()