#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import json
import pickle
import platform
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote
import requests
import chardet

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException
from bs4 import BeautifulSoup

class KawasakiSpider:
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.base_url = "https://kawasakirobotics.cn"
        
        # æœåŠ¡å™¨å›ºå®šè·¯å¾„ï¼ˆæŒ‰è§„èŒƒè¦æ±‚ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨å½“å‰ç›®å½•
        if os.path.exists("/srv/downloads/approved/"):
            self.base_dir = "/srv/downloads/approved/å·å´"
        else:
            # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
            self.base_dir = os.path.join(os.getcwd(), "downloads", "å·å´")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.base_dir, exist_ok=True)
        
        self.processed_urls = self.load_processed_urls()
        self.new_files = []
        self.debug = True
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆå…¨é‡çˆ¬å–ï¼‰
        self.is_first_run = not os.path.exists(os.path.join(self.base_dir, 'processed_urls.pkl'))
        
        # é’‰é’‰é€šçŸ¥é…ç½®
        self.ACCESS_TOKEN = "1a431b6482e59ec652a6eab37705d50fd282dc426f0b829b3eb9a9c1b277ed24"
        self.SECRET = "SECc5e2168011dd97d4c511e06832d172f31635ef635771668e4e6003fce23c07bb"
        
        # åˆå§‹åŒ–WebDriver
        self.driver = None
        self.init_webdriver()
        
        # ä¸»è¦çˆ¬å–æ¨¡å— - åŸºäºHTMLç»“æ„åˆ†æ
        self.main_modules = [
            {
                'name': 'æœºå™¨äºº',
                'url': 'https://kawasakirobotics.cn/products-robots/',
                'categories': [
                    {'name': 'ä¸­å°å‹é€šç”¨æœºå™¨äºº~80kgè´Ÿè½½', 'url': 'https://kawasakirobotics.cn/robots-category/small-medium-payloads/'},
                    {'name': 'å¤§å‹é€šç”¨æœºå™¨äºº~300kgè´Ÿè½½', 'url': 'https://kawasakirobotics.cn/robots-category/large-payloads/'},
                    {'name': 'è¶…å¤§å‹é€šç”¨æœºå™¨äºº~1,500kgè´Ÿè½½', 'url': 'https://kawasakirobotics.cn/robots-category/extra-large-payloads/'},
                    {'name': 'åä½œæœºå™¨äºº', 'url': 'https://kawasakirobotics.cn/robots-category/dual-arm-scara/'},
                    {'name': 'ç å›æœºå™¨äºº', 'url': 'https://kawasakirobotics.cn/robots-category/palletizing/'},
                    {'name': 'é«˜é€Ÿåˆ†æ‹£æœºå™¨äºº', 'url': 'https://kawasakirobotics.cn/robots-category/pick-and-place/'},
                    {'name': 'åŒ»è¯æœºå™¨äºº', 'url': 'https://kawasakirobotics.cn/robots-category/medical/'},
                    {'name': 'ç„Šæ¥/åˆ‡å‰²æœºå™¨äºº', 'url': 'https://kawasakirobotics.cn/robots-category/arc-welding/'},
                    {'name': 'å–·æ¶‚æœºå™¨äºº', 'url': 'https://kawasakirobotics.cn/robots-category/painting/'},
                    {'name': 'æ™¶åœ†æ¬è¿æœºå™¨äºº', 'url': 'https://kawasakirobotics.cn/robots-category/wafer/'}
                ]
            },
            {
                'name': 'æ§åˆ¶æŸœ',
                'url': 'https://kawasakirobotics.cn/controllers-category/',
                'categories': [
                    {'name': 'F æ§åˆ¶æŸœ', 'url': 'https://kawasakirobotics.cn/controllers-category/f-controllers/'},
                    {'name': 'E æ§åˆ¶æŸœ', 'url': 'https://kawasakirobotics.cn/controllers-category/e-controllers/'},
                    {
                        'name': 'é˜²çˆ† Eæ§åˆ¶æŸœ', 
                        'url': 'https://kawasakirobotics.cn/controllers-category/explosion-proof-e-controllers/',
                                            'subcategories': [
                        {'name': 'E25äºšæ´²_E35ç¾æ´²_E45æ¬§æ´²', 'url': 'https://kawasakirobotics.cn/controllers-category/explosion-proof-e-controllers/'}
                    ]
                    }
                ]
            },
            {
                'name': 'å…¶ä»–äº§å“',
                'url': 'https://kawasakirobotics.cn/others-category/',
                'categories': [
                    {'name': 'ç¼–ç¨‹å·¥å…·', 'url': 'https://kawasakirobotics.cn/others-category/programming-tool/'},
                    {'name': 'è§†è§‰é€‰é…', 'url': 'https://kawasakirobotics.cn/others-category/vision-option/'},
                    {'name': 'å®‰å…¨ç›‘æ§é€‰é…', 'url': 'https://kawasakirobotics.cn/others-category/safety/'},
                    {'name': 'ç›‘æ§ä¸ç®¡æŠ¤å·¥å…·', 'url': 'https://kawasakirobotics.cn/others-category/operation-maintenance-monitoring-tool/'}
                ]
            },
            {
                'name': 'K-AddOn',
                'url': 'https://kawasakirobotics.cn/products-kaddon/',
                'categories': [
                    {'name': 'K-AddOn äº§å“', 'url': 'https://kawasakirobotics.cn/products-kaddon/'},
                    {'name': 'K-AddOn è½¯ä»¶', 'url': 'https://kawasakirobotics.cn/products-kaddon/software/'},
                    {'name': 'K-AddOn ç¡¬ä»¶', 'url': 'https://kawasakirobotics.cn/products-kaddon/hardware/'}
                ]
            }
        ]
        
    def init_webdriver(self):
        """åˆå§‹åŒ–Chrome WebDriver"""
        try:
            # æ£€æµ‹ç³»ç»Ÿæ¶æ„
            system = platform.system()
            machine = platform.machine()
            
            # ç¡®å®šchromedriverè·¯å¾„
            # åœ¨æœåŠ¡å™¨ä¸ŠæŸ¥æ‰¾chromedriverç›®å½•
            if os.path.exists("/srv/crawler/chromedriver_downloads"):
                chromedriver_dir = "/srv/crawler/chromedriver_downloads"
            else:
                chromedriver_dir = os.path.join(os.getcwd(), "chromedriver_downloads")
            
            if system == "Darwin":  # macOS
                if machine == "arm64":
                    chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_mac-arm64", "chromedriver-mac-arm64", "chromedriver")
                else:
                    chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_mac-x64", "chromedriver-mac-x64", "chromedriver")
            elif system == "Linux":
                chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_linux64", "chromedriver-linux64", "chromedriver")
            elif system == "Windows":
                chromedriver_path = os.path.join(chromedriver_dir, "chromedriver_win64", "chromedriver-win64", "chromedriver.exe")
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ: {system}")
            
            if not os.path.exists(chromedriver_path):
                raise Exception(f"ChromeDriveræœªæ‰¾åˆ°: {chromedriver_path}")
            
            # è®¾ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # åˆå§‹åŒ–WebDriver
            service = Service(executable_path=chromedriver_path)
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(10)
            
            self.log("âœ… Chrome WebDriveråˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.log(f"âŒ WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URLåˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_urls.pkl')
        if os.path.exists(processed_file):
            try:
                with open(processed_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return set()
        return set()
    
    def save_processed_urls(self):
        """ä¿å­˜å·²å¤„ç†çš„URLåˆ—è¡¨"""
        processed_file = os.path.join(self.base_dir, 'processed_urls.pkl')
        with open(processed_file, 'wb') as f:
            pickle.dump(self.processed_urls, f)
    
    def visit_page(self, url, retry_count=3):
        """è®¿é—®é¡µé¢å¹¶è¿”å›BeautifulSoupå¯¹è±¡"""
        for attempt in range(retry_count):
            try:
                self.log(f"ğŸ”„ è®¿é—®é¡µé¢ (å°è¯•{attempt+1}/{retry_count}): {url}")
                self.driver.get(url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                WebDriverWait(self.driver, 30).until(
                    lambda driver: driver.execute_script("return document.readyState") == "complete"
                )
                
                # é¢å¤–ç­‰å¾…JavaScriptæ‰§è¡Œ
                time.sleep(3)
                
                # å°è¯•æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                # è·å–é¡µé¢æºç 
                page_source = self.driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')
                
                return soup
                
            except Exception as e:
                self.log(f"âŒ é¡µé¢è®¿é—®å¤±è´¥ (å°è¯•{attempt+1}): {str(e)}")
                time.sleep(5)
        
        self.log(f"âŒ é¡µé¢è®¿é—®å®Œå…¨å¤±è´¥: {url}")
        return None
    
    def find_product_links(self, soup, page_url):
        """ä»åˆ†ç±»é¡µé¢ä¸­æŸ¥æ‰¾äº§å“é“¾æ¥"""
        products = []
        
        try:
            # æŸ¥æ‰¾äº§å“ç½‘æ ¼ä¸­çš„äº§å“é“¾æ¥
            product_columns = soup.find_all('div', class_='wp-block-column krobot-pattern-products__item')
            self.log(f"ğŸ” æ‰¾åˆ° {len(product_columns)} ä¸ªäº§å“åˆ—")
            
            for column in product_columns:
                try:
                    # æŸ¥æ‰¾äº§å“æ ‡é¢˜
                    title_element = column.find('h2', class_='krobot-pattern-products__title')
                    if title_element:
                        product_title = title_element.get_text().strip()
                        
                        # æŸ¥æ‰¾"æ›´å¤šçš„"æŒ‰é’®é“¾æ¥
                        more_button = column.find('a', class_='wp-block-button__link')
                        if more_button and 'æ›´å¤šçš„' in more_button.get_text():
                            product_url = more_button.get('href', '')
                            if product_url:
                                # æ„å»ºå®Œæ•´URL
                                if not product_url.startswith('http'):
                                    product_url = urljoin(self.base_url, product_url)
                                
                                # æŸ¥æ‰¾äº§å“è§„æ ¼ä¿¡æ¯
                                specs = {}
                                spec_list = column.find('ul', class_='krobot-pattern-products__info')
                                if spec_list:
                                    spec_items = spec_list.find_all('li')
                                    for item in spec_items:
                                        strong = item.find('strong')
                                        if strong:
                                            key = strong.get_text().strip()
                                            # è·å–strongæ ‡ç­¾åçš„æ–‡æœ¬
                                            value = item.get_text().replace(key, '').strip()
                                            specs[key] = value
                                
                                products.append({
                                    'title': product_title,
                                    'url': product_url,
                                    'specs': specs,
                                    'type': 'product_page'
                                })
                                
                                self.log(f"   âœ… æ‰¾åˆ°äº§å“: {product_title} -> {product_url}")
                
                except Exception as e:
                    self.log(f"   âŒ å¤„ç†äº§å“åˆ—æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            if products:
                self.log(f"ğŸ“¦ åœ¨é¡µé¢ä¸­æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
            else:
                self.log(f"âŒ é¡µé¢ä¸­æœªæ‰¾åˆ°äº§å“: {page_url}")
                
        except Exception as e:
            self.log(f"âŒ æŸ¥æ‰¾äº§å“é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
        
        return products
    
    def find_download_links(self, soup, page_url):
        """ä»äº§å“è¯¦æƒ…é¡µé¢ä¸­æŸ¥æ‰¾ä¸‹è½½é“¾æ¥"""
        downloads = []
        
        try:
            # æŸ¥æ‰¾èµ„æ–™ä¸‹è½½åŒºåŸŸ
            download_section = soup.find('div', class_='product-download product-section entry-content')
            if download_section:
                self.log(f"ğŸ” æ‰¾åˆ°èµ„æ–™ä¸‹è½½åŒºåŸŸ")
                
                # æŸ¥æ‰¾ä¸‹è½½åˆ—è¡¨
                download_list = download_section.find('ul', class_='product-download__list')
                if download_list:
                    download_items = download_list.find_all('li', class_='product-download__item')
                    
                    for item in download_items:
                        try:
                            # è·å–æ–‡ä»¶ç±»å‹æè¿°
                            text_element = item.find('span', class_='product-download__text')
                            file_type = text_element.get_text().strip() if text_element else "æœªçŸ¥ç±»å‹"
                            
                            # è·å–ä¸‹è½½é“¾æ¥
                            download_link = item.find('a', class_='product-download__btn')
                            if download_link:
                                href = download_link.get('href', '')
                                if href:
                                    # æ„å»ºå®Œæ•´URL
                                    if not href.startswith('http'):
                                        full_url = urljoin(page_url, href)
                                    else:
                                        full_url = href
                                    
                                    # ä»URLè·å–æ–‡ä»¶å
                                    filename = os.path.basename(urlparse(full_url).path)
                                    if not filename:
                                        filename = f"{file_type}_{int(time.time())}.pdf"
                                    
                                    downloads.append({
                                        'title': file_type,
                                        'url': full_url,
                                        'filename': filename,
                                        'type': 'direct_download'
                                    })
                                    
                                    self.log(f"   âœ… æ‰¾åˆ°ä¸‹è½½: {file_type} -> {full_url}")
                        
                        except Exception as e:
                            self.log(f"   âŒ å¤„ç†ä¸‹è½½é¡¹æ—¶å‡ºé”™: {str(e)}")
                            continue
                
                if downloads:
                    self.log(f"ğŸ“ åœ¨é¡µé¢ä¸­æ‰¾åˆ° {len(downloads)} ä¸ªä¸‹è½½æ–‡ä»¶")
                else:
                    self.log(f"âŒ èµ„æ–™ä¸‹è½½åŒºåŸŸä¸­æœªæ‰¾åˆ°ä¸‹è½½é“¾æ¥")
            else:
                self.log(f"âŒ é¡µé¢ä¸­æœªæ‰¾åˆ°èµ„æ–™ä¸‹è½½åŒºåŸŸ: {page_url}")
                
        except Exception as e:
            self.log(f"âŒ æŸ¥æ‰¾ä¸‹è½½é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
        
        return downloads
    
    def download_file(self, url, filename, base_folder, file_type_category=None):
        """ä¸‹è½½æ–‡ä»¶åˆ°æŒ‡å®šæ–‡ä»¶å¤¹"""
        try:
            # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹åˆ†ç±»ï¼Œåˆ›å»ºå¯¹åº”çš„å­æ–‡ä»¶å¤¹
            if file_type_category:
                target_folder = os.path.join(base_folder, file_type_category)
                # åªåœ¨æœ‰æ–‡ä»¶æ—¶åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹
                os.makedirs(target_folder, exist_ok=True)
                self.log(f"ğŸ“ åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹: {target_folder}")
            else:
                target_folder = base_folder
            
            file_path = os.path.join(target_folder, filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å¤§å°åˆç†ï¼ˆé¿å…ä¸‹è½½æŸåçš„æ–‡ä»¶ï¼‰
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                if file_size > 1024:  # å¦‚æœæ–‡ä»¶å¤§äº1KBï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆæ–‡ä»¶
                    self.log(f"ğŸ“ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                    return True
                else:
                    self.log(f"ğŸ”„ æ–‡ä»¶å­˜åœ¨ä½†å¤§å°å¼‚å¸¸({file_size}å­—èŠ‚)ï¼Œé‡æ–°ä¸‹è½½: {filename}")
                    os.remove(file_path)
            
            # ä¸‹è½½æ–‡ä»¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': self.base_url
            }
            
            response = requests.get(url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(file_path)
            self.log(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} -> {target_folder} ({file_size} bytes)")
            
            self.new_files.append({
                'filename': filename,
                'path': file_path,
                'url': url,
                'size': file_size,
                'category': file_type_category
            })
            
            return True
            
        except Exception as e:
            self.log(f"âŒ ä¸‹è½½å¤±è´¥ {filename}: {str(e)}")
            return False
    
    def generate_clean_filename(self, title, download_filename="", url=""):
        """ç”Ÿæˆæ¸…æ´çš„æ–‡ä»¶å"""
        try:
            # å¦‚æœæœ‰æŒ‡å®šçš„ä¸‹è½½æ–‡ä»¶åï¼Œä¼˜å…ˆä½¿ç”¨
            if download_filename:
                return download_filename
            
            # å¦åˆ™ä»æ ‡é¢˜ç”Ÿæˆæ–‡ä»¶å
            clean_title = re.sub(r'[^\w\s\-\u4e00-\u9fff]', '', title)
            clean_title = re.sub(r'\s+', '_', clean_title.strip())
            
            # æˆªæ–­è¿‡é•¿çš„æ–‡ä»¶å
            if len(clean_title) > 100:
                clean_title = clean_title[:100]
            
            # ä»URLè·å–æ–‡ä»¶æ‰©å±•å
            if url:
                parsed_url = urlparse(url)
                path = parsed_url.path
                ext = os.path.splitext(path)[1]
                
                if not ext:
                    ext = '.pdf'  # é»˜è®¤ä¸ºPDF
            else:
                ext = '.pdf'
            
            return f"{clean_title}{ext}"
            
        except Exception as e:
            self.log(f"âš ï¸ æ–‡ä»¶åç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"document_{int(time.time())}.pdf"
    
    def extract_product_model(self, product_title):
        """ä»äº§å“æ ‡é¢˜ä¸­æå–äº§å“å‹å·"""
        try:
            # æ”¹è¿›çš„å·å´æœºå™¨äººå‹å·æ¨¡å¼ - æ›´ç²¾ç¡®çš„åŒ¹é…
            patterns = [
                # RSç³»åˆ— - æ›´ç²¾ç¡®çš„åŒ¹é…
                r'RS\d{3}[NLHX]?',  # RS003N, RS003L, RS003H, RS003Xç­‰
                r'RS\d{3}[NLHX]?\s*[NLHX]',  # å¤„ç†ç©ºæ ¼åˆ†éš”çš„æƒ…å†µ
                
                # BAç³»åˆ—
                r'BA\d{3}[NLHX]?',  # BA006N, BA006L, BA006Hç­‰
                r'BA\d{3}[NLHX]?\s*[NLHX]',
                
                # MSç³»åˆ—
                r'MS\d{3}[NLHX]?',  # MS005N, MS005L, MS005Hç­‰
                r'MS\d{3}[NLHX]?\s*[NLHX]',
                
                # VSç³»åˆ—
                r'VS\d{3}[NLHX]?',  # VS050N, VS050L, VS050Hç­‰
                r'VS\d{3}[NLHX]?\s*[NLHX]',
                
                # KJç³»åˆ—
                r'KJ\d{3}[NLHX]?',  # KJ264N, KJ264Lç­‰
                r'KJ\d{3}[NLHX]?\s*[NLHX]',
                
                # Fç³»åˆ—æ§åˆ¶æŸœ
                r'F\d{3}[NLHX]?',   # F001N, F001L, F001Hç­‰
                r'F\d{3}[NLHX]?\s*[NLHX]',
                
                # Eç³»åˆ—æ§åˆ¶æŸœ
                r'E\d{3}[NLHX]?',   # E001N, E001L, E001Hç­‰
                r'E\d{3}[NLHX]?\s*[NLHX]',
                
                # å…¶ä»–å¯èƒ½çš„å‹å·æ ¼å¼
                r'[A-Z]{1,3}\d{2,4}[NLHX]?',  # é€šç”¨æ¨¡å¼
                r'[A-Z]{1,3}\d{2,4}\s*[NLHX]',  # å¸¦ç©ºæ ¼åˆ†éš”
            ]
            
            # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
            for pattern in patterns:
                match = re.search(pattern, product_title, re.IGNORECASE)
                if match:
                    model = match.group().upper()
                    # æ ‡å‡†åŒ–å‹å·æ ¼å¼
                    model = self.standardize_model_name(model)
                    self.log(f"ğŸ·ï¸ æå–åˆ°å‹å·: {model} (ä»æ ‡é¢˜: {product_title})")
                    return model
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†å‹å·ï¼Œå°è¯•ä»URLè·¯å¾„ä¸­æå–
            # è¿™é€šå¸¸æ›´å¯é ï¼Œå› ä¸ºURLé€šå¸¸åŒ…å«å‡†ç¡®çš„å‹å·ä¿¡æ¯
            if hasattr(self, 'current_url') and self.current_url:
                url_model = self.extract_model_from_url(self.current_url)
                if url_model:
                    self.log(f"ğŸ·ï¸ ä»URLæå–åˆ°å‹å·: {url_model}")
                    return url_model
            
            # æœ€åçš„åå¤‡æ–¹æ¡ˆ - ä½¿ç”¨æ ‡é¢˜çš„å‰å‡ ä¸ªå­—ç¬¦
            clean_title = re.sub(r'[^\w\s\u4e00-\u9fff]', '', product_title)
            words = clean_title.split()
            if words:
                for word in words:
                    if len(word) >= 2 and not word.isdigit():
                        # å°è¯•ä»å•è¯ä¸­æå–å‹å·ä¿¡æ¯
                        word_model = self.extract_model_from_word(word)
                        if word_model:
                            return word_model
                        return word[:10]  # é™åˆ¶é•¿åº¦
            
            # æœ€ç»ˆåå¤‡æ–¹æ¡ˆ
            fallback_name = f"äº§å“_{int(time.time())}"
            self.log(f"âš ï¸ æ— æ³•æå–å‹å·ï¼Œä½¿ç”¨åå¤‡åç§°: {fallback_name}")
            return fallback_name
            
        except Exception as e:
            self.log(f"âš ï¸ äº§å“å‹å·æå–å¤±è´¥: {str(e)}")
            return f"äº§å“_{int(time.time())}"
    
    def standardize_model_name(self, model):
        """æ ‡å‡†åŒ–å‹å·åç§°ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´"""
        try:
            # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
            model = re.sub(r'\s+', '', model)
            
            # ç¡®ä¿å‹å·æ ¼å¼æ­£ç¡®
            # ä¾‹å¦‚ï¼šRS003N, RS003L, RS003H, RS003X
            if re.match(r'^[A-Z]{1,3}\d{2,4}[NLHX]?$', model):
                return model
            
            # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œå°è¯•ä¿®å¤
            # æå–åŸºç¡€å‹å·å’Œåç¼€
            base_match = re.match(r'^([A-Z]{1,3}\d{2,4})([NLHX]?)$', model)
            if base_match:
                base, suffix = base_match.groups()
                return f"{base}{suffix}"
            
            return model
            
        except Exception as e:
            self.log(f"âš ï¸ å‹å·æ ‡å‡†åŒ–å¤±è´¥: {str(e)}")
            return model
    
    def extract_model_from_url(self, url):
        """ä»URLä¸­æå–äº§å“å‹å·"""
        try:
            # è§£æURLè·¯å¾„
            parsed_url = urlparse(url)
            path_parts = parsed_url.path.strip('/').split('/')
            
            # æŸ¥æ‰¾åŒ…å«å‹å·çš„è·¯å¾„æ®µ
            for part in path_parts:
                # å°è¯•åŒ¹é…å‹å·æ¨¡å¼
                model_match = re.search(r'([A-Z]{1,3}\d{2,4}[NLHX]?)', part, re.IGNORECASE)
                if model_match:
                    model = model_match.group().upper()
                    return self.standardize_model_name(model)
            
            return None
            
        except Exception as e:
            self.log(f"âš ï¸ ä»URLæå–å‹å·å¤±è´¥: {str(e)}")
            return None
    
    def extract_model_from_word(self, word):
        """ä»å•è¯ä¸­æå–å‹å·ä¿¡æ¯"""
        try:
            # å°è¯•åŒ¹é…å‹å·æ¨¡å¼
            model_match = re.search(r'([A-Z]{1,3}\d{2,4}[NLHX]?)', word, re.IGNORECASE)
            if model_match:
                model = model_match.group().upper()
                return self.standardize_model_name(model)
            
            return None
            
        except Exception as e:
            return None
    
    def categorize_file_type(self, file_type, filename, url):
        """æ ¹æ®ç½‘é¡µä¸Šæ˜¾ç¤ºçš„æ–‡ä»¶ç±»å‹åˆ†ç±»åç§°æ¥åˆ†ç±»æ–‡ä»¶"""
        try:
            # ç›´æ¥ä½¿ç”¨ç½‘é¡µä¸Šæ˜¾ç¤ºçš„åˆ†ç±»åç§°ï¼Œè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
            file_type_clean = file_type.strip()
            
            # æ‰‹å†Œç±» - åŒ…å«"æ‰‹å†Œ"å…³é”®è¯
            if 'æ‰‹å†Œ' in file_type_clean:
                return 'æ‰‹å†Œ'
            
            # CADç±» - åŒ…å«"CAD"å…³é”®è¯
            if 'CAD' in file_type_clean:
                return 'CAD'
            
            # è§„æ ¼ç±» - åŒ…å«"è§„æ ¼"å…³é”®è¯
            if 'è§„æ ¼' in file_type_clean:
                return 'è§„æ ¼ä¹¦'
            
            # è½¯ä»¶ç±» - åŒ…å«"è½¯ä»¶"å…³é”®è¯
            if 'è½¯ä»¶' in file_type_clean:
                return 'è½¯ä»¶'
            
            # è§†é¢‘ç±» - åŒ…å«"è§†é¢‘"å…³é”®è¯
            if 'è§†é¢‘' in file_type_clean:
                return 'å…¶ä»–æ–‡æ¡£'
            
            # å…¶ä»–æ–‡æ¡£ç±» - åŒ…å«"æ–‡æ¡£"ã€"èµ„æ–™"ç­‰å…³é”®è¯
            if any(keyword in file_type_clean for keyword in ['æ–‡æ¡£', 'èµ„æ–™', 'æ–‡ä»¶']):
                return 'å…¶ä»–æ–‡æ¡£'
            
            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œæ ¹æ®å¸¸è§ç±»å‹è¿›è¡Œæ™ºèƒ½åˆ†ç±»
            if any(keyword in file_type_clean.lower() for keyword in ['manual', 'handbook', 'guide', 'instruction']):
                return 'æ‰‹å†Œ'
            elif any(keyword in file_type_clean.lower() for keyword in ['drawing', 'model', 'step', 'dxf', 'dwg']):
                return 'CAD'
            elif any(keyword in file_type_clean.lower() for keyword in ['spec', 'specification', 'parameter', 'technical']):
                return 'è§„æ ¼ä¹¦'
            elif any(keyword in file_type_clean.lower() for keyword in ['software', 'program', 'exe', 'msi']):
                return 'è½¯ä»¶'
            elif any(keyword in file_type_clean.lower() for keyword in ['video', 'demo', 'tutorial']):
                return 'å…¶ä»–æ–‡æ¡£'
            
            # é»˜è®¤åˆ†ç±»ä¸ºå…¶ä»–æ–‡æ¡£
            return 'å…¶ä»–æ–‡æ¡£'
            
        except Exception as e:
            self.log(f"âš ï¸ æ–‡ä»¶ç±»å‹åˆ†ç±»å¤±è´¥: {str(e)}")
            return 'å…¶ä»–æ–‡æ¡£'
    
    def create_product_folder_structure(self, product_model, base_folder):
        """ä¸ºäº§å“åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„"""
        try:
            # åˆ›å»ºäº§å“ä¸»æ–‡ä»¶å¤¹
            product_folder = os.path.join(base_folder, product_model)
            os.makedirs(product_folder, exist_ok=True)
            
            # ä¸é¢„å…ˆåˆ›å»ºå­åˆ†ç±»æ–‡ä»¶å¤¹ï¼Œåªåœ¨æœ‰æ–‡ä»¶æ—¶åˆ›å»º
            self.log(f"ğŸ“ åˆ›å»ºäº§å“æ–‡ä»¶å¤¹: {product_folder}")
            
            return product_folder
            
        except Exception as e:
            self.log(f"âŒ åˆ›å»ºäº§å“æ–‡ä»¶å¤¹ç»“æ„å¤±è´¥: {str(e)}")
            return base_folder
    
    def process_product_page(self, product_url, product_title, folder_path):
        """å¤„ç†äº§å“è¯¦æƒ…é¡µé¢ï¼Œä¸‹è½½ç›¸å…³èµ„æ–™"""
        try:
            self.log(f"ğŸ” å¤„ç†äº§å“é¡µé¢: {product_title}")
            
            if product_url in self.processed_urls:
                self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†äº§å“: {product_title}")
                return
            
            # è®°å½•å½“å‰URLï¼Œç”¨äºå‹å·æå–
            self.current_url = product_url
            
            soup = self.visit_page(product_url)
            if not soup:
                return
            
            # æå–äº§å“å‹å·
            product_model = self.extract_product_model(product_title)
            self.log(f"ğŸ·ï¸ äº§å“å‹å·: {product_model}")
            
            # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
            downloads = self.find_download_links(soup, product_url)
            
            if downloads:
                self.log(f"ğŸš€ å¼€å§‹å¤„ç† {len(downloads)} ä¸ªæ–‡ä»¶åˆ°äº§å“æ–‡ä»¶å¤¹: {product_model}")
                
                for download in downloads:
                    try:
                        title = download['title']
                        url = download['url']
                        filename = download.get('filename', '')
                        
                        # ç”Ÿæˆæ–‡ä»¶å
                        clean_filename = self.generate_clean_filename(title, filename, url)
                        
                        # åˆ†ç±»æ–‡ä»¶
                        file_type_category = self.categorize_file_type(title, filename, url)
                        
                        # åˆ›å»ºäº§å“æ–‡ä»¶å¤¹ç»“æ„
                        product_folder = self.create_product_folder_structure(product_model, folder_path)
                        
                        # ä¸‹è½½æ–‡ä»¶åˆ°å¯¹åº”çš„åˆ†ç±»æ–‡ä»¶å¤¹
                        self.download_file(url, clean_filename, product_folder, file_type_category)
                        
                        time.sleep(1)  # ä¸‹è½½é—´éš”
                        
                    except Exception as e:
                        self.log(f"âŒ å¤„ç†ä¸‹è½½é¡¹æ—¶å‡ºé”™: {str(e)}")
                        continue
            else:
                self.log(f"âš ï¸ äº§å“é¡µé¢ä¸­æœªæ‰¾åˆ°ä¸‹è½½æ–‡ä»¶: {product_title}")
            
            # æ ‡è®°ä¸ºå·²å¤„ç†
            self.processed_urls.add(product_url)
            
        except Exception as e:
            self.log(f"âŒ å¤„ç†äº§å“é¡µé¢æ—¶å‡ºé”™: {str(e)}")
    
    def process_category_page(self, module_name, category):
        """å¤„ç†åˆ†ç±»é¡µé¢"""
        category_name = category['name']
        category_url = category['url']
        
        if category_url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†åˆ†ç±»: {category_name}")
            return
        
        self.log(f"ğŸ“‹ å¤„ç†åˆ†ç±»: {module_name} -> {category_name}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å­åˆ†ç±»
        if 'subcategories' in category:
            self.log(f"ğŸ” å‘ç°å­åˆ†ç±»ï¼Œå¼€å§‹å¤„ç†å­åˆ†ç±»")
            # åˆ›å»ºçˆ¶åˆ†ç±»æ–‡ä»¶å¤¹ï¼Œæ‰€æœ‰å­åˆ†ç±»çš„å†…å®¹éƒ½æ”¾åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹
            safe_category_name = category_name.replace('/', '_').replace('\\', '_')
            folder_path = os.path.join(self.base_dir, module_name, safe_category_name)
            
            for subcategory in category['subcategories']:
                self.process_subcategory_page(module_name, category_name, subcategory, folder_path)
                time.sleep(2)  # å­åˆ†ç±»é—´å»¶è¿Ÿ
        else:
            # å¤„ç†æ™®é€šåˆ†ç±»é¡µé¢
            soup = self.visit_page(category_url)
            if not soup:
                return
            
            # æŸ¥æ‰¾äº§å“é“¾æ¥
            products = self.find_product_links(soup, category_url)
            
            if products:
                # åˆ›å»ºæ¨¡å—ç›®å½•ï¼ŒæŒ‰ç…§é¡¶çº§æ¨¡å—åç§°åˆ†ç±»
                safe_category_name = category_name.replace('/', '_').replace('\\', '_')
                folder_path = os.path.join(self.base_dir, module_name, safe_category_name)
                
                self.log(f"ğŸš€ å¼€å§‹å¤„ç† {len(products)} ä¸ªäº§å“åˆ°: {folder_path}")
                
                for product in products:
                    try:
                        product_title = product['title']
                        product_url = product['url']
                        
                        # å¤„ç†äº§å“è¯¦æƒ…é¡µé¢
                        self.process_product_page(product_url, product_title, folder_path)
                        
                        time.sleep(2)  # äº§å“é—´å»¶è¿Ÿ
                        
                    except Exception as e:
                        self.log(f"âŒ å¤„ç†äº§å“æ—¶å‡ºé”™: {str(e)}")
                        continue
            else:
                self.log(f"âš ï¸ åˆ†ç±»é¡µé¢ä¸­æœªæ‰¾åˆ°äº§å“: {category_name}")
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        self.processed_urls.add(category_url)
    
    def process_subcategory_page(self, module_name, parent_category_name, subcategory, folder_path):
        """å¤„ç†å­åˆ†ç±»é¡µé¢"""
        subcategory_name = subcategory['name']
        subcategory_url = subcategory['url']
        
        if subcategory_url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†å­åˆ†ç±»: {subcategory_name}")
            return
        
        self.log(f"ğŸ” å¤„ç†å­åˆ†ç±»: {module_name} -> {parent_category_name} -> {subcategory_name}")
        
        soup = self.visit_page(subcategory_url)
        if not soup:
            return
        
        # æŸ¥æ‰¾äº§å“é“¾æ¥
        products = self.find_product_links(soup, subcategory_url)
        
        if products:
            self.log(f"ğŸš€ å¼€å§‹å¤„ç† {len(products)} ä¸ªäº§å“åˆ°: {folder_path}")
            
            for product in products:
                try:
                    product_title = product['title']
                    product_url = product['url']
                    
                    # å¤„ç†äº§å“è¯¦æƒ…é¡µé¢ï¼Œæ‰€æœ‰å­åˆ†ç±»çš„äº§å“éƒ½æ”¾åœ¨åŒä¸€ä¸ªçˆ¶åˆ†ç±»æ–‡ä»¶å¤¹ä¸‹
                    self.process_product_page(product_url, product_title, folder_path)
                    
                    time.sleep(2)  # äº§å“é—´å»¶è¿Ÿ
                    
                except Exception as e:
                    self.log(f"âŒ å¤„ç†äº§å“æ—¶å‡ºé”™: {str(e)}")
                    continue
        else:
            self.log(f"âš ï¸ å­åˆ†ç±»é¡µé¢ä¸­æœªæ‰¾åˆ°äº§å“: {subcategory_name}")
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        self.processed_urls.add(subcategory_url)
    
    def process_main_page(self, module):
        """å¤„ç†ä¸»é¡µé¢ï¼ˆå¯èƒ½åŒ…å«ä¸€äº›é€šç”¨ä¸‹è½½ï¼‰"""
        module_name = module['name']
        module_url = module['url']
        
        if module_url in self.processed_urls:
            self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†ä¸»é¡µ: {module_name}")
            return
        
        self.log(f"ğŸŒ å¤„ç†ä¸»é¡µ: {module_name}")
        
        soup = self.visit_page(module_url)
        if not soup:
            return
        
        # æŸ¥æ‰¾ä¸»é¡µé¢çš„ä¸‹è½½é“¾æ¥
        downloads = self.find_download_links(soup, module_url)
        
        if downloads:
            # åˆ›å»ºä¸»æ¨¡å—ç›®å½•ï¼Œä¿å­˜é€šç”¨æ–‡ä»¶åˆ°å¯¹åº”çš„äº§å“åˆ†ç±»
            folder_path = os.path.join(self.base_dir, module_name, "é€šç”¨èµ„æ–™")
            
            self.log(f"ğŸš€ å¤„ç†ä¸»é¡µ {len(downloads)} ä¸ªæ–‡ä»¶åˆ°: {folder_path}")
            
            for download in downloads:
                try:
                    title = download['title']
                    url = download['url']
                    filename = download.get('filename', '')
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    clean_filename = self.generate_clean_filename(title, filename, url)
                    
                    # ä¸‹è½½æ–‡ä»¶
                    self.download_file(url, clean_filename, folder_path)
                    
                    time.sleep(1)
                    
                except Exception as e:
                    self.log(f"âŒ å¤„ç†ä¸‹è½½é¡¹æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        self.processed_urls.add(module_url)
    
    def send_dingtalk_notification(self, message):
        """å‘é€é’‰é’‰é€šçŸ¥"""
        try:
            # è¿™é‡Œéœ€è¦é…ç½®ä½ çš„é’‰é’‰æœºå™¨äººwebhookåœ°å€
            webhook_url = os.environ.get('DINGTALK_WEBHOOK', '')
            
            if not webhook_url:
                self.log("âš ï¸ æœªé…ç½®é’‰é’‰webhookï¼Œè·³è¿‡é€šçŸ¥")
                return
            
            data = {
                "msgtype": "text",
                "text": {
                    "content": message
                }
            }
            
            response = requests.post(webhook_url, json=data, timeout=10)
            
            if response.status_code == 200:
                self.log("âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            self.log(f"âŒ é’‰é’‰é€šçŸ¥å‘é€å‡ºé”™: {str(e)}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        start_time = datetime.now()
        
        try:
            self.log("ğŸš€ å¼€å§‹è¿è¡Œå·å´æœºå™¨äººæ–‡æ¡£çˆ¬è™«")
            
            # çˆ¬å–æ¯ä¸ªä¸»è¦æ¨¡å—
            for module in self.main_modules:
                module_name = module['name']
                self.log(f"ğŸ“‚ å¼€å§‹å¤„ç†æ¨¡å—: {module_name}")
                
                # å¤„ç†ä¸»é¡µé¢
                self.process_main_page(module)
                time.sleep(2)
                
                # å¤„ç†å„ä¸ªåˆ†ç±»
                for category in module['categories']:
                    self.process_category_page(module_name, category)
                    time.sleep(3)  # åˆ†ç±»é—´å»¶è¿Ÿ
                
                self.log(f"âœ… å®Œæˆæ¨¡å—: {module_name}")
                time.sleep(5)  # æ¨¡å—é—´å»¶è¿Ÿ
            
            # ä¿å­˜è¿›åº¦
            self.save_processed_urls()
            
            # ç»Ÿè®¡ç»“æœ
            end_time = datetime.now()
            duration = end_time - start_time
            total_files = len(self.new_files)
            
            self.log(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±ä¸‹è½½ {total_files} ä¸ªæ–°æ–‡ä»¶ï¼Œè€—æ—¶ {duration}")
            
            # å‘é€é’‰é’‰é€šçŸ¥
            if self.new_files:
                notification_message = f"""å·å´æœºå™¨äººçˆ¬è™«å®Œæˆï¼
ğŸ“Š çˆ¬å–ç»Ÿè®¡ï¼š
â€¢ æ–°æ–‡ä»¶æ•°é‡ï¼š{total_files}
â€¢ çˆ¬å–è€—æ—¶ï¼š{duration}
â€¢ å®Œæˆæ—¶é—´ï¼š{end_time.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“ æ–°ä¸‹è½½æ–‡ä»¶ï¼ˆå‰10ä¸ªï¼‰ï¼š"""
                
                for i, file_info in enumerate(self.new_files[:10]):
                    notification_message += f"\n{i+1}. {file_info['filename']} ({file_info['size']} bytes)"
                
                if len(self.new_files) > 10:
                    notification_message += f"\n... è¿˜æœ‰ {len(self.new_files) - 10} ä¸ªæ–‡ä»¶"
                
                self.send_dingtalk_notification(notification_message)
                
                self.log("ğŸ“ æ–°ä¸‹è½½çš„æ–‡ä»¶:")
                for file_info in self.new_files[:10]:
                    self.log(f"   ğŸ“„ {file_info['filename']} ({file_info['size']} bytes)")
                
                if len(self.new_files) > 10:
                    self.log(f"   ... è¿˜æœ‰ {len(self.new_files) - 10} ä¸ªæ–‡ä»¶")
            else:
                notification_message = f"""å·å´æœºå™¨äººçˆ¬è™«å®Œæˆï¼
ğŸ“Š æœ¬æ¬¡æœªå‘ç°æ–°æ–‡ä»¶
â€¢ çˆ¬å–è€—æ—¶ï¼š{duration}
â€¢ å®Œæˆæ—¶é—´ï¼š{end_time.strftime('%Y-%m-%d %H:%M:%S')}"""
                
                self.send_dingtalk_notification(notification_message)
                self.log("â„¹ï¸ æœ¬æ¬¡çˆ¬å–æœªå‘ç°æ–°æ–‡ä»¶")
            
        except Exception as e:
            error_message = f"å·å´æœºå™¨äººçˆ¬è™«è¿è¡Œå‡ºé”™ï¼š{str(e)}"
            self.log(f"âŒ {error_message}")
            self.send_dingtalk_notification(error_message)
            
        finally:
            # å…³é—­WebDriver
            if self.driver:
                self.driver.quit()
                self.log("ğŸ”’ WebDriverå·²å…³é—­")

    def validate_and_fix_folder_names(self):
        """éªŒè¯å’Œä¿®å¤ç°æœ‰æ–‡ä»¶å¤¹çš„å‘½åé—®é¢˜"""
        try:
            self.log("ğŸ” å¼€å§‹éªŒè¯å’Œä¿®å¤æ–‡ä»¶å¤¹å‘½å...")
            
            # éå†å·å´ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹
            for root, dirs, files in os.walk(self.base_dir):
                for dir_name in dirs:
                    dir_path = os.path.join(root, dir_name)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“å‹å·æ–‡ä»¶å¤¹ï¼ˆåŒ…å«RSã€BAã€MSç­‰å‰ç¼€ï¼‰
                    if re.match(r'^[A-Z]{1,3}\d{2,4}[NLHX]?$', dir_name, re.IGNORECASE):
                        # è¿™æ˜¯äº§å“å‹å·æ–‡ä»¶å¤¹ï¼ŒéªŒè¯å‘½å
                        corrected_name = self.standardize_model_name(dir_name)
                        
                        if corrected_name != dir_name:
                            # éœ€è¦é‡å‘½å
                            corrected_path = os.path.join(root, corrected_name)
                            
                            # æ£€æŸ¥ç›®æ ‡è·¯å¾„æ˜¯å¦å·²å­˜åœ¨
                            if os.path.exists(corrected_path):
                                self.log(f"âš ï¸ ç›®æ ‡è·¯å¾„å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å‘½å: {dir_name} -> {corrected_name}")
                                continue
                            
                            try:
                                os.rename(dir_path, corrected_path)
                                self.log(f"âœ… é‡å‘½åæ–‡ä»¶å¤¹: {dir_name} -> {corrected_name}")
                            except Exception as e:
                                self.log(f"âŒ é‡å‘½åå¤±è´¥ {dir_name}: {str(e)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ§åˆ¶æŸœå‹å·æ–‡ä»¶å¤¹
                    elif re.match(r'^[EF]\d{3}[NLHX]?$', dir_name, re.IGNORECASE):
                        corrected_name = self.standardize_model_name(dir_name)
                        
                        if corrected_name != dir_name:
                            corrected_path = os.path.join(root, corrected_name)
                            
                            if os.path.exists(corrected_path):
                                self.log(f"âš ï¸ ç›®æ ‡è·¯å¾„å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å‘½å: {dir_name} -> {corrected_name}")
                                continue
                            
                            try:
                                os.rename(dir_path, corrected_path)
                                self.log(f"âœ… é‡å‘½åæ§åˆ¶æŸœæ–‡ä»¶å¤¹: {dir_name} -> {corrected_name}")
                            except Exception as e:
                                self.log(f"âŒ é‡å‘½åå¤±è´¥ {dir_name}: {str(e)}")
            
            self.log("âœ… æ–‡ä»¶å¤¹å‘½åéªŒè¯å’Œä¿®å¤å®Œæˆ")
            
        except Exception as e:
            self.log(f"âŒ æ–‡ä»¶å¤¹å‘½åéªŒè¯å¤±è´¥: {str(e)}")
    
    def get_folder_statistics(self):
        """è·å–æ–‡ä»¶å¤¹ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'total_products': 0,
                'rs_series': 0,
                'ba_series': 0,
                'ms_series': 0,
                'vs_series': 0,
                'kj_series': 0,
                'f_controllers': 0,
                'e_controllers': 0,
                'other': 0,
                'naming_issues': []
            }
            
            # éå†å·å´ç›®å½•
            for root, dirs, files in os.walk(self.base_dir):
                for dir_name in dirs:
                    dir_path = os.path.join(root, dir_name)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“å‹å·æ–‡ä»¶å¤¹
                    if re.match(r'^[A-Z]{1,3}\d{2,4}[NLHX]?$', dir_name, re.IGNORECASE):
                        stats['total_products'] += 1
                        
                        # åˆ†ç±»ç»Ÿè®¡
                        if dir_name.upper().startswith('RS'):
                            stats['rs_series'] += 1
                        elif dir_name.upper().startswith('BA'):
                            stats['ba_series'] += 1
                        elif dir_name.upper().startswith('MS'):
                            stats['ms_series'] += 1
                        elif dir_name.upper().startswith('VS'):
                            stats['vs_series'] += 1
                        elif dir_name.upper().startswith('KJ'):
                            stats['kj_series'] += 1
                        elif dir_name.upper().startswith('F'):
                            stats['f_controllers'] += 1
                        elif dir_name.upper().startswith('E'):
                            stats['e_controllers'] += 1
                        else:
                            stats['other'] += 1
                        
                        # æ£€æŸ¥å‘½åé—®é¢˜
                        if not re.match(r'^[A-Z]{1,3}\d{2,4}[NLHX]?$', dir_name):
                            stats['naming_issues'].append(dir_name)
            
            return stats
            
        except Exception as e:
            self.log(f"âŒ è·å–æ–‡ä»¶å¤¹ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return None

def test_single_category(category_url=None):
    """æµ‹è¯•å•ä¸ªåˆ†ç±»çš„çˆ¬å–åŠŸèƒ½"""
    spider = KawasakiSpider()
    
    try:
        # é»˜è®¤æµ‹è¯•URL
        test_url = category_url or "https://kawasakirobotics.cn/robots-category/small-medium-payloads/"
        
        spider.log(f"ğŸ§ª æµ‹è¯•å•åˆ†ç±»çˆ¬å–åŠŸèƒ½")
        spider.log(f"ğŸ“‹ æµ‹è¯•URL: {test_url}")
        
        # åˆ›å»ºæµ‹è¯•åˆ†ç±»é…ç½®
        test_category = {
            'name': 'æµ‹è¯•åˆ†ç±»',
            'url': test_url
        }
        
        # å¤„ç†æµ‹è¯•åˆ†ç±»
        spider.process_category_page("æµ‹è¯•æ¨¡å—", test_category)
        
        if spider.new_files:
            spider.log(f"âœ… æµ‹è¯•æˆåŠŸï¼æ‰¾åˆ° {len(spider.new_files)} ä¸ªæ–‡ä»¶")
            for file_info in spider.new_files:
                spider.log(f"   ğŸ“„ {file_info['filename']}")
        else:
            spider.log(f"âš ï¸ æµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ–°æ–‡ä»¶")
        
    except Exception as e:
        spider.log(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    finally:
        if spider.driver:
            spider.driver.quit()

def test_single_product(product_url=None):
    """æµ‹è¯•å•ä¸ªäº§å“çš„çˆ¬å–åŠŸèƒ½"""
    spider = KawasakiSpider()
    
    try:
        # é»˜è®¤æµ‹è¯•URL
        test_url = product_url or "https://kawasakirobotics.cn/products-robots/rs080n/"
        
        spider.log(f"ğŸ§ª æµ‹è¯•å•äº§å“çˆ¬å–åŠŸèƒ½")
        spider.log(f"ğŸ” æµ‹è¯•URL: {test_url}")
        
        # æµ‹è¯•äº§å“é¡µé¢å¤„ç†
        spider.process_product_page(test_url, "RS080Næµ‹è¯•", os.path.join(spider.base_dir, "æµ‹è¯•"))
        
        if spider.new_files:
            spider.log(f"âœ… æµ‹è¯•æˆåŠŸï¼æ‰¾åˆ° {len(spider.new_files)} ä¸ªæ–‡ä»¶")
            for file_info in spider.new_files:
                spider.log(f"   ğŸ“„ {file_info['filename']}")
        else:
            spider.log(f"âš ï¸ æµ‹è¯•å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ–°æ–‡ä»¶")
        
    except Exception as e:
        spider.log(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    finally:
        if spider.driver:
            spider.driver.quit()

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            # æµ‹è¯•æ¨¡å¼
            test_url = sys.argv[2] if len(sys.argv) > 2 else None
            test_single_category(test_url)
        elif sys.argv[1] == "test_product":
            # äº§å“æµ‹è¯•æ¨¡å¼
            test_url = sys.argv[2] if len(sys.argv) > 2 else None
            test_single_product(test_url)
        elif sys.argv[1] == "fix_folders":
            # ä¿®å¤æ–‡ä»¶å¤¹å‘½åæ¨¡å¼
            spider = KawasakiSpider()
            spider.validate_and_fix_folder_names()
        elif sys.argv[1] == "stats":
            # æ˜¾ç¤ºæ–‡ä»¶å¤¹ç»Ÿè®¡ä¿¡æ¯
            spider = KawasakiSpider()
            stats = spider.get_folder_statistics()
            if stats:
                print("\nğŸ“Š å·å´çˆ¬è™«æ–‡ä»¶å¤¹ç»Ÿè®¡ä¿¡æ¯:")
                print(f"æ€»äº§å“æ•°é‡: {stats['total_products']}")
                print(f"RSç³»åˆ—: {stats['rs_series']}")
                print(f"BAç³»åˆ—: {stats['ba_series']}")
                print(f"MSç³»åˆ—: {stats['ms_series']}")
                print(f"VSç³»åˆ—: {stats['vs_series']}")
                print(f"KJç³»åˆ—: {stats['kj_series']}")
                print(f"Fæ§åˆ¶æŸœ: {stats['f_controllers']}")
                print(f"Eæ§åˆ¶æŸœ: {stats['e_controllers']}")
                print(f"å…¶ä»–: {stats['other']}")
                
                if stats['naming_issues']:
                    print(f"\nâš ï¸ å‘½åé—®é¢˜æ–‡ä»¶å¤¹:")
                    for issue in stats['naming_issues']:
                        print(f"  - {issue}")
                else:
                    print("\nâœ… æ‰€æœ‰æ–‡ä»¶å¤¹å‘½åæ­£ç¡®")
        else:
            print("ç”¨æ³•:")
            print("  python å·å´çˆ¬è™«.py                    # æ­£å¸¸è¿è¡Œçˆ¬è™«")
            print("  python å·å´çˆ¬è™«.py test [url]        # æµ‹è¯•å•ä¸ªåˆ†ç±»")
            print("  python å·å´çˆ¬è™«.py test_product [url] # æµ‹è¯•å•ä¸ªäº§å“")
            print("  python å·å´çˆ¬è™«.py fix_folders        # ä¿®å¤æ–‡ä»¶å¤¹å‘½å")
            print("  python å·å´çˆ¬è™«.py stats              # æ˜¾ç¤ºæ–‡ä»¶å¤¹ç»Ÿè®¡")
    else:
        # æ­£å¸¸è¿è¡Œ
        spider = KawasakiSpider()
        spider.run()
