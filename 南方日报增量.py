#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import os
import json
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import pandas as pd
from bs4 import BeautifulSoup
import random
import schedule  # ç”¨äºå®šæ—¶ä»»åŠ¡
import requests
import re

class NanfangDailySpider:
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Connection': 'keep-alive'
        }
        
        # åŸºç¡€URLå’Œç›®å½•
        self.base_url = "https://epaper.nfnews.com/nfdaily/html"
        self.base_dir = "å—æ–¹æ—¥æŠ¥"
        
        # çˆ¬è™«é…ç½®
        self.max_workers = 5
        self.request_delay = 1
        
        # è®°å½•æ–‡ä»¶
        self.record_file = "crawled_articles.json"
        self.crawled_records = self.load_crawled_records()
        
        # åˆ›å»ºåŸºç¡€ç›®å½•
        os.makedirs(self.base_dir, exist_ok=True)

    def clean_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        return re.sub(r'[\\/:*?"<>|]', '_', filename)

    def create_article_dir(self, date_str, version_code, version_name, title):
        """åˆ›å»ºæ–‡ç« ç›®å½•ç»“æ„"""
        # æ—¥æœŸç›®å½•
        date_dir = os.path.join(self.base_dir, date_str)
        
        # ç‰ˆé¢ç›®å½•
        version_dir = os.path.join(date_dir, f"{version_code}_{version_name}")
        
        # æ–‡ç« ç›®å½•
        article_dir = os.path.join(version_dir, self.clean_filename(title))
        
        # åˆ›å»ºç›®å½•
        os.makedirs(article_dir, exist_ok=True)
        
        return article_dir

    def process_version(self, version, date_str):
        """å¤„ç†å•ä¸ªç‰ˆé¢"""
        try:
            print(f"\nâ””â”€â”€ ç‰ˆé¢: {version['code']}_{version['name']}")
            
            formatted_date = f"{date_str[:6]}/{date_str[6:]}"
            
            resp = self.safe_request(version['url'])
            if not resp:
                return
            
            resp.encoding = 'utf-8'
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è·å–ç‰ˆé¢ä¸Šçš„æ‰€æœ‰æ–‡ç« é“¾æ¥
            articles = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if 'content_' in href and href.endswith('.html'):
                    if href.startswith('http'):
                        article_url = href
                    else:
                        article_url = f"https://epaper.nfnews.com/nfdaily/html/{formatted_date}/{href}"
                    articles.append(article_url)
            
            # å¤„ç†æ¯ç¯‡æ–‡ç« 
            for article_url in articles:
                try:
                    self.process_article(article_url, date_str, version['code'], version['name'])
                    time.sleep(self.request_delay)
                except Exception as e:
                    print(f"    â”œâ”€â”€ âŒ å¤„ç†æ–‡ç« å¤±è´¥: {article_url}")
                    print(f"    â””â”€â”€ é”™è¯¯: {str(e)}")
            
        except Exception as e:
            print(f"â””â”€â”€ âŒ å¤„ç†ç‰ˆé¢å¤±è´¥: {version['url']}")
            print(f"    â””â”€â”€ é”™è¯¯: {str(e)}")

    def crawl_today(self):
        """çˆ¬å–ä»Šå¤©çš„æ–°é—»"""
        today = datetime.now().strftime('%Y-%m-%d')
        print(f"\nå¼€å§‹çˆ¬å– {today} çš„æ–°é—»...")
        self.run(today, today)

    def load_crawled_records(self):
        """åŠ è½½å·²çˆ¬å–çš„æ–‡ç« è®°å½•"""
        try:
            if os.path.exists(self.record_file):
                with open(self.record_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            print(f"åŠ è½½çˆ¬å–è®°å½•å¤±è´¥: {str(e)}")
            return {}

    def save_crawled_records(self):
        """ä¿å­˜å·²çˆ¬å–çš„æ–‡ç« è®°å½•"""
        try:
            with open(self.record_file, 'w', encoding='utf-8') as f:
                json.dump(self.crawled_records, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜çˆ¬å–è®°å½•å¤±è´¥: {str(e)}")

    def is_article_crawled(self, date_str, article_url, title):
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²ç»çˆ¬å–è¿‡"""
        article_key = f"{date_str}_{article_url}_{title}"
        return article_key in self.crawled_records

    def mark_article_crawled(self, date_str, article_url, title, file_path):
        """æ ‡è®°æ–‡ç« ä¸ºå·²çˆ¬å–"""
        article_key = f"{date_str}_{article_url}_{title}"
        self.crawled_records[article_key] = {
            'date': date_str,
            'url': article_url,
            'title': title,
            'file_path': file_path,
            'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    def process_article(self, article_url, date_str, version_code, version_name):
        """å¤„ç†å•ç¯‡æ–‡ç« ï¼ˆå¢é‡ç‰ˆæœ¬ï¼‰"""
        try:
            formatted_date = f"{date_str[:6]}/{date_str[6:]}"
            
            resp = self.safe_request(article_url)
            if not resp:
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è·å–æ–‡ç« æ ‡é¢˜
            title_elem = soup.find('div', class_='title') or soup.find('h1')
            if not title_elem:
                print(f"    â””â”€â”€ âŒ æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜: {article_url}")
                return None
            
            title = title_elem.text.strip()
            print(f"    â”œâ”€â”€ æ–‡ç« : {title}")
            
            # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²çˆ¬å–
            if self.is_article_crawled(date_str, article_url, title):
                print(f"    â”‚   â””â”€â”€ â­ï¸  æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡")
                return self.crawled_records[f"{date_str}_{article_url}_{title}"]['file_path']

            # åˆ›å»ºæ–‡ç« ç›®å½•
            article_dir = self.create_article_dir(date_str, version_code, version_name, title)
            
            # å¤„ç†æ–‡ç« å†…å®¹
            content = soup.find('div', class_='article-content') or soup.find('div', id='content')
            if not content:
                print(f"    â”‚   â””â”€â”€ âŒ æœªæ‰¾åˆ°æ–‡ç« å†…å®¹")
                return None
            
            # æ¸…ç†å†…å®¹
            for tag in content.find_all(['script', 'style']):
                tag.decompose()
            
            text = content.get_text(separator='\n', strip=True)
            
            # ä¿å­˜æ–‡ç« æ–‡æœ¬
            clean_title = self.clean_filename(title)
            text_file = os.path.join(article_dir, f"{clean_title}.txt")
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(f"æ ‡é¢˜ï¼š{title}\n")
                f.write(f"æ—¥æœŸï¼š{date_str}\n")
                f.write(f"ç‰ˆé¢ï¼š{version_code}_{version_name}\n")
                f.write(f"é“¾æ¥ï¼š{article_url}\n\n")
                f.write("æ­£æ–‡ï¼š\n")
                f.write(text)
            
            print(f"    â”‚   â”œâ”€â”€ ğŸ“„ å·²ä¿å­˜æ–‡æœ¬")
            
            # ä¸‹è½½å›¾ç‰‡
            images = content.find_all('img')
            for i, img in enumerate(images, 1):
                img_url = img.get('src')
                if img_url:
                    if img_url.startswith('..'):
                        img_url = f"https://epaper.nfnews.com/nfdaily/res/{formatted_date}/{img_url.split('/')[-1]}"
                    elif not img_url.startswith('http'):
                        img_url = f"https://epaper.nfnews.com{img_url}"
                    
                    img_path = os.path.join(article_dir, f"{clean_title}_{i:02d}.jpg")
                    if self.download_image(img_url, img_path):
                        print(f"    â”‚   â”œâ”€â”€ ğŸ–¼ï¸  å·²ä¿å­˜å›¾ç‰‡_{i:02d}")
            
            # æ ‡è®°æ–‡ç« ä¸ºå·²çˆ¬å–
            self.mark_article_crawled(date_str, article_url, title, text_file)
            print(f"    â”‚   â””â”€â”€ âœ… å®Œæˆ")
            return text_file
        
        except Exception as e:
            print(f"    â”‚   â””â”€â”€ âŒ å¤„ç†å¤±è´¥: {str(e)}")
            return None

    def run(self, start_date, end_date):
        """è¿è¡Œçˆ¬è™«ï¼ˆå¢é‡ç‰ˆæœ¬ï¼‰"""
        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            for date in date_range:
                date_str = date.strftime('%Y%m%d')
                print(f"\nğŸ“… {date_str}")
                
                versions = self.get_version_list(date_str)
                if not versions:
                    print(f"â””â”€â”€ âŒ æœªè·å–åˆ°ç‰ˆé¢åˆ—è¡¨")
                    continue

                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = []
                    for version in versions:
                        futures.append(
                            executor.submit(self.process_version, version, date_str)
                        )
                        time.sleep(self.request_delay)

                    for future in as_completed(futures):
                        try:
                            future.result()
                        except Exception as e:
                            print(f"â””â”€â”€ âŒ ç‰ˆé¢å¤„ç†ä»»åŠ¡å¤±è´¥: {str(e)}")

                # æ¯å¤„ç†å®Œä¸€å¤©çš„æ•°æ®å°±ä¿å­˜ä¸€æ¬¡è®°å½•
                self.save_crawled_records()
                print(f"\nâ””â”€â”€ âœ… å®Œæˆ {date_str} çš„æ•°æ®å¤„ç†")
                time.sleep(5)

        except Exception as e:
            print(f"\nâŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {str(e)}")
        finally:
            # æœ€åå†ä¿å­˜ä¸€æ¬¡è®°å½•
            self.save_crawled_records()
            print("\nå·²å®Œæˆçˆ¬å–ä»»åŠ¡ï¼")

    def safe_request(self, url, retry_times=3):
        """å®‰å…¨çš„è¯·æ±‚æ–¹æ³•"""
        for i in range(retry_times):
            try:
                # æ›´æ–°è¯·æ±‚å¤´
                headers = {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Cache-Control': 'max-age=0',
                    'Connection': 'keep-alive',
                    'Host': 'epaper.nfnews.com',
                    'Referer': 'https://epaper.nfnews.com/',
                    'Sec-Ch-Ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
                    'Sec-Ch-Ua-Mobile': '?0',
                    'Sec-Ch-Ua-Platform': '"macOS"',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'same-origin',
                    'Sec-Fetch-User': '?1',
                    'Upgrade-Insecure-Requests': '1',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
                }
                
                # éšæœºå»¶è¿Ÿ
                time.sleep(random.uniform(2, 5))
                
                response = requests.get(url, headers=headers, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response
                
            except Exception as e:
                print(f"    â”œâ”€â”€ âš ï¸ è¯·æ±‚å¤±è´¥ {url}")
                print(f"    â”œâ”€â”€ âš ï¸ é‡è¯• {i+1}/{retry_times}: {str(e)}")
                if i == retry_times - 1:
                    return None
                time.sleep(random.uniform(5, 10))  # å¤±è´¥åç­‰å¾…æ›´é•¿æ—¶é—´

    def download_image(self, img_url, save_path):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            response = self.safe_request(img_url)
            if response and response.content:
                with open(save_path, 'wb') as f:
                    f.write(response.content)
                return True
            return False
        except Exception as e:
            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {str(e)}")
            return False

    def get_version_list(self, date_str):
        """è·å–ç‰ˆé¢åˆ—è¡¨"""
        try:
            formatted_date = f"{date_str[:6]}/{date_str[6:]}"
            versions = []
            visited_urls = set()  # è®°å½•å·²è®¿é—®çš„URLï¼Œé¿å…å¾ªç¯
            print(f"â”œâ”€â”€ ğŸ” å¼€å§‹è·å–ç‰ˆé¢åˆ—è¡¨...")

            # ä»A01ç‰ˆå¼€å§‹
            current_url = f"{self.base_url}/{formatted_date}/node_A01.html"
            
            while current_url and current_url not in visited_urls:
                visited_urls.add(current_url)
                try:
                    resp = self.safe_request(current_url)
                    if not resp:
                        break
                    
                    soup = BeautifulSoup(resp.text, 'html.parser')
                    
                    # è·å–å½“å‰ç‰ˆé¢ä¿¡æ¯
                    version_code = current_url.split('node_')[1].split('.')[0]
                    
                    # å°è¯•å¤šç§æ–¹å¼è·å–ç‰ˆé¢åç§°
                    version_name = None
                    # 1. ä»æ ‡é¢˜è·å–
                    title_elem = soup.find('title')
                    if title_elem:
                        title_text = title_elem.text.strip()
                        if 'å—æ–¹æ—¥æŠ¥' in title_text:
                            version_name = title_text.split('å—æ–¹æ—¥æŠ¥')[1].strip()
                    
                    # 2. ä»ç‰ˆé¢å¯¼èˆªè·å–
                    if not version_name:
                        nav_elem = soup.find('div', class_='position') or soup.find('div', class_='nav')
                        if nav_elem:
                            version_name = nav_elem.text.strip()
                    
                    # 3. å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤åç§°
                    if not version_name:
                        version_name = f"ç¬¬{version_code}ç‰ˆ"
                    
                    # æ·»åŠ åˆ°ç‰ˆé¢åˆ—è¡¨
                    if not any(v['code'] == version_code for v in versions):
                        versions.append({
                            'code': version_code,
                            'name': version_name,
                            'url': current_url
                        })
                        print(f"â”‚   â”œâ”€â”€ âœ“ æ‰¾åˆ°ç‰ˆé¢: {version_code}_{version_name}")
                    
                    # æŸ¥æ‰¾ä¸‹ä¸€ç‰ˆé“¾æ¥
                    next_link = None
                    
                    # 1. å°è¯•æ‰¾"ä¸‹ä¸€ç‰ˆ"æŒ‰é’®
                    for link in soup.find_all('a', href=True):
                        if 'ä¸‹ä¸€ç‰ˆ' in link.text or 'ä¸‹ä¸€é¡µ' in link.text:
                            next_link = link.get('href')
                            break
                    
                    # 2. å°è¯•ä»ç‰ˆé¢å¯¼èˆªè·å–
                    if not next_link:
                        nav_area = soup.find('div', class_='nav-area') or soup.find('div', class_='list-box')
                        if nav_area:
                            current_found = False
                            for link in nav_area.find_all('a', href=True):
                                href = link.get('href', '')
                                if current_url.endswith(href):
                                    current_found = True
                                elif current_found and 'node_' in href:
                                    next_link = href
                                    break
                    
                    # æ›´æ–°ä¸‹ä¸€ä¸ªURL
                    if next_link:
                        if next_link.startswith('http'):
                            current_url = next_link
                        else:
                            current_url = f"{self.base_url}/{formatted_date}/{next_link}"
                    else:
                        break
                    
                except Exception as e:
                    print(f"â”‚   â”œâ”€â”€ âš ï¸ å¤„ç†ç‰ˆé¢å‡ºé”™: {str(e)}")
                    break
            
            # æŒ‰ç‰ˆé¢ç¼–å·æ’åº
            versions.sort(key=lambda x: (x['code'][0], int(x['code'][1:] if x['code'][1:].isdigit() else x['code'][2:])))
            
            if versions:
                print(f"\nâ”œâ”€â”€ ğŸ“‹ å…±æ‰¾åˆ° {len(versions)} ä¸ªç‰ˆé¢:")
                for v in versions:
                    print(f"â”‚   â”œâ”€â”€ {v['code']}_{v['name']}")
            else:
                print(f"â”œâ”€â”€ âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç‰ˆé¢ï¼Œè¯·æ£€æŸ¥ç½‘å€æ˜¯å¦æ­£ç¡®")
            
            return versions
            
        except Exception as e:
            print(f"â”œâ”€â”€ âŒ è·å–ç‰ˆé¢åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

class NanfangDailyIncrementalSpider(NanfangDailySpider):
    def __init__(self):
        super().__init__()  # æ­£ç¡®è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        self.record_file = "crawled_articles.json"
        self.crawled_records = self.load_crawled_records()

    def process_article(self, article_url, date_str, version_code, version_name):
        """å¤„ç†å•ç¯‡æ–‡ç« ï¼ˆå¢é‡ç‰ˆæœ¬ï¼‰"""
        try:
            formatted_date = f"{date_str[:6]}/{date_str[6:]}"
            
            resp = self.safe_request(article_url)
            if not resp:
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è·å–æ–‡ç« æ ‡é¢˜
            title_elem = soup.find('div', class_='title') or soup.find('h1')
            if not title_elem:
                print(f"æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜: {article_url}")
                return None
            
            title = title_elem.text.strip()
            print(f"å¤„ç†æ–‡ç« : {title}")
            
            # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²çˆ¬å–
            if self.is_article_crawled(date_str, article_url, title):
                print(f"æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡: {title}")
                return self.crawled_records[f"{date_str}_{article_url}_{title}"]['file_path']

            # åˆ›å»ºæ–‡ç« ç›®å½•
            article_dir = self.create_article_dir(date_str, version_code, version_name, title)
            
            # å¤„ç†æ–‡ç« å†…å®¹
            content = soup.find('div', class_='article-content') or soup.find('div', id='content')
            if not content:
                print(f"æœªæ‰¾åˆ°æ–‡ç« å†…å®¹: {article_url}")
                return None
            
            # æ¸…ç†å†…å®¹
            for tag in content.find_all(['script', 'style']):
                tag.decompose()
            
            text = content.get_text(separator='\n', strip=True)
            
            # ä¿å­˜æ–‡ç« æ–‡æœ¬
            clean_title = self.clean_filename(title)
            text_file = os.path.join(article_dir, f"{clean_title}.txt")
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(f"æ ‡é¢˜ï¼š{title}\n")
                f.write(f"æ—¥æœŸï¼š{date_str}\n")
                f.write(f"ç‰ˆé¢ï¼š{version_code}_{version_name}\n")
                f.write(f"é“¾æ¥ï¼š{article_url}\n\n")
                f.write("æ­£æ–‡ï¼š\n")
                f.write(text)
            
            # ä¸‹è½½å›¾ç‰‡
            images = content.find_all('img')
            for i, img in enumerate(images, 1):
                img_url = img.get('src')
                if img_url:
                    if img_url.startswith('..'):
                        img_url = f"https://epaper.nfnews.com/nfdaily/res/{formatted_date}/{img_url.split('/')[-1]}"
                    elif not img_url.startswith('http'):
                        img_url = f"https://epaper.nfnews.com{img_url}"
                    
                    img_path = os.path.join(article_dir, f"{clean_title}_{i:02d}.jpg")
                    if self.download_image(img_url, img_path):
                        print(f"å·²ä¿å­˜å›¾ç‰‡: {img_path}")
            
            # æ ‡è®°æ–‡ç« ä¸ºå·²çˆ¬å–
            self.mark_article_crawled(date_str, article_url, title, text_file)
            print(f"å·²ä¿å­˜æ–‡ç« : {title}")
            return text_file
        
        except Exception as e:
            print(f"å¤„ç†æ–‡ç« å¤±è´¥: {article_url}, é”™è¯¯: {str(e)}")
            return None

def run_spider():
    """è¿è¡Œçˆ¬è™«çš„å®šæ—¶ä»»åŠ¡"""
    spider = NanfangDailyIncrementalSpider()
    spider.crawl_today()
    print(f"å®Œæˆå®šæ—¶çˆ¬å–ä»»åŠ¡: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å¼
    TEST_MODE = True
    spider = NanfangDailyIncrementalSpider()
    
    if TEST_MODE:
        print("=== æµ‹è¯•æ¨¡å¼ ===")
        # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ—¥æœŸ
        test_date = '2025-07-3'  # ä½¿ç”¨ä¸€ä¸ªç¡®å®šå­˜åœ¨çš„æ—¥æœŸ
        print(f"æµ‹è¯•çˆ¬å–æ—¥æœŸ: {test_date}")
        spider.run(test_date, test_date)
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šè®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every().day.at("06:00").do(run_spider)  # æ—©ä¸Š6ç‚¹
        schedule.every().day.at("14:00").do(run_spider)  # ä¸‹åˆ2ç‚¹
        schedule.every().day.at("22:00").do(run_spider)  # æ™šä¸Š10ç‚¹
        
        print(f"å¢é‡çˆ¬è™«å·²å¯åŠ¨ï¼Œå°†åœ¨æ¯å¤© 06:00ã€14:00ã€22:00 è¿è¡Œ...")
        print(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # å…ˆæ‰§è¡Œä¸€æ¬¡ï¼Œçˆ¬å–å½“å¤©çš„å†…å®¹
        print("æ‰§è¡Œé¦–æ¬¡çˆ¬å–...")
        run_spider()
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡å¾ªç¯
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                print("\nçˆ¬è™«å·²åœæ­¢è¿è¡Œ")
                break
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(300)  # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…5åˆ†é’Ÿåç»§ç»­
