#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import os
import time
import json
import requests
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import re

class PeoplesDailySpider:
    def __init__(self):
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'http://paper.people.com.cn/',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Upgrade-Insecure-Requests': '1',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        }
        
        self.layout_url = "http://paper.people.com.cn/rmrb/pc/layout"  # ç‰ˆé¢å¸ƒå±€URL
        self.content_url = "http://paper.people.com.cn/rmrb/pc/content"  # æ–‡ç« å†…å®¹URL
        self.base_dir = "äººæ°‘æ—¥æŠ¥"
        self.max_workers = 5
        self.request_delay = 1

    def safe_request(self, url, max_retry=3):
        """å®‰å…¨çš„ç½‘ç»œè¯·æ±‚å‡½æ•°"""
        for attempt in range(max_retry):
            try:
                time.sleep(random.uniform(2, 5))
                resp = requests.get(url, headers=self.headers, timeout=30)
                resp.raise_for_status()
                resp.encoding = 'utf-8'
                
                if not resp.text.strip():
                    raise Exception("Empty response")
                    
                return resp
                
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥(å°è¯• {attempt + 1}/{max_retry}): {url}, é”™è¯¯: {str(e)}")
                if attempt == max_retry - 1:
                    return None
                time.sleep(5 * (attempt + 1))
        return None

    def get_version_list(self, date_str, current_url=None):
        """è·å–ç‰ˆé¢åˆ—è¡¨"""
        try:
            formatted_date = f"{date_str[:6]}/{date_str[6:]}"  # YYYYMM/DD
            
            # ä½¿ç”¨ä¼ å…¥çš„URLæˆ–æ„å»ºåˆå§‹URL
            url = current_url or f"{self.layout_url}/{formatted_date}/node_01.html"
            print(f"â”œâ”€â”€ ğŸ” è®¿é—®ç‰ˆé¢: {url}")
            
            resp = self.safe_request(url)
            if not resp:
                return []

            soup = BeautifulSoup(resp.text, 'html.parser')
            versions = []
            
            # è·å–æ‰€æœ‰ç‰ˆé¢é“¾æ¥
            for link in soup.find_all('a'):
                href = link.get('href', '')
                text = link.text.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰ˆé¢é“¾æ¥
                if 'node_' in href and '.html' in href and 'ç‰ˆï¼š' in text:
                    version_code = text.split('ç‰ˆï¼š')[0].strip()
                    version_name = text.split('ç‰ˆï¼š')[1].strip()
                    version_url = f"{self.layout_url}/{formatted_date}/node_{int(version_code):02d}.html"
                    
                    if not any(v['code'] == f"{int(version_code):02d}" for v in versions):
                        versions.append({
                            'code': f"{int(version_code):02d}",
                            'name': version_name,
                            'url': version_url
                        })
                        print(f"â”‚   â”œâ”€â”€ âœ“ æ‰¾åˆ°ç‰ˆé¢: {version_code}_{version_name}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰ˆé¢ï¼Œå°è¯•ç›´æ¥éå†ç‰ˆé¢å·
            if not versions:
                print("â”‚   â”œâ”€â”€ å°è¯•éå†ç‰ˆé¢...")
                for i in range(1, 21):  # å‡è®¾æœ€å¤š20ä¸ªç‰ˆé¢
                    version_url = f"{self.layout_url}/{formatted_date}/node_{i:02d}.html"
                    try:
                        resp = self.safe_request(version_url)
                        if resp and resp.status_code == 200:
                            # ä»é¡µé¢æ ‡é¢˜è·å–ç‰ˆé¢åç§°
                            soup = BeautifulSoup(resp.text, 'html.parser')
                            title = soup.find('title')
                            if title:
                                title_text = title.text.strip()
                                if 'ï¼š' in title_text:
                                    version_name = title_text.split('ï¼š')[1].split('_')[0]
                                else:
                                    version_name = f"ç¬¬{i:02d}ç‰ˆ"
                                
                                versions.append({
                                    'code': f"{i:02d}",
                                    'name': version_name,
                                    'url': version_url
                                })
                                print(f"â”‚   â”œâ”€â”€ âœ“ æ‰¾åˆ°ç‰ˆé¢: {i:02d}_{version_name}")
                    except Exception:
                        continue
            
            # æŒ‰ç‰ˆé¢å·æ’åº
            versions.sort(key=lambda x: int(x['code']))
            
            if versions:
                print(f"\nâ”œâ”€â”€ ğŸ“‹ å…±æ‰¾åˆ° {len(versions)} ä¸ªç‰ˆé¢")
            else:
                print(f"â”œâ”€â”€ âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç‰ˆé¢ï¼Œè¯·æ£€æŸ¥ç½‘å€æ˜¯å¦æ­£ç¡®")
            
            return versions
            
        except Exception as e:
            print(f"â”œâ”€â”€ âŒ è·å–ç‰ˆé¢åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def create_article_dir(self, date_str, version_code, version_name, title):
        """åˆ›å»ºæ–‡ç« ç›®å½•ç»“æ„"""
        date_dir = os.path.join(self.base_dir, date_str)
        version_dir = os.path.join(date_dir, f"{version_code}_{version_name}")
        article_dir = os.path.join(version_dir, self.clean_filename(title))
        os.makedirs(article_dir, exist_ok=True)
        return article_dir

    def clean_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        return filename.strip()

    def process_article(self, article_url, date_str, version_code, version_name):
        """å¤„ç†å•ç¯‡æ–‡ç« """
        try:
            resp = self.safe_request(article_url)
            if not resp:
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è·å–æ–‡ç« æ ‡é¢˜å’Œå†…å®¹
            title = None
            content = None
            
            # 1. å°è¯•ä»metaæ ‡ç­¾è·å–æ ‡é¢˜
            meta_title = soup.find('meta', {'name': 'ArticleTitle'}) or soup.find('meta', {'property': 'og:title'})
            if meta_title:
                title = meta_title.get('content', '').strip()
            
            # 2. å°è¯•ä»æ ‡é¢˜æ ‡ç­¾è·å–
            if not title:
                # é¦–å…ˆå°è¯•ç‰¹å®šçš„classåç§°
                for class_name in ['article-title', 'title', 'art_title', 'main-title']:
                    title_elem = soup.find(class_=class_name)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        if title:
                            break
                
                # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•h1-h3æ ‡ç­¾
                if not title:
                    for tag in ['h1', 'h2', 'h3']:
                        title_elem = soup.find(tag)
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            if title:
                                break
                
                # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•åŒ…å«"title"æˆ–"heading"çš„class
                if not title:
                    for elem in soup.find_all(class_=True):
                        classes = elem.get('class', [])
                        if any('title' in c.lower() or 'heading' in c.lower() for c in classes):
                            title = elem.get_text(strip=True)
                            if title:
                                break
            
            # 3. å°è¯•ä»é¡µé¢ç»“æ„ä¸­æŸ¥æ‰¾æ ‡é¢˜
            if not title:
                # æŸ¥æ‰¾é¡µé¢ä¸­æœ€æ˜¾è‘—çš„æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜ï¼‰
                candidates = []
                for tag in soup.find_all(['div', 'p', 'span']):
                    text = tag.get_text(strip=True)
                    if text and 2 < len(text) < 100:  # æ ‡é¢˜é€šå¸¸ä¸ä¼šå¤ªé•¿æˆ–å¤ªçŸ­
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜ç‰¹å¾ï¼ˆå­—ä½“å¤§å°ã€ç²—ä½“ç­‰ï¼‰
                        style = tag.get('style', '').lower()
                        if ('font-size' in style and 'px' in style) or 'font-weight' in style:
                            candidates.append((text, len(tag.find_all()), tag.get('style', '')))
                
                if candidates:
                    # é€‰æ‹©æœ€å¯èƒ½æ˜¯æ ‡é¢˜çš„æ–‡æœ¬ï¼ˆä¼˜å…ˆè€ƒè™‘æ ·å¼ç‰¹å¾å’ŒåµŒå¥—æ·±åº¦ï¼‰
                    candidates.sort(key=lambda x: (-len(x[2]), x[1]))  # æ ·å¼å¤šçš„ä¼˜å…ˆï¼ŒåµŒå¥—å°‘çš„ä¼˜å…ˆ
                    title = candidates[0][0]
            
            # 4. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä½¿ç”¨ä¼ å…¥çš„æ ‡é¢˜
            if not title and 'title' in article_url:
                # ä»URLä¸­æå–æ ‡é¢˜éƒ¨åˆ†
                title_match = re.search(r'/([^/]+?)(?:\.html?)?$', article_url)
                if title_match:
                    title = title_match.group(1)
                    title = title.replace('_', ' ').replace('-', ' ')
            
            if not title:
                print(f"    â”‚   â””â”€â”€ âŒ æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜")
                return None
            
            # æ¸…ç†æ ‡é¢˜
            title = re.sub(r'\s+', ' ', title)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
            title = title.strip('ã€€ \t\r\n')  # ç§»é™¤ä¸­è‹±æ–‡ç©ºæ ¼å’Œæ¢è¡Œç¬¦
            if not title:
                print(f"    â”‚   â””â”€â”€ âŒ æ ‡é¢˜ä¸ºç©º")
                return None
            
            # æŸ¥æ‰¾æ–‡ç« å†…å®¹
            # 1. å°è¯•æ‰¾ç‰¹å®šIDçš„å†…å®¹åŒºåŸŸ
            content = soup.find('div', id='ozoom') or soup.find('div', id='articleContent')
            
            # 2. å°è¯•æ‰¾ç‰¹å®šclassçš„å†…å®¹åŒºåŸŸ
            if not content:
                for class_name in ['article', 'article-content', 'text', 'content']:
                    content = soup.find(class_=class_name)
                    if content and len(content.get_text(strip=True)) > 200:
                        break
            
            # 3. å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾æœ€é•¿çš„æ–‡æœ¬å—
            if not content:
                max_length = 200  # æœ€å°å†…å®¹é•¿åº¦é˜ˆå€¼
                for div in soup.find_all('div'):
                    text = div.get_text(strip=True)
                    if len(text) > max_length:
                        max_length = len(text)
                        content = div
            
            if content:
                # æ¸…ç†å†…å®¹
                for tag in content.find_all(['script', 'style']):
                    tag.decompose()
                
                text = content.get_text(separator='\n', strip=True)
                
                # ä¿å­˜æ–‡ç« æ–‡æœ¬
                clean_title = self.clean_filename(title)
                article_dir = self.create_article_dir(date_str, version_code, version_name, clean_title)
                text_file = os.path.join(article_dir, f"{clean_title}.txt")
                
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(f"æ ‡é¢˜ï¼š{title}\n")
                    f.write(f"æ—¥æœŸï¼š{date_str}\n")
                    f.write(f"ç‰ˆé¢ï¼š{version_code}_{version_name}\n")
                    f.write(f"é“¾æ¥ï¼š{article_url}\n\n")
                    f.write("æ­£æ–‡ï¼š\n")
                    f.write(text)
                
                print(f"    â”‚   â”œâ”€â”€ ğŸ“„ å·²ä¿å­˜: {title[:30]}...")
                
                # ä¿å­˜æ–‡ç« ä¸­çš„å›¾ç‰‡
                saved_images = 0
                # åªæŸ¥æ‰¾æ–‡ç« å†…å®¹åŒºåŸŸä¸­çš„å›¾ç‰‡
                images = content.find_all('img')
                
                for i, img in enumerate(images, 1):
                    # è·å–å›¾ç‰‡URL
                    img_url = img.get('src', '')
                    if not img_url:
                        continue
                    
                    # è¿‡æ»¤æ‰è£…é¥°æ€§å›¾ç‰‡
                    # 1. æ£€æŸ¥å›¾ç‰‡å°ºå¯¸ï¼ˆå¦‚æœæœ‰ï¼‰
                    width = img.get('width', '0')
                    height = img.get('height', '0')
                    try:
                        w = int(width) if str(width).isdigit() else 0
                        h = int(height) if str(height).isdigit() else 0
                        if 0 < w < 50 or 0 < h < 50:  # è¿‡æ»¤æ‰å°å›¾æ ‡
                            continue
                    except ValueError:
                        pass
                    
                    # 2. æ£€æŸ¥å›¾ç‰‡URLå…³é”®è¯
                    skip_keywords = ['icon', 'logo', 'button', 'bg', 'background', 'banner', 'nav', 
                                  'd1.gif', 'd.gif', 'dot', 'line', 'split', 'div']
                    if any(keyword in img_url.lower() for keyword in skip_keywords):
                        continue
                    
                    # 3. æ£€æŸ¥å›¾ç‰‡altå’Œtitle
                    img_alt = img.get('alt', '').strip()
                    img_title = img.get('title', '').strip()
                    if any(keyword in (img_alt + img_title).lower() for keyword in skip_keywords):
                        continue
                        
                    # å¤„ç†ç›¸å¯¹URL
                    if not img_url.startswith('http'):
                        if img_url.startswith('//'):
                            img_url = 'http:' + img_url
                        elif img_url.startswith('/'):
                            img_url = 'http://paper.people.com.cn' + img_url
                        else:
                            base_url = '/'.join(article_url.split('/')[:-1])
                            img_url = f"{base_url}/{img_url}"
                    
                    # è·å–å›¾ç‰‡è¯´æ˜æ–‡å­—
                    caption = img_alt or img_title or f'å›¾ç‰‡_{i}'
                    caption = self.clean_filename(caption)
                    
                    # è·å–å›¾ç‰‡æ‰©å±•å
                    img_ext = os.path.splitext(img_url)[1]
                    if not img_ext or img_ext.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:
                        img_ext = '.jpg'  # é»˜è®¤æ‰©å±•å
                    
                    # ä¿å­˜åœ¨æ–‡ç« ç›®å½•ä¸‹
                    img_path = os.path.join(article_dir, f"{caption}{img_ext}")
                    
                    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ åºå·
                    if os.path.exists(img_path):
                        img_path = os.path.join(article_dir, f"{caption}_{i}{img_ext}")
                    
                    # ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡
                    try:
                        img_resp = self.safe_request(img_url)
                        if img_resp and img_resp.content:
                            # æ£€æŸ¥æ–‡ä»¶å¤§å°
                            if len(img_resp.content) < 1024:  # è·³è¿‡å°äº1KBçš„å›¾ç‰‡
                                continue
                            
                            with open(img_path, 'wb') as f:
                                f.write(img_resp.content)
                            saved_images += 1
                    except Exception as e:
                        continue
                
                if saved_images > 0:
                    print(f"    â”‚   â”œâ”€â”€ ğŸ–¼ï¸ å·²ä¿å­˜ {saved_images} å¼ å›¾ç‰‡")
                print(f"    â”‚   â””â”€â”€ âœ… å®Œæˆ")
                return text_file
            
            print(f"    â”‚   â””â”€â”€ âŒ æœªæ‰¾åˆ°æ–‡ç« å†…å®¹: {title[:30]}...")
            return None
            
        except Exception as e:
            print(f"    â”‚   â””â”€â”€ âŒ å¤„ç†å¤±è´¥: {str(e)}")
            return None

    def download_image(self, url, filepath):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            resp = self.safe_request(url)
            if resp and resp.content:
                with open(filepath, 'wb') as f:
                    f.write(resp.content)
                return True
            return False
        except Exception as e:
            print(f"    â”œâ”€â”€ âš ï¸ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}")
            print(f"    â”œâ”€â”€ âš ï¸ é”™è¯¯: {str(e)}")
            return False

    def get_articles_from_version(self, version_url):
        """è·å–ç‰ˆé¢ä¸­çš„æ‰€æœ‰æ–‡ç« é“¾æ¥"""
        try:
            resp = self.safe_request(version_url)
            if not resp:
                return []

            soup = BeautifulSoup(resp.text, 'html.parser')
            articles = []
            
            # æå–æ—¥æœŸä¿¡æ¯ä»ç‰ˆé¢URL
            date_match = re.search(r'/(\d{6})/(\d{2})/', version_url)
            if not date_match:
                return []
                
            yyyymm, dd = date_match.groups()
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
            for link in soup.find_all(['a', 'div']):  # åŒæ—¶æŸ¥æ‰¾aæ ‡ç­¾å’Œdivæ ‡ç­¾
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥
                onclick = link.get('onclick', '')
                href = link.get('href', '')
                
                # è·å–æ ‡é¢˜
                title = None
                # 1. ä»æ–‡æœ¬å†…å®¹è·å–
                title_text = link.get_text(strip=True)
                if title_text and len(title_text) > 2:  # å¿½ç•¥å¤ªçŸ­çš„æ ‡é¢˜
                    title = title_text
                # 2. ä»titleå±æ€§è·å–
                if not title:
                    title = link.get('title', '').strip()
                
                if not title:
                    continue
                
                # å°è¯•è·å–content_id
                content_id = None
                
                # 1. ä»onclickä¸­è·å–
                onclick_match = re.search(r'content_(\d+)', onclick)
                if onclick_match:
                    content_id = onclick_match.group(1)
                
                # 2. ä»hrefä¸­è·å–
                if not content_id and href:
                    href_match = re.search(r'content_(\d+)', href)
                    if href_match:
                        content_id = href_match.group(1)
                
                # 3. ä»çˆ¶å…ƒç´ æˆ–å­å…ƒç´ ä¸­æŸ¥æ‰¾
                if not content_id:
                    # æ£€æŸ¥çˆ¶å…ƒç´ 
                    parent = link.parent
                    if parent:
                        parent_onclick = parent.get('onclick', '')
                        parent_match = re.search(r'content_(\d+)', parent_onclick)
                        if parent_match:
                            content_id = parent_match.group(1)
                    
                    # æ£€æŸ¥å­å…ƒç´ 
                    if not content_id:
                        for child in link.find_all(['a', 'div']):
                            child_onclick = child.get('onclick', '')
                            child_match = re.search(r'content_(\d+)', child_onclick)
                            if child_match:
                                content_id = child_match.group(1)
                                break
                
                if content_id and title:
                    article_url = f"{self.content_url}/{yyyymm}/{dd}/content_{content_id}.html"
                    if not any(a['url'] == article_url for a in articles):  # é¿å…é‡å¤
                        articles.append({
                            'title': title,
                            'url': article_url
                        })
                        print(f"    â”œâ”€â”€ ğŸ“„ æ‰¾åˆ°æ–‡ç« : {title}")
            
            return articles
            
        except Exception as e:
            print(f"    â”œâ”€â”€ âŒ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def process_version(self, version, date_str):
        """å¤„ç†å•ä¸ªç‰ˆé¢"""
        try:
            print(f"\nâ””â”€â”€ ç‰ˆé¢: {version['code']}_{version['name']}")
            
            # è·å–ç‰ˆé¢ä¸­çš„æ‰€æœ‰æ–‡ç« 
            articles = self.get_articles_from_version(version['url'])
            if not articles:
                print(f"    â””â”€â”€ âš ï¸ æœªæ‰¾åˆ°æ–‡ç« ")
                return
            
            # å¤„ç†æ¯ç¯‡æ–‡ç« 
            for article in articles:
                try:
                    self.process_article(article['url'], date_str, version['code'], version['name'])
                    time.sleep(self.request_delay)
                except Exception as e:
                    print(f"    â”œâ”€â”€ âŒ å¤„ç†æ–‡ç« å¤±è´¥: {article['title']}")
                    print(f"    â””â”€â”€ é”™è¯¯: {str(e)}")
            
        except Exception as e:
            print(f"â””â”€â”€ âŒ å¤„ç†ç‰ˆé¢å¤±è´¥: {version['url']}")
            print(f"    â””â”€â”€ é”™è¯¯: {str(e)}")

    def run(self, start_date, end_date):
        """è¿è¡Œçˆ¬è™«"""
        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            for date in date_range:
                date_str = date.strftime('%Y%m%d')
                print(f"\nğŸ“… {date_str}")
                
                # è·å–ç‰ˆé¢åˆ—è¡¨
                versions = self.get_version_list(date_str)
                if not versions:
                    print(f"â””â”€â”€ âŒ æœªè·å–åˆ°ç‰ˆé¢åˆ—è¡¨")
                    continue

                # å¤„ç†æ¯ä¸ªç‰ˆé¢
                for version in versions:
                    self.process_version(version, date_str)
                    time.sleep(self.request_delay)

                print(f"\nâ””â”€â”€ âœ… å®Œæˆ {date_str} çš„æ•°æ®å¤„ç†")
                time.sleep(5)

        except Exception as e:
            print(f"\nâŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {str(e)}")
        finally:
            print("\nğŸ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")

if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å¼
    TEST_MODE = True
    spider = PeoplesDailySpider()
    
    if TEST_MODE:
        print("=== æµ‹è¯•æ¨¡å¼ ===")
        # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ—¥æœŸ
        test_date = '2025-07-02'  # ä½¿ç”¨ä¸€ä¸ªç¡®å®šå­˜åœ¨çš„æ—¥æœŸ
        print(f"æµ‹è¯•çˆ¬å–æ—¥æœŸ: {test_date}")
        spider.run(test_date, test_date)
    else:
        # ç›´æ¥æŒ‡å®šæ—¥æœŸèŒƒå›´
        start_date = '2025-07-01'
        end_date = '2025-07-02'
        print(f"å¼€å§‹çˆ¬å–ä» {start_date} åˆ° {end_date} çš„æ–°é—»...")
        spider.run(start_date, end_date)
