#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import os
import json
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import schedule
import pandas as pd
from bs4 import BeautifulSoup
from äººæ°‘æ—¥æŠ¥ import PeoplesDailySpider

class IncrementalPeoplesDailySpider(PeoplesDailySpider):
    def __init__(self):
        super().__init__()
        self.record_file = "rmrb_crawled_articles.json"
        self.crawled_records = self.load_crawled_records()
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, f"rmrb_spider_{datetime.now().strftime('%Y%m%d')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )

    def load_crawled_records(self):
        """åŠ è½½å·²çˆ¬å–çš„æ–‡ç« è®°å½•"""
        try:
            if os.path.exists(self.record_file):
                with open(self.record_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logging.error(f"åŠ è½½çˆ¬å–è®°å½•å¤±è´¥: {str(e)}")
            return {}

    def save_crawled_records(self):
        """ä¿å­˜å·²çˆ¬å–çš„æ–‡ç« è®°å½•"""
        try:
            with open(self.record_file, 'w', encoding='utf-8') as f:
                json.dump(self.crawled_records, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜çˆ¬å–è®°å½•å¤±è´¥: {str(e)}")

    def is_article_crawled(self, date_str, article_url, title):
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²ç»çˆ¬å–è¿‡"""
        article_key = f"{date_str}_{article_url}_{title}"
        return article_key in self.crawled_records

    def mark_article_crawled(self, date_str, article_url, title, file_path):
        """æ ‡è®°æ–‡ç« ä¸ºå·²çˆ¬å–"""
        article_key = f"{date_str}_{article_url}_{title}"
        self.crawled_records[article_key] = {
            'date': date_str,
            'url': article_url,
            'title': title,
            'file_path': file_path,
            'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    def process_article(self, article_url, date_str, version_code, version_name):
        """å¤„ç†å•ç¯‡æ–‡ç« ï¼ˆå¢é‡ç‰ˆæœ¬ï¼‰"""
        try:
            resp = self.safe_request(article_url)
            if not resp:
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è·å–æ–‡ç« æ ‡é¢˜å’Œå†…å®¹
            title = None
            content = None
            
            # 1. å°è¯•ä»metaæ ‡ç­¾è·å–æ ‡é¢˜
            meta_title = soup.find('meta', {'name': 'ArticleTitle'}) or soup.find('meta', {'property': 'og:title'})
            if meta_title:
                title = meta_title.get('content', '').strip()
            
            # 2. å°è¯•ä»h1æˆ–h2æ ‡ç­¾è·å–æ ‡é¢˜
            if not title:
                title_elem = soup.find('h1') or soup.find('h2')
                if title_elem:
                    title = title_elem.get_text(strip=True)
            
            # 3. å°è¯•ä»ç‰¹å®šclassè·å–æ ‡é¢˜
            if not title:
                for class_name in ['article-title', 'title', 'art_title', 'main-title', 'title_word']:
                    title_elem = soup.find(class_=class_name)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        if title:
                            break

            # 4. å°è¯•ä»é¡µé¢æ ‡é¢˜è·å–
            if not title:
                page_title = soup.find('title')
                if page_title:
                    title_text = page_title.get_text(strip=True)
                    # ç§»é™¤ç½‘ç«™åç§°ç­‰é¢å¤–ä¿¡æ¯
                    title = title_text.split('_')[0].split('-')[0].split('|')[0].strip()
            
            if not title:
                logging.error(f"æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜: {article_url}")
                return None

            # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²çˆ¬å–
            if self.is_article_crawled(date_str, article_url, title):
                logging.info(f"æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡: {title}")
                return self.crawled_records[f"{date_str}_{article_url}_{title}"]['file_path']
            
            # æŸ¥æ‰¾æ–‡ç« å†…å®¹
            content = None
            
            # 1. å°è¯•ä»idè·å–å†…å®¹
            for content_id in ['ozoom', 'articleContent', 'article', 'content', 'mainContent']:
                content = soup.find('div', id=content_id)
                if content and len(content.get_text(strip=True)) > 100:
                    break
            
            # 2. å°è¯•ä»classè·å–å†…å®¹
            if not content:
                for class_name in ['article', 'article-content', 'text', 'content', 'article_content']:
                    content = soup.find(['div', 'article'], class_=class_name)
                    if content and len(content.get_text(strip=True)) > 100:
                        break

            # 3. å°è¯•æŸ¥æ‰¾æœ€é•¿çš„æ–‡æœ¬å—
            if not content:
                text_blocks = soup.find_all(['div', 'article', 'section'])
                if text_blocks:
                    content = max(text_blocks, key=lambda x: len(x.get_text(strip=True)))
                    if len(content.get_text(strip=True)) < 100:
                        content = None

            if not content:
                logging.error(f"æœªæ‰¾åˆ°æ–‡ç« å†…å®¹: {article_url}")
                return None

            # æ¸…ç†å†…å®¹
            for tag in content.find_all(['script', 'style', 'iframe', 'button', 'input', 'meta']):
                tag.decompose()
            
            text = content.get_text(separator='\n', strip=True)
            
            # ä¿å­˜æ–‡ç« æ–‡æœ¬
            clean_title = self.clean_filename(title)
            article_dir = self.create_article_dir(date_str, version_code, version_name, clean_title)
            text_file = os.path.join(article_dir, f"{clean_title}.txt")
            
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(f"æ ‡é¢˜ï¼š{title}\n")
                f.write(f"æ—¥æœŸï¼š{date_str}\n")
                f.write(f"ç‰ˆé¢ï¼š{version_code}_{version_name}\n")
                f.write(f"é“¾æ¥ï¼š{article_url}\n\n")
                f.write("æ­£æ–‡ï¼š\n")
                f.write(text)
            
            logging.info(f"å·²ä¿å­˜æ–‡ç« : {title}")
            
            # ä¿å­˜æ–‡ç« ä¸­çš„å›¾ç‰‡
            saved_images = 0
            images = content.find_all('img')
            
            for i, img in enumerate(images, 1):
                img_url = img.get('src', '')
                if not img_url:
                    continue
                
                # è¿‡æ»¤æ‰è£…é¥°æ€§å›¾ç‰‡
                skip_keywords = ['icon', 'logo', 'button', 'bg', 'background', 'banner', 'nav', 
                               'd1.gif', 'd.gif', 'dot', 'line', 'split', 'div']
                if any(keyword in img_url.lower() for keyword in skip_keywords):
                    continue
                
                # å¤„ç†ç›¸å¯¹URL
                if not img_url.startswith('http'):
                    if img_url.startswith('//'):
                        img_url = 'http:' + img_url
                    elif img_url.startswith('/'):
                        img_url = 'http://paper.people.com.cn' + img_url
                    else:
                        base_url = '/'.join(article_url.split('/')[:-1])
                        img_url = f"{base_url}/{img_url}"
                
                # è·å–å›¾ç‰‡è¯´æ˜æ–‡å­—
                img_alt = img.get('alt', '').strip()
                img_title = img.get('title', '').strip()
                caption = img_alt or img_title or f'å›¾ç‰‡_{i}'
                caption = self.clean_filename(caption)
                
                # ä¿å­˜å›¾ç‰‡
                img_path = os.path.join(article_dir, f"{caption}.jpg")
                if os.path.exists(img_path):
                    img_path = os.path.join(article_dir, f"{caption}_{i}.jpg")
                
                if self.download_image(img_url, img_path):
                    saved_images += 1
            
            if saved_images > 0:
                logging.info(f"å·²ä¿å­˜ {saved_images} å¼ å›¾ç‰‡")
            
            # æ ‡è®°æ–‡ç« ä¸ºå·²çˆ¬å–
            self.mark_article_crawled(date_str, article_url, title, text_file)
            self.save_crawled_records()  # æ¯ç¯‡æ–‡ç« ä¿å­˜åå°±æ›´æ–°è®°å½•
            
            return text_file
            
        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ç« å¤±è´¥: {article_url}, é”™è¯¯: {str(e)}")
            return None

    def crawl_today(self):
        """çˆ¬å–ä»Šå¤©çš„æ–°é—»"""
        today = datetime.now().strftime('%Y-%m-%d')
        logging.info(f"å¼€å§‹çˆ¬å– {today} çš„æ–°é—»...")
        self.run(today, today)

    def run(self, start_date, end_date):
        """è¿è¡Œçˆ¬è™«ï¼ˆå¢é‡ç‰ˆæœ¬ï¼‰"""
        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            for date in date_range:
                date_str = date.strftime('%Y%m%d')
                logging.info(f"\nğŸ“… å¼€å§‹å¤„ç† {date_str}")
                
                versions = self.get_version_list(date_str)
                if not versions:
                    logging.error(f"æœªè·å–åˆ°ç‰ˆé¢åˆ—è¡¨")
                    continue

                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = []
                    for version in versions:
                        futures.append(
                            executor.submit(self.process_version, version, date_str)
                        )
                        time.sleep(self.request_delay)

                    for future in as_completed(futures):
                        try:
                            future.result()
                        except Exception as e:
                            logging.error(f"ç‰ˆé¢å¤„ç†ä»»åŠ¡å¤±è´¥: {str(e)}")

                self.save_crawled_records()
                logging.info(f"å®Œæˆ {date_str} çš„æ•°æ®å¤„ç†")
                time.sleep(5)

        except Exception as e:
            logging.error(f"çˆ¬è™«è¿è¡Œå‡ºé”™: {str(e)}")
        finally:
            self.save_crawled_records()
            logging.info("å·²å®Œæˆçˆ¬å–ä»»åŠ¡ï¼")

def run_spider():
    """è¿è¡Œçˆ¬è™«çš„å®šæ—¶ä»»åŠ¡"""
    spider = IncrementalPeoplesDailySpider()
    spider.crawl_today()
    logging.info(f"å®Œæˆå®šæ—¶çˆ¬å–ä»»åŠ¡: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å¼
    TEST_MODE = False
    spider = IncrementalPeoplesDailySpider()
    
    if TEST_MODE:
        print("=== æµ‹è¯•æ¨¡å¼ ===")
        # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ—¥æœŸ
        test_date = '2025-06-30'  # ä½¿ç”¨ä¸€ä¸ªç¡®å®šå­˜åœ¨çš„æ—¥æœŸ
        print(f"æµ‹è¯•çˆ¬å–æ—¥æœŸ: {test_date}")
        spider.run(test_date, test_date)
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šè®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every().day.at("08:35").do(run_spider)  # å‡Œæ™¨2ç‚¹
        schedule.every().day.at("10:00").do(run_spider)  # ä¸Šåˆ10ç‚¹
        schedule.every().day.at("22:31").do(run_spider)  # æ™šä¸Š6ç‚¹
        
        logging.info(f"å¢é‡çˆ¬è™«å·²å¯åŠ¨ï¼Œå°†åœ¨æ¯å¤© 02:00ã€10:00ã€18:00 è¿è¡Œ...")
        logging.info(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # å…ˆæ‰§è¡Œä¸€æ¬¡ï¼Œçˆ¬å–å½“å¤©çš„å†…å®¹
        logging.info("æ‰§è¡Œé¦–æ¬¡çˆ¬å–...")
        run_spider()
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡å¾ªç¯
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                logging.info("\nçˆ¬è™«å·²åœæ­¢è¿è¡Œ")
                break
            except Exception as e:
                logging.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(300)  # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…5åˆ†é’Ÿåç»§ç»­ 